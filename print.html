<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded "><a href="0001-telemetry-without-manual-instrumentation.html"><strong aria-hidden="true">1.</strong> 0001-telemetry-without-manual-instrumentation</a></li><li class="chapter-item expanded "><a href="0002-remove-spandata.html"><strong aria-hidden="true">2.</strong> 0002-remove-spandata</a></li><li class="chapter-item expanded "><a href="0003-measure-metric-type.html"><strong aria-hidden="true">3.</strong> 0003-measure-metric-type</a></li><li class="chapter-item expanded "><a href="0005-global-init.html"><strong aria-hidden="true">4.</strong> 0005-global-init</a></li><li class="chapter-item expanded "><a href="0006-sampling.html"><strong aria-hidden="true">5.</strong> 0006-sampling</a></li><li class="chapter-item expanded "><a href="0007-no-out-of-band-reporting.html"><strong aria-hidden="true">6.</strong> 0007-no-out-of-band-reporting</a></li><li class="chapter-item expanded "><a href="0008-metric-observer.html"><strong aria-hidden="true">7.</strong> 0008-metric-observer</a></li><li class="chapter-item expanded "><a href="0009-metric-handles.html"><strong aria-hidden="true">8.</strong> 0009-metric-handles</a></li><li class="chapter-item expanded "><a href="0010-cumulative-to-counter.html"><strong aria-hidden="true">9.</strong> 0010-cumulative-to-counter</a></li><li class="chapter-item expanded "><a href="0016-named-tracers.html"><strong aria-hidden="true">10.</strong> 0016-named-tracers</a></li><li class="chapter-item expanded "><a href="0035-opentelemetry-protocol.html"><strong aria-hidden="true">11.</strong> 0035-opentelemetry-protocol</a></li><li class="chapter-item expanded "><a href="0038-version-semantic-attribute.html"><strong aria-hidden="true">12.</strong> 0038-version-semantic-attribute</a></li><li class="chapter-item expanded "><a href="0049-metric-label-set.html"><strong aria-hidden="true">13.</strong> 0049-metric-label-set</a></li><li class="chapter-item expanded "><a href="0059-otlp-trace-data-format.html"><strong aria-hidden="true">14.</strong> 0059-otlp-trace-data-format</a></li><li class="chapter-item expanded "><a href="0066-separate-context-propagation.html"><strong aria-hidden="true">15.</strong> 0066-separate-context-propagation</a></li><li class="chapter-item expanded "><a href="0070-metric-bound-instrument.html"><strong aria-hidden="true">16.</strong> 0070-metric-bound-instrument</a></li><li class="chapter-item expanded "><a href="0072-metric-observer.html"><strong aria-hidden="true">17.</strong> 0072-metric-observer</a></li><li class="chapter-item expanded "><a href="0080-remove-metric-gauge.html"><strong aria-hidden="true">18.</strong> 0080-remove-metric-gauge</a></li><li class="chapter-item expanded "><a href="0083-component.html"><strong aria-hidden="true">19.</strong> 0083-component</a></li><li class="chapter-item expanded "><a href="0088-metric-instrument-optional-refinements.html"><strong aria-hidden="true">20.</strong> 0088-metric-instrument-optional-refinements</a></li><li class="chapter-item expanded "><a href="0090-remove-labelset-from-metrics-api.html"><strong aria-hidden="true">21.</strong> 0090-remove-labelset-from-metrics-api</a></li><li class="chapter-item expanded "><a href="0091-logs-vocabulary.html"><strong aria-hidden="true">22.</strong> 0091-logs-vocabulary</a></li><li class="chapter-item expanded "><a href="0092-logs-vision.html"><strong aria-hidden="true">23.</strong> 0092-logs-vision</a></li><li class="chapter-item expanded "><a href="0097-log-data-model.html"><strong aria-hidden="true">24.</strong> 0097-log-data-model</a></li><li class="chapter-item expanded "><a href="0098-metric-instruments-explained.html"><strong aria-hidden="true">25.</strong> 0098-metric-instruments-explained</a></li><li class="chapter-item expanded "><a href="0099-otlp-http.html"><strong aria-hidden="true">26.</strong> 0099-otlp-http</a></li><li class="chapter-item expanded "><a href="0108-naming-guidelines.html"><strong aria-hidden="true">27.</strong> 0108-naming-guidelines</a></li><li class="chapter-item expanded "><a href="0110-z-pages.html"><strong aria-hidden="true">28.</strong> 0110-z-pages</a></li><li class="chapter-item expanded "><a href="0111-auto-resource-detection.html"><strong aria-hidden="true">29.</strong> 0111-auto-resource-detection</a></li><li class="chapter-item expanded "><a href="0113-exemplars.html"><strong aria-hidden="true">30.</strong> 0113-exemplars</a></li><li class="chapter-item expanded "><a href="0119-standard-system-metrics.html"><strong aria-hidden="true">31.</strong> 0119-standard-system-metrics</a></li><li class="chapter-item expanded "><a href="0121-config-service.html"><strong aria-hidden="true">32.</strong> 0121-config-service</a></li><li class="chapter-item expanded "><a href="0122-otlp-http-json.html"><strong aria-hidden="true">33.</strong> 0122-otlp-http-json</a></li><li class="chapter-item expanded "><a href="0126-Configurable-Metric-Aggregations.html"><strong aria-hidden="true">34.</strong> 0126-Configurable-Metric-Aggregations</a></li><li class="chapter-item expanded "><a href="0130-logs-1.0ga-definition.html"><strong aria-hidden="true">35.</strong> 0130-logs-1.0ga-definition</a></li><li class="chapter-item expanded "><a href="0131-otlp-export-behavior.html"><strong aria-hidden="true">36.</strong> 0131-otlp-export-behavior</a></li><li class="chapter-item expanded "><a href="0136-error_flagging.html"><strong aria-hidden="true">37.</strong> 0136-error_flagging</a></li><li class="chapter-item expanded "><a href="0143-versioning-and-stability.html"><strong aria-hidden="true">38.</strong> 0143-versioning-and-stability</a></li><li class="chapter-item expanded "><a href="0146-metrics-prototype-scenarios.html"><strong aria-hidden="true">39.</strong> 0146-metrics-prototype-scenarios</a></li><li class="chapter-item expanded "><a href="SUMMARY.html"><strong aria-hidden="true">40.</strong> SUMMARY</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#opentelemetry-enhancement-proposal-otep" id="opentelemetry-enhancement-proposal-otep">OpenTelemetry Enhancement Proposal (OTEP)</a></h1>
<p><a href="https://gitter.im/open-telemetry/opentelemetry-specification?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge"><img src="https://badges.gitter.im/open-telemetry/opentelemetry-specification.svg" alt="Gitter chat" /></a>
<a href="https://circleci.com/gh/open-telemetry/oteps"><img src="https://circleci.com/gh/open-telemetry/oteps.svg?style=svg" alt="Build Status" /></a></p>
<h2><a class="header" href="#evolving-opentelemetry-at-the-speed-of-markdown" id="evolving-opentelemetry-at-the-speed-of-markdown">Evolving OpenTelemetry at the speed of Markdown</a></h2>
<p>OpenTelemetry uses an &quot;OTEP&quot; (similar to a RFC) process for proposing changes to the <a href="https://github.com/open-telemetry/opentelemetry-specification">OpenTelemetry specification</a>.</p>
<h3><a class="header" href="#table-of-contents" id="table-of-contents">Table of Contents</a></h3>
<ul>
<li><a href="introduction.html#opentelemetry-enhancement-proposal-otep">OpenTelemetry Enhancement Proposal (OTEP)</a>
<ul>
<li><a href="introduction.html#evolving-opentelemetry-at-the-speed-of-markdown">Evolving OpenTelemetry at the speed of Markdown</a>
<ul>
<li><a href="introduction.html#table-of-contents">Table of Contents</a></li>
<li><a href="introduction.html#what-changes-require-an-otep">What changes require an OTEP</a>
<ul>
<li><a href="introduction.html#extrapolating-cross-cutting-changes">Extrapolating cross-cutting changes</a></li>
</ul>
</li>
<li><a href="introduction.html#otep-scope">OTEP scope</a></li>
<li><a href="introduction.html#writing-an-otep">Writing an OTEP</a></li>
<li><a href="introduction.html#submitting-the-otep">Submitting the OTEP</a></li>
<li><a href="introduction.html#integrating-the-otep-into-the-spec">Integrating the OTEP into the Spec</a></li>
<li><a href="introduction.html#implementing-the-otep">Implementing the OTEP</a></li>
</ul>
</li>
<li><a href="introduction.html#changes-to-the-otep-process">Changes to the OTEP process</a></li>
<li><a href="introduction.html#background-on-the-opentelemetry-otep-process">Background on the OpenTelemetry OTEP process</a></li>
</ul>
</li>
</ul>
<h3><a class="header" href="#what-changes-require-an-otep" id="what-changes-require-an-otep">What changes require an OTEP</a></h3>
<p>The OpenTelemetry OTEP process is intended for changes that are <strong>cross-cutting</strong> - that is, applicable across <em>languages</em> and <em>implementations</em> - and either <strong>introduce new behaviour</strong>, <strong>change desired behaviour</strong>, or otherwise <strong>modify requirements</strong>.</p>
<p>In practice, this means that OTEPs should be used for such changes as:</p>
<ul>
<li>New tracer configuration options</li>
<li>Additions to span data</li>
<li>New metric types</li>
<li>Modifications to extensibility requirements</li>
</ul>
<p>On the other hand, they do not need to be used for such changes as:</p>
<ul>
<li>Bug fixes</li>
<li>Rephrasing, grammatical fixes, typos, etc.</li>
<li>Refactoring</li>
<li>Things that affect only a single language or implementation</li>
</ul>
<p><strong>Note:</strong> The above lists are intended only as examples and are not meant to be exhaustive. If you don't know whether a change requires an OTEP, please feel free to ask!</p>
<h4><a class="header" href="#extrapolating-cross-cutting-changes" id="extrapolating-cross-cutting-changes">Extrapolating cross-cutting changes</a></h4>
<p>Sometimes, a change that is only immediately relevant within a single language or implementation may be indicative of a problem upstream in the specification. We encourage you to add an OTEP if and when you notice such cases.</p>
<h3><a class="header" href="#otep-scope" id="otep-scope">OTEP scope</a></h3>
<p>While OTEPs are intended for &quot;significant&quot; changes, we recommend trying to keep each OTEP's scope as small as makes sense. A general rule of thumb is that if the core functionality proposed could still provide value without a particular piece, then that piece should be removed from the proposal and used instead as an <em>example</em> (and, ideally, given its own OTEP!).</p>
<p>For example, an OTEP proposing configurable sampling <em>and</em> various samplers should instead be split into one OTEP proposing configurable sampling as well as an OTEP per sampler.</p>
<h3><a class="header" href="#writing-an-otep" id="writing-an-otep">Writing an OTEP</a></h3>
<ul>
<li>First, <a href="https://help.github.com/en/articles/fork-a-repo">fork</a> this <a href="https://github.com/open-telemetry/oteps">repo</a>.</li>
<li>Copy <a href="./0000-template.html"><code>0000-template.md</code></a> to <code>text/0000-my-OTEP.md</code>, where <code>my-OTEP</code> is a title relevant to your proposal, and <code>0000</code> is the OTEP ID. Leave the number as is for now. Once a Pull Request is made, update this ID to match the PR ID.</li>
<li>Fill in the template. Put care into the details: It is important to present convincing motivation, demonstrate an understanding of the design's impact, and honestly assess the drawbacks and potential alternatives.</li>
</ul>
<h3><a class="header" href="#submitting-the-otep" id="submitting-the-otep">Submitting the OTEP</a></h3>
<ul>
<li>An OTEP is <code>proposed</code> by posting it as a PR. Once the PR is created, update the OTEP file name to use the PR ID as the OTEP ID.</li>
<li>An OTEP is <code>approved</code> when four reviewers github-approve the PR. The OTEP is then merged.</li>
<li>If an OTEP is <code>rejected</code> or <code>withdrawn</code>, the PR is closed. Note that these OTEPs submissions are still recorded, as Github retains both the discussion and the proposal, even if the branch is later deleted.</li>
<li>If an OTEP discussion becomes long, and the OTEP then goes through a major revision, the next version of the OTEP can be posted as a new PR, which references the old PR. The old PR is then closed. This makes OTEP review easier to follow and participate in.</li>
</ul>
<h3><a class="header" href="#integrating-the-otep-into-the-spec" id="integrating-the-otep-into-the-spec">Integrating the OTEP into the Spec</a></h3>
<ul>
<li>Once an OTEP is <code>approved</code>, an issue is created in the <a href="https://github.com/open-telemetry/opentelemetry-specification">specification repo</a> to integrate the OTEP into the spec.</li>
<li>When reviewing the spec PR for the OTEP, focus on whether the spec is written clearly, and reflects the changes approved in the OTEP. Please abstain from relitigating the approved OTEP changes at this stage.</li>
<li>An OTEP is <code>integrated</code> when four reviewers github-approve the spec PR. The PR is then merged, and the spec is versioned.</li>
</ul>
<h3><a class="header" href="#implementing-the-otep" id="implementing-the-otep">Implementing the OTEP</a></h3>
<ul>
<li>Once an OTEP is <code>integrated</code> into the spec, an issue is created in the backlog of every relevant OpenTelemetry implementation.</li>
<li>PRs are made until the all the requested changes are implemented.</li>
<li>The status of the OpenTelemetry implementation is updated to reflect that it is implementing a new version of the spec.</li>
</ul>
<h2><a class="header" href="#changes-to-the-otep-process" id="changes-to-the-otep-process">Changes to the OTEP process</a></h2>
<p>The hope and expectation is that the OTEP process will <strong>evolve</strong> with the OpenTelemetry. The process is by no means fixed.</p>
<p>Have suggestions? Concerns? Questions? <strong>Please</strong> raise an issue or raise the matter on our <a href="https://github.com/open-telemetry/community">community</a> chat.</p>
<h2><a class="header" href="#background-on-the-opentelemetry-otep-process" id="background-on-the-opentelemetry-otep-process">Background on the OpenTelemetry OTEP process</a></h2>
<p>Our OTEP process borrows from the <a href="https://github.com/rust-lang/rfcs">Rust RFC</a> and <a href="https://github.com/kubernetes/enhancements">Kubernetes Enhancement Proposal</a> processes, the former also being <a href="https://github.com/kubernetes/enhancements/blob/master/keps/0001-kubernetes-enhancement-proposal-process.md#prior-art">very influential</a> on the latter; as well as the <a href="https://github.com/opentracing/specification/tree/master/rfc_process.md">OpenTracing OTEP</a> process. Massive kudos and thanks to the respective authors and communities for providing excellent prior art 💖</p>
<h1><a class="header" href="#open-telemetry-without-manual-instrumentation" id="open-telemetry-without-manual-instrumentation">(Open) Telemetry Without Manual Instrumentation</a></h1>
<p><em>Cross-language requirements for automated approaches to extracting portable telemetry data with zero source code modification.</em></p>
<h2><a class="header" href="#motivation" id="motivation">Motivation</a></h2>
<p>The purpose of OpenTelemetry is to make robust, portable telemetry a built-in feature of cloud-native software. For some software and some situations, that instrumentation can literally be part of the source code. In other situations, it’s not so simple: for example, we can’t necessarily edit or even recompile some of our software, the OpenTelemetry instrumentation only exists as a plugin, or instrumentation just never rises to the top of the priority list for a service-owner. Furthermore, there is occasionally a desire to disable instrumentation for a single plugin or module at runtime, again without requiring developers to make changes to source code.</p>
<p>One way to navigate situations like these is with a software layer that adds OpenTelemetry instrumentation to a service without modifying the source code for that service. (In the conventional APM world, these software layers are often called “agents”, though that term is overloaded and ambiguous so we try avoid it in this document.)</p>
<h3><a class="header" href="#why-cross-language" id="why-cross-language">Why “cross-language”?</a></h3>
<p>Many people have correctly observed that “agent” design is highly language-dependent. This is certainly true, but there are still higher-level “product” objectives for OpenTelemetry that can guide the design choices we make across languages and help users form a consistent impression of what OpenTelemetry provides (and what it does not).</p>
<h3><a class="header" href="#suggested-reading" id="suggested-reading">Suggested reading</a></h3>
<ul>
<li>This GitHub issue: <a href="https://github.com/open-telemetry/community/pull/87">Propose an &quot;Auto-Instrumentation SIG&quot;</a></li>
<li><a href="https://docs.google.com/document/d/1ix0WtzB5j-DRj1VQQxraoqeUuvgvfhA6Sd8mF5WLNeY/edit">Rough notes from the June 11, 2019 meeting</a> following this ^^ issue</li>
<li>The <a href="https://docs.google.com/document/d/1sovSQIGdxXtsauxUNp4qUMEIJZzObdukzPT52eyPCHM/edit#">rough draft for this RFC</a>, including the comments</li>
</ul>
<h2><a class="header" href="#proposed-guidelines" id="proposed-guidelines">Proposed guidelines</a></h2>
<h3><a class="header" href="#requirements" id="requirements">Requirements</a></h3>
<p>Without further ado, here are a set of requirements for “official” OpenTelemetry efforts to accomplish zero-source-code-modification instrumentation (i.e., “OpenTelemetry agents”) in any given language:</p>
<ul>
<li><em>Manual</em> source code modifications &quot;very strongly discouraged&quot;, with an exception for languages or environments that leave no credible alternatives. Any code changes must be trivial and <code>O(1)</code> per source file (rather than per-function, etc).</li>
<li>Licensing must be permissive (e.g., ASL / BSD)</li>
<li>Packaging must allow vendors to “wrap” or repackage the portable (OpenTelemetry) library into a single asset that’s delivered to customers
<ul>
<li>That is, vendors do not want to require users to comprehend both an OpenTelemetry package and a vendor-specific package</li>
</ul>
</li>
<li>Explicit, whitebox OpenTelemetry instrumentation must interoperate with the “automatic” / zero-source-code-modification / blackbox instrumentation.
<ul>
<li>If the blackbox instrumentation starts a Span, whitebox instrumentation must be able to discover it as the active Span (and vice versa)</li>
<li>Relatedly, there also must be a way to discover and avoid potential conflicts/overlap/redundancy between explicit whitebox instrumentation and blackbox instrumentation of the same libraries/packages
<ul>
<li>That is, if a developer has already added the “official” OpenTelemetry plugin for, say, gRPC, then when the blackbox instrumentation effort adds gRPC support, it should <em>not</em> “double-instrument” it and create a mess of extra spans/etc</li>
</ul>
</li>
</ul>
</li>
<li>From the standpoint of the actual telemetry being gathered, the same standards and expectations (about tagging, metadata, and so on) apply to &quot;whitebox&quot; instrumentation and automatic instrumentation</li>
<li>The code in the OpenTelemetry package must not take a hard dependency on any particular vendor/vendors (that sort of functionality should work via a plugin or registry mechanism)
<ul>
<li>Further, the code in the OpenTelemetry package must be isolated to avoid possible conflicts with the host application (e.g., shading in Java, etc)</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#nice-to-have-properties" id="nice-to-have-properties">Nice-to-have properties</a></h3>
<ul>
<li>Run-time integration (vs compile-time integration)</li>
<li>Automated and modular testing of individual library/package plugins
<ul>
<li>Note that this also makes it easy to test against multiple different versions of any given library</li>
</ul>
</li>
<li>A fully pluggable architecture, where plugins can be registered at runtime without requiring changes to the central repo at github.com/open-telemetry
<ul>
<li>E.g., for ops teams that want to write a plugin for a proprietary piece of legacy software they are unable to recompile</li>
</ul>
</li>
<li>Augemntation of whitebox instrumentation by blackbox instrumentation (or, perhaps, vice versa). That is, not only can the trace context be shared by these different flavors of instrumentation, but even things like in-flight Span objects can be shared and co-modified (e.g., to use runtime interposition to grab local variables and attach them to a manually-instrumented span).</li>
</ul>
<h2><a class="header" href="#trade-offs-and-mitigations" id="trade-offs-and-mitigations">Trade-offs and mitigations</a></h2>
<p>Approaching a problem this language-specific at the cross-language altitude is intrinsically challenging since &quot;different languages are different&quot; – e.g., in Go there is no way to perform the kind of runtime interpositioning that's possible in Python, Ruby, or even Java.</p>
<p>There is also a school of thought that we should only be focusing on the bits and bytes that actually escape the running process and ignore how that's actually accomplished. This has a certain elegance to it, but it also runs afoul of the need to have manual instrumentation interoperate with the zero-touch instrumentation, especially when it comes to the (shared) distributed context itself.</p>
<h2><a class="header" href="#proposal" id="proposal">Proposal</a></h2>
<h3><a class="header" href="#what-is-our-desired-end-state-for-opentelemetry-end-users" id="what-is-our-desired-end-state-for-opentelemetry-end-users">What is our desired end state for OpenTelemetry end-users?</a></h3>
<p>To reiterate much of the above:</p>
<ul>
<li>First and foremost, <strong>portable OpenTelemetry instrumentation can be installed without manual source code modification</strong></li>
<li>There’s one “clear winner” when it comes to portable, automatic instrumentation; just like with OpenTracing and OpenCensus, this is a situation where choice is not necessarily a good thing. End-users who wish to contribute instrumentation plugins should not have their enthusiasm and generosity diluted across competing projects.</li>
<li>As much as such a thing is possible, consistency across languages</li>
<li>Broad coverage / “plugin support”</li>
<li>Broad vendor support for OpenTelemetry</li>
<li>All other things being equal, get all of these ^^ benefits ASAP!</li>
</ul>
<h3><a class="header" href="#whats-the-basic-proposal" id="whats-the-basic-proposal">What's the basic proposal?</a></h3>
<p>Given the desired end state, the Datadog tracers seem like the closest-fit, permissively-licensed option out there today. We asked Datadog's leadership whether they would be interested in donating that code to OpenTelemetry, and they were receptive to the idea. (I.e., this would not be a &quot;hard fork&quot; that must be maintained in parallel forever)</p>
<h3><a class="header" href="#the-overarching-technical-process-per-language" id="the-overarching-technical-process-per-language">The overarching (technical) process, per-language</a></h3>
<ul>
<li>Start with <a href="https://github.com/DataDog">the Datadog <code>dd-trace-foo</code> tracers</a></li>
<li>For each language:
<ul>
<li>Fork the Datadog <code>datadog/dd-trace-foo</code> repo into a <code>open-telemetry/auto-instr-foo</code> OpenTelemetry repo (exact naming TBD)</li>
<li>In parallel:
<ul>
<li>The <code>dd-trace-foo</code> codebases already do a good job separating Datadog-specific functionality from general-purpose functionality. Where needed, make that boundary even more explicit through an API (or &quot;SPI&quot;, really).</li>
<li>Create a new <code>dd-trace-foo</code> lib that wraps <code>auto-instr-foo</code> and includes the Datadog-specific pieces factored out above</li>
<li>Show that it’s also possible to bind to arbitrary OpenTelemetry-based tracers to the above API/SPI</li>
</ul>
</li>
<li>Declare the forked <code>auto-instr-foo</code> repository ready for production beta use</li>
<li>For some (ideally brief) period:
<ul>
<li>When new plugins are added to Datadog's (original) repo, merge them over into the <code>auto-instr-foo</code> repo</li>
<li>Allow Datadog end-users to bind to either for some period of time (ultimately Datadog's decision on timeline here, and does not prevent other tracers from using <code>auto-instr-foo</code>)</li>
<li>Finally, when the combination of <code>auto-instr-foo</code> and a Datadog wrapper is functionally equivalent to the <code>dd-trace-foo</code> mainline, the latter can be safely replaced by the former.
<ul>
<li>Note that, by design, this is not expected to affect Datadog end-users</li>
</ul>
</li>
</ul>
</li>
<li>Moved repo is GA’d: all new plugins (and improvements to the auto-instrumentation core) happen in the <code>auto-instr-foo</code> repo</li>
</ul>
</li>
</ul>
<p>There are some languages that will have OpenTelemetry support before they have Datadog <code>dd-trace-foo</code> support. In those situations, we will fall back to the requirements in this OTEP and leave the technical determinations up to the language SIG and the OpenTelemetry TC.</p>
<h3><a class="header" href="#governance-of-the-auto-instrumentation-libraries" id="governance-of-the-auto-instrumentation-libraries">Governance of the auto-instrumentation libraries</a></h3>
<p>Each <code>auto-instr-foo</code> repository must have at least one <a href="https://github.com/open-telemetry/community/blob/master/community-membership.md#maintainer">Maintainer</a> in common with the main <code>opentelemetry-foo</code> language repository. There are no other requirements or constraints about the set of maintainers/approvers for the main language repository and the respective auto-instrumentation repository; in particular, there may be maintainers/approvers of the main language repository that are not maintainers/approvers for the auto-instrumentation repository, and vice versa.</p>
<h3><a class="header" href="#mini-faq-about-this-proposal" id="mini-faq-about-this-proposal">Mini-FAQ about this proposal</a></h3>
<p><strong>Will this be the only auto-instrumentation story for OpenTelemetry?</strong> It need not be. The auto-instrumentation libraries described above will have no privileged access to OpenTelemetry APIs, and as such they have no exclusive advantage over any other auto-instrumentation libraries.</p>
<p><strong>What about auto-instrumenting <em>Project X</em>? Why aren't we using that instead??</strong> First of all, there's nothing preventing any of us from taking great ideas from <em>Project X</em> and incorporating them into these auto-instrumentation libraries. We propose that we start with the Datadog codebases and iterate from there as need be. If there are improvements to be made in any given language, they will be welcomed by all.</p>
<h2><a class="header" href="#prior-art-and-alternatives" id="prior-art-and-alternatives">Prior art and alternatives</a></h2>
<p>There are many proprietary APM language agents – no need to survey them all here. There is a much smaller list of &quot;APM agents&quot; (or other auto-instrumentation efforts) that are already permissively-licensed OSS. For instance, when we met to discuss options for JVM (longer notes <a href="https://docs.google.com/document/d/1ix0WtzB5j-DRj1VQQxraoqeUuvgvfhA6Sd8mF5WLNeY/edit#heading=h.kjctiyv4rxup">here</a>), we came away with the following list:</p>
<ul>
<li><a href="https://github.com/honeycombio/beeline-java">Honeycomb's Java beeline</a></li>
<li><a href="https://github.com/datadog/dd-trace-java">Datadog's Java tracer</a></li>
<li><a href="https://glowroot.org/">Glowroot</a></li>
<li><a href="https://github.com/opentracing-contrib/java-specialagent">SpecialAgent</a></li>
</ul>
<p>The most obvious &quot;alternative approach&quot; would be to choose &quot;starting points&quot; independently in each language. This has several problems:</p>
<ul>
<li>Higher likelihood of &quot;hard forks&quot;: we want to avoid an end state where two projects (the OpenTelemetry version, and the original version) evolve – and diverge – independently</li>
<li>Higher likelihood of &quot;concept divergence&quot; across languages: while each language presents unique requirements and challenges, the Datadog auto-instrumentation libraries were written by a single organization with some common concepts and architectural requirements (they were also written to be OpenTracing-compatible, which greatly increases our odds of success given the similarities to OpenTelemetry)</li>
<li>Datadog would also like a uniform strategy here, and this donation requires their consent (unless we want to do a hard fork, which is suboptimal for everyone). So starting with the Datadog libraries in &quot;all but one&quot; (or &quot;all but two&quot;, etc) languages makes this less palatable for them</li>
</ul>
<h1><a class="header" href="#remove-spandata" id="remove-spandata">Remove SpanData</a></h1>
<p>Remove and replace SpanData by adding span start and end options.</p>
<h2><a class="header" href="#motivation-1" id="motivation-1">Motivation</a></h2>
<p>SpanData represents an immutable span object, creating a fairly large API for all of the fields (12 to be exact). It exposes what feels like an SDK concern and implementation detail to the API surface. As a user, this is another API I need to learn how to use, and ID generation might also need to be exposed. As an implementer, it is a new data type that needs to be supported. The primary motivation for removing SpanData revolves around the desire to reduce the size of the tracing API.</p>
<h2><a class="header" href="#explanation" id="explanation">Explanation</a></h2>
<p>SpanData has a couple of use cases.</p>
<p>The first use case revolves around creating a span synchronously but needing to change the start time to a more accurate timestamp. For example, in an HTTP server, you might record the time the first byte was received, parse the headers, determine the span name, and then create the span. The moment the span was created isn't representative of when the request actually began, so the time the first byte was received would become the span's start time. Since the current API doesn't allow start timestamps, you'd need to create a SpanData object. The big downside is that you don't end up with an active span object.</p>
<p>The second use case comes from the need to construct and report out of band spans, meaning that you're creating &quot;custom&quot; spans for an operation you don't own. One good example of this is a span sink that takes in structured logs that contain correlation IDs and a duration (e.g. from splunk) and <a href="https://github.com/lightstep/splunktospan/blob/58ef9bca81933a47605a51047b12742e2d13aa8f/splunktospan/span.py#L43">converts them</a> to spans for your tracing system. <a href="https://github.com/lightstep/haproxy_log2span/blob/761da5bf3e4b6cce56039d65439ae7db57f48603/lib/lib.go#L292">Another example</a> is running a sidecar on an HAProxy machine, tailing the request logs, and creating spans. SpanData allows you to report the out of band reporting case, whereas you can’t with the current Span API as you cannot set the start and end timestamp.</p>
<p>I'd like to propose getting rid of SpanData and <code>tracer.recordSpanData()</code> and replacing it by allowing <code>tracer.startSpan()</code> to accept a start timestamp option and <code>span.end()</code> to accept end timestamp option. This reduces the API surface, consolidating on a single span type. Options would meet the requirements for out of band reporting.</p>
<h2><a class="header" href="#internal-details" id="internal-details">Internal details</a></h2>
<p><code>startSpan()</code> would change so you can include an optional start timestamp, span ID, and resource. When you have a span sink, out of band spans may have different resources than the tracer they are being reported to, so you want to pass an explicit resource. For <code>span.end()</code> you would have an optional end timestamp. The exact implementation would be language specific, so some would use an options pattern with function overloading or variadic parameters, or add these options to the span builder.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-1" id="trade-offs-and-mitigations-1">Trade-offs and mitigations</a></h2>
<p>From <a href="https://github.com/open-telemetry/opentelemetry-specification/issues/71">https://github.com/open-telemetry/opentelemetry-specification/issues/71</a>: If the underlying SDK automatically adds tags to spans such as thread-id, stacktrace, and cpu-usage when a span is started, they would be incorrect for out of band spans as the tracer would not know the difference between in and out of band spans. This can be mitigated by indicating that the span is out of band to prevent attaching incorrect information, possibly with an <code>isOutOfBand()</code> option on <code>startSpan()</code>.</p>
<h2><a class="header" href="#prior-art-and-alternatives-1" id="prior-art-and-alternatives-1">Prior art and alternatives</a></h2>
<p>The OpenTracing specification for <code>tracer.startSpan()</code> includes an optional start timestamp and zero or more tags. It also calls out an optional end timestamp and bulk logging for <code>span.end()</code>.</p>
<h2><a class="header" href="#open-questions" id="open-questions">Open questions</a></h2>
<p>There also seems to be some hidden dependency between SpanData and the sampler API. For example, given a complete SpanData object with a start and end timestamp, I imagine there's a use case where the sampler can look at the that and decide &quot;this took a long time&quot; and sample it. Is this a real use case? Is there a requirement to be able to provide complete span objects to the sampler?</p>
<h2><a class="header" href="#future-work" id="future-work">Future Work</a></h2>
<p>We might want to include attributes as a start option to give the underlying sampler more information to sample with. We also might want to include optional events, which would be for bulk adding events with explicit timestamps.</p>
<p>We will also want to ensure, assuming the span or subtrace is being created in the same process, that the timestamps use the same precision and are monotonic.</p>
<h2><a class="header" href="#related-issues" id="related-issues">Related Issues</a></h2>
<p>Removing SpanData would resolve <a href="https://github.com/open-telemetry/opentelemetry-specification/issues/71">open-telemetry/opentelemetry-specification#71</a>.</p>
<p>Options would solve <a href="https://github.com/open-telemetry/opentelemetry-specification/issues/139">open-telemetry/opentelemetry-specification#139</a>.</p>
<p>By removing SpanData, <a href="https://github.com/open-telemetry/opentelemetry-specification/issues/92">open-telemetry/opentelemetry-specification#92</a> can be resolved and closed.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/68">open-telemetry/opentelemetry-specification#68</a> can be closed. An optional resource can provide a different resource for out of band spans, otherwise the tracer can provide the default resource.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/60">open-telemetry/opentelemetry-specification#60</a> can be closed due to removal of SpanData.</p>
<h1><a class="header" href="#consolidate-pre-aggregated-and-raw-metrics-apis" id="consolidate-pre-aggregated-and-raw-metrics-apis">Consolidate pre-aggregated and raw metrics APIs</a></h1>
<h2><a class="header" href="#foreword" id="foreword">Foreword</a></h2>
<p>A working group convened on 8/21/2019 to discuss and debate the two metrics RFCs (0003 and 0004) and several surrounding concerns.  This document has been revised with related updates that were agreed upon during this working session.  See the <a href="https://docs.google.com/document/d/1d0afxe3J6bQT-I6UbRXeIYNcTIyBQv4axfjKF4yvAPA/edit#">meeting notes</a>.</p>
<h2><a class="header" href="#overview" id="overview">Overview</a></h2>
<p>Introduce a <code>Measure</code> kind of metric object that supports a <code>Record</code> API method.  Like existing <code>Gauge</code> and <code>Cumulative</code> metrics, the new <code>Measure</code> metric supports pre-defined labels.  A new <code>RecordBatch</code> measurement API  is introduced for recording multiple metric observations simultaneously.</p>
<h2><a class="header" href="#terminology" id="terminology">Terminology</a></h2>
<p>This RFC changes how &quot;Measure&quot; is used in the OpenTelemetry metrics specification.  Before, &quot;Measure&quot; was the name of a series of raw measurements.  After, &quot;Measure&quot; is the kind of a metric object used for recording a series raw measurements.</p>
<p>Since this document will be read in the future after the proposal has been written, uses of the word &quot;current&quot; lead to confusion.  For this document, the term &quot;preceding&quot; refers to the state that was current prior to these changes.</p>
<p>The preceding specification used the term <code>TimeSeries</code> to describe an instrument bound with a set of pre-defined labels.  In this document, <a href="0009-metric-handles.html">the term &quot;Handle&quot; is used to describe an instrument with bound labels</a>.  In a future OTEP this will be again changed to &quot;Bound instrument&quot;.  The term &quot;Handle&quot; is used throughout this document to refer to a bound instrument.</p>
<h2><a class="header" href="#motivation-2" id="motivation-2">Motivation</a></h2>
<p>In the preceding <code>Metric.GetOrCreateTimeSeries</code> API for Gauges and Cumulatives, the caller obtains a <code>TimeSeries</code> handle for repeatedly recording metrics with certain pre-defined label values set.  This enables an important optimization for exporting pre-aggregated metrics, since the implementation is able to compute the aggregate summary &quot;entry&quot; using a pointer or fast table lookup. The efficiency gain requires that the aggregation keys be a subset of the pre-defined labels.</p>
<p>Application programs with long-lived objects and associated Metrics can take advantage of pre-defined labels by computing label values once per object (e.g., in a constructor), rather than once per call site. In this way, the use of pre-defined labels improves the usability of the API as well as makes an important optimization possible to the implementation.</p>
<p>The preceding raw statistics API did not specify support for pre-defined labels.  This RFC replaces the raw statistics API by a new, general-purpose kind of metric, <code>MeasureMetric</code>, generally intended for recording individual measurements like the preceding raw statistics API, with explicit support for pre-defined labels.</p>
<p>The preceding raw statistics API supported all-or-none recording for interdependent measurements using a common label set.  This RFC introduces a <code>RecordBatch</code> API to support recording batches of measurements in a single API call, where a <code>Measurement</code> is now defined as a pair of <code>MeasureMetric</code> and <code>Value</code> (integer or floating point).</p>
<h2><a class="header" href="#explanation-1" id="explanation-1">Explanation</a></h2>
<p>The common use for <code>MeasureMetric</code>, like the preceding raw statistics API, is for reporting information about rates and distributions over structured, numerical event data.  Measure metrics are the most general-purpose of metrics.  Informally, the individual metric event has a logical format expressed as one primary key=value (the metric name and a numerical value) and any number of secondary key=values (the labels, resources, and context).</p>
<pre><code>metric_name=_number_
pre_defined1=_any_value_
pre_defined2=_any_value_
...
resource1=_any_value_
resource2=_any_value_
...
context_tag1=_any_value_
context_tag2=_any_value_
...
</code></pre>
<p>Here, &quot;pre_defined&quot; keys are those captured in the metrics handle, &quot;resource&quot; keys are those configured when the SDK was initialized, and &quot;context_tag&quot; keys are those propagated via context.</p>
<p>Events of this form can logically capture a single update to a named metric, whether a cumulative, gauge, or measure kind of metric.  This logical structure defines a <em>low-level encoding</em> of any metric event, across the three kinds of metric.  An SDK could simply encode a stream of these events and the consumer, provided access to the metric definition, should be able to interpret these events according to the semantics prescribed for each kind of metric.</p>
<h2><a class="header" href="#metrics-api-concepts" id="metrics-api-concepts">Metrics API concepts</a></h2>
<p>The <code>Meter</code> interface represents the metrics portion of the OpenTelemetry API.</p>
<p>There are three kinds of metric instrument, <code>CumulativeMetric</code>, <code>GaugeMetric</code>, and <code>MeasureMetric</code>.</p>
<p>Metric instruments are constructed through the <code>Meter</code> API. Constructing an instrument automatically registers it with the SDK. The common attributes of a metric instrument are:</p>
<table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody>
<tr><td>Name</td><td>A string.</td></tr>
<tr><td>Kind</td><td>One of Cumulative, Gauge, or Measure.</td></tr>
<tr><td>Recommended Keys</td><td>Default aggregation keys.</td></tr>
<tr><td>Unit</td><td>The unit of measurement being recorded.</td></tr>
<tr><td>Description</td><td>Information about this metric.</td></tr>
</tbody></table>
<p>See the specification for more information on these fields, including formatting and uniqueness requirements.  To define a new metric, use one of the <code>Meter</code> API methods (e.g., with names like <code>NewCumulativeMetric</code>, <code>NewGaugeMetric</code>, or <code>NewMeasureMetric</code>).</p>
<p>Metric instrument Handles combine a metric instrument with a set of pre-defined labels.  Handles are obtained by calling a language-specific API method (e.g., <code>GetHandle</code>) on the metric instrument with certain label values.  Handles may be used to <code>Set()</code>, <code>Add()</code>, or <code>Record()</code> metrics according to their kind.</p>
<h2><a class="header" href="#selecting-metric-kind" id="selecting-metric-kind">Selecting Metric Kind</a></h2>
<p>By separation of API and implementation in OpenTelemetry, we know that an implementation is free to do <em>anything</em> in response to a metric API call.  By the low-level interpretation defined above, all metric events have the same structural representation, only their logical interpretation varies according to the metric definition.  Therefore, we select metric kinds based on two primary concerns:</p>
<ol>
<li>What should be the default implementation behavior?  Unless configured otherwise, how should the implementation treat this metric variable?</li>
<li>How will the program source code read?  Each metric uses a different verb, which helps convey meaning and describe default behavior.  Cumulatives have an <code>Add()</code> method.  Gauges have a <code>Set()</code> method.  Measures have a <code>Record()</code> method.</li>
</ol>
<p>To guide the user in selecting the right kind of metric for an application, we'll consider the following questions about the primary intent of reporting given data.  We use &quot;of primary interest&quot; here to mean information that is almost certainly useful in understanding system behavior.  Consider these questions:</p>
<ul>
<li>Does the measurement represent a quantity of something?  Is it also non-negative?</li>
<li>Is the sum a matter of primary interest?</li>
<li>Is the event count a matter of primary interest?</li>
<li>Is the distribution (p50, p99, etc.) a matter of primary interest?</li>
</ul>
<p>The specification will be updated with the following guidance.</p>
<h3><a class="header" href="#cumulative-metric" id="cumulative-metric">Cumulative metric</a></h3>
<p>Likely to be the most common kind of metric, cumulative metric events express the computation of a sum.  Choose this kind of metric when the value is a quantity, the sum is of primary interest, and the event count and distribution are not of primary interest.  To raise (or lower) a cumulative metric, call the <code>Add()</code> method.</p>
<p>If the quantity in question is always non-negative, it implies that the sum is monotonic.  This is the common case, <code>Monotonic(true)</code>, where cumulative sums only rise, and these metric instruments serve to compute a rate.  For this reason, cumulative metrics have a <code>Monotonic(false)</code> option to be declared as allowing negative inputs, the uncommon case.  The SDK should reject negative inputs to monotonic cumulative metrics, but it is not required to.</p>
<p>For cumulative metrics, the default OpenTelemetry implementation exports the sum of event values taken over an interval of time.</p>
<h3><a class="header" href="#gauge-metric" id="gauge-metric">Gauge metric</a></h3>
<p>Gauge metrics express a pre-calculated value that is either <code>Set()</code> by explicit instrumentation or observed through a callback.  Generally, this kind of metric should be used when the metric cannot be expressed as a sum or a rate because the measurement interval is arbitrary.  Use this kind of metric when the measurement is a computed value and the sum and event count are not of interest.</p>
<p>Only the gauge kind of metric supports observing the metric via a gauge <code>Observer</code> callback (as an option, see <code>0008-metric-observer.md</code>).  Semantically, there is an important difference between explicitly setting a gauge and observing it through a callback.  In case of setting the gauge explicitly, the <code>Set()</code> call happens inside of an implicit or explicit context.  The implementation is free to associate the explicit <code>Set()</code> event with a context, for example.  When observing gauge metrics via a callback, there is no context associated with the event.</p>
<p>As a special case, to support existing metrics infrastructure and the <code>Observer</code> pattern, a gauge metric may be declared as a precomputed, monotonic sum using the <code>Monotonic(true)</code> option, in which case it is may be used to define a rate.  The initial value is presumed to be zero.  The SDK should reject descending updates to monotonic gauges, but it is not required to.</p>
<p>For gauge metrics, the default OpenTelemetry implementation exports the last value that was explicitly <code>Set()</code>, or if using a callback, the current value from the <code>Observer</code>.</p>
<h3><a class="header" href="#measure-metric" id="measure-metric">Measure metric</a></h3>
<p>Measure metrics express a distribution of measured values.  This kind of metric should be used when the count or rate of events is meaningful and either:</p>
<ol>
<li>The sum is of interest in addition to the count (rate)</li>
<li>Quantile information is of interest.</li>
</ol>
<p>The key property of a measure metric event is that computing quantiles and/or summarizing a distribution (e.g., via a histogram) may be expensive.  Not only will implementations have various capabilities and algorithms for this task, users may wish to control the quality and cost of aggregating measure metrics.</p>
<p>Like cumulative metrics, non-negative measures are an important case because they support rate calculations.  Measure metrics are described as <code>Absolute(true)</code> when the inputs are non-negative.  As an option, measure metrics may be declared as <code>Absolute(false)</code> to support positive and negative values.  The SDK should reject negative measurements for Absolute measures, but it is not required to.</p>
<h3><a class="header" href="#option-to-disable-metrics-by-default" id="option-to-disable-metrics-by-default">Option to disable metrics by default</a></h3>
<p>Metric instruments are enabled by default, meaning that SDKs will export metric data for this instrument without configuration.  Metric instruments support a <code>Disabled</code> option, marking them as verbose sources of information that may be configured on an as-needed basis to control cost (e.g., using a &quot;views&quot; API).</p>
<h3><a class="header" href="#kind-specific-option-summary" id="kind-specific-option-summary">Kind-specific option summary</a></h3>
<p>The kind-specific optional properties of a metric instrument are:</p>
<table><thead><tr><th>Property</th><th>Description</th><th>Metric kind</th></tr></thead><tbody>
<tr><td>Monotonic(true)</td><td>Indicates a cumulative that accepts only non-negative values</td><td>Cumulative (default)</td></tr>
<tr><td></td><td>Indicate a gauge supports ascending value sequences starting at 0</td><td>Gauge</td></tr>
<tr><td>Monotonic(false)</td><td>Indicates a cumulative that accepts positive and negative values</td><td>Cumulative</td></tr>
<tr><td></td><td>Indicate a gauge that expresses a monotonic cumulative value</td><td>Gauge (default)</td></tr>
<tr><td>Absolute(true)</td><td>Indicates a measure that accepts non-negative values</td><td>Measure (default)</td></tr>
<tr><td>Absolute(false)</td><td>Indicates a measure that accepts positive and negative values</td><td>Measure</td></tr>
</tbody></table>
<h3><a class="header" href="#recordbatch-api" id="recordbatch-api">RecordBatch API</a></h3>
<p>Applications sometimes want to act upon multiple metric instruments in a single API call, either because the values are inter-related to each other, or because it lowers overhead.  RecordBatch logically updates each instrument in the batch using the supplied value.  A single label set applies to the batch.</p>
<p>A single measurement is defined as:</p>
<ul>
<li>Instrument: the measure instrument (not a Handle)</li>
<li>Value: the recorded floating point or integer data</li>
</ul>
<p>The batch measurement API uses a language-specific method name (e.g., <code>RecordBatch</code>).  The entire batch of measurements takes place within a (implicit or explicit) context.</p>
<h2><a class="header" href="#prior-art-and-alternatives-2" id="prior-art-and-alternatives-2">Prior art and alternatives</a></h2>
<p>Prometheus supports the notion of vector metrics, which are those that support pre-defined labels for a specific set of required keys.  The vector-metric API supports a variety of methods like <code>WithLabelValues</code> to associate labels with a metric handle, similar to <code>GetHandle</code> in OpenTelemetry.  As in this proposal, Prometheus supports a vector API for all metric types.</p>
<h2><a class="header" href="#open-questions-1" id="open-questions-1">Open questions</a></h2>
<h3><a class="header" href="#gethandle-argument-ordering" id="gethandle-argument-ordering"><code>GetHandle</code> argument ordering</a></h3>
<p>Argument ordering has been proposed as the way to pass pre-defined label values in <code>GetHandle</code>.  The argument list must match the parameter list exactly, and if it doesn't we generally find out at runtime or not at all.  This model has more optimization potential, but is easier to misuse than the alternative.  The alternative approach is to always pass label:value pairs to <code>GetOrCreateTimeseries</code>, as opposed to an ordered list of values.</p>
<h3><a class="header" href="#recordbatch-argument-ordering" id="recordbatch-argument-ordering"><code>RecordBatch</code> argument ordering</a></h3>
<p>The discussion above can be had for the proposed <code>RecordBatch</code> method.  It can be declared with an ordered list of metrics, then the <code>Record</code> API takes only an ordered list of numbers.  Alternatively, and less prone to misuse, the <code>RecordBatch</code> API has been declared with a list of metric:number pairs.</p>
<h3><a class="header" href="#eliminate-getdefaulthandle" id="eliminate-getdefaulthandle">Eliminate <code>GetDefaultHandle()</code></a></h3>
<p>Instead of a mechanism to obtain a default handle, some languages may prefer to simply operate on the metric instrument directly in this case.  Should OpenTelemetry eliminate <code>GetDefaultHandle</code> and instead specify that cumulative, gauge, and measure metric instruments implement <code>Add()</code>, <code>Set()</code>, and <code>Record()</code> with the same interpretation?</p>
<p>If we eliminate <code>GetDefaultHandle()</code>, the SDK may keep a map of metric instrument to default handle on its own.</p>
<h3><a class="header" href="#recordbatch-support-for-all-metrics" id="recordbatch-support-for-all-metrics"><code>RecordBatch</code> support for all metrics</a></h3>
<p>In the 8/21 working session, we agreed to limit <code>RecordBatch</code> to recording of simultaneous measure metrics, meaning to exclude cumulatives and gauges from batch recording.  There are arguments in favor of supporting batch recording for all metric instruments.</p>
<ul>
<li>If atomicity (i.e., the all-or-none property) is the reason for batch reporting, it makes sense to include all the metric instruments in the API</li>
<li><code>RecordBatch</code> support for cumulatives and gauges will be natural for SDKs that act as forwarders for metric events . The natural implementation for <code>Add()</code> and <code>Set()</code> methods will be <code>RecordBatch</code> with a single event.</li>
<li>Likewise, it is simple for an SDK that acts as an aggregator (not a forwarder) to redirect <code>Add()</code> and <code>Set()</code> APIs to the handle-specific <code>Add()</code> and <code>Set()</code> methods; while the SDK, as the implementation, still may (not must) treat these cumulative and gauge updates as atomic.</li>
</ul>
<p>Arguments against batch recording for all metric instruments:</p>
<ul>
<li>The <code>Record</code> in <code>RecordBatch</code> suggests it is to be applied to measure metrics.  This is due to measure metrics being the most general-purpose of metric instruments.</li>
</ul>
<h2><a class="header" href="#issues-addressed" id="issues-addressed">Issues addressed</a></h2>
<p><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/83">Raw vs. other metrics / measurements are unclear</a></p>
<p><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/145">Eliminate Measurement class to save on allocations</a></p>
<p><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/146">Implement three more types of Metric</a></p>
<h1><a class="header" href="#global-sdk-initialization" id="global-sdk-initialization">Global SDK initialization</a></h1>
<p><strong>Status</strong>: proposed</p>
<p>Specify the behavior of OpenTelemetry APIs and implementations at startup.</p>
<h2><a class="header" href="#motivation-3" id="motivation-3">Motivation</a></h2>
<p>OpenTelemetry is designed with a separation between the API and the
SDK which implements it, allowing an application to configure and bind
any compatible SDK at runtime.  OpenTelemetry is designed to support
&quot;zero touch&quot; instrumentation for third party libraries through the use
of a global instance.</p>
<p>In many programming environments, it is possible for libraries of code
to auto-initialize, allowing them to begin operation concurrently with
the main program, e.g., while initializing static program state.  This
presents a set of opposing requirements: (1) the API supports a
configurable SDK; (2) third party libraries may use OpenTelemetry
without configuration.</p>
<h2><a class="header" href="#explanation-2" id="explanation-2">Explanation</a></h2>
<p>There are several acceptable ways to address this situation.  The
feasibility of each approach varies by language.  The implementation
must select one of the following strategies:</p>
<h3><a class="header" href="#service-provider-mechanism" id="service-provider-mechanism">Service provider mechanism</a></h3>
<p>Where the language provides a commonly accepted way to inject SDK
components, it should be preferred.  The Java SPI supports loading and
configuring the global SDK before it is first used, and because of
this property the service provider mechanism case leaves little else
to specify.</p>
<h3><a class="header" href="#explicit-initializer" id="explicit-initializer">Explicit initializer</a></h3>
<p>When it is not possible to ensure the SDK is installed and configured
before the API is first used, loading the SDK is handed off to the
user &quot;at the right time&quot;, as stated in <a href="https://github.com/open-telemetry/opentelemetry-ruby/issues/19">Ruby issue
19</a>.
In this case, a number of requirements must be specified, as discussed
next.</p>
<h2><a class="header" href="#requirements-explicit-initializer" id="requirements-explicit-initializer">Requirements: Explicit initializer</a></h2>
<p>OpenTelemetry specifies that the default implementation is
non-operational (i.e., a &quot;no-op&quot;), requiring that API method calls
result in effectively zero instrumentation overhead.  We expect third
party libraries to use the global SDK before it is installed, which is
addressed in a requirement stated below.</p>
<p>The explicit initializer method should take independent <code>Tracer</code> and
<code>Meter</code> objects (e.g., <code>opentelemetry.Init(Tracer, Meter)</code>).  The SDK
may be installed no more than once.  After the first SDK installed,
subsequent calls to the explicit initializer shall log console
warnings.</p>
<p>In common language, uses of the global SDK instance (i.e., the Tracer
and Meter) must &quot;begin working&quot; once the SDK is installed, with the
following stipulations:</p>
<h3><a class="header" href="#tracer" id="tracer">Tracer</a></h3>
<p>There may be loss of spans at startup.</p>
<p>Spans that are started before the SDK is installed are not recovered,
they continue as No-op spans.</p>
<h3><a class="header" href="#meter" id="meter">Meter</a></h3>
<p>There may be loss of metrics at startup.</p>
<p>Metric SubMeasure objects (i.e., metrics w/ predefined labels)
initialized before the SDK is installed will redirect to the global
SDK after it is installed.</p>
<h3><a class="header" href="#concrete-types" id="concrete-types">Concrete types</a></h3>
<p>Keys, tags, attributes, labels, resources, span context, and
distributed context are specified as pure API objects, therefore do
not depend on the SDK being installed.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-2" id="trade-offs-and-mitigations-2">Trade-offs and mitigations</a></h2>
<h3><a class="header" href="#testing-support" id="testing-support">Testing support</a></h3>
<p>Testing should be performed without depending on the global SDK.</p>
<h3><a class="header" href="#synchronization" id="synchronization">Synchronization</a></h3>
<p>Since the global Tracer and Meter objects are required to begin
working once the SDK is installed, there is some implied
synchronization overhead at startup, overhead we expect to fall after
the SDK is installed.  We recommend explicitly installing a No-op SDK
to fully disable instrumentation, as this approach will have a lower
overhead than leaving the OpenTelemetry library uninitialized.</p>
<h2><a class="header" href="#prior-art-and-alternatives-3" id="prior-art-and-alternatives-3">Prior art and alternatives</a></h2>
<p>As an example that does not qualify as &quot;commonly accepted&quot;, see <a href="https://github.com/open-telemetry/opentelemetry-go/issues/52">Go
issue 52</a>
which demonstrates using the Go <code>plugin</code> package to load a
configurable SDK prior to first use.</p>
<h2><a class="header" href="#open-questions-2" id="open-questions-2">Open questions</a></h2>
<p>What other options should be passed to the explicit global initializer?</p>
<p>Is there a public test for &quot;is the SDK installed; is it a no-op&quot;?</p>
<h1><a class="header" href="#sampling-api" id="sampling-api">Sampling API</a></h1>
<h2><a class="header" href="#tldr" id="tldr">TL;DR</a></h2>
<p>This section tries to summarize all the changes proposed in this RFC:</p>
<ol>
<li>Move the <code>Sampler</code> interface from the API to SDK package. Apply some minor changes to the
<code>Sampler</code> API.</li>
<li>Add capability to record <code>Attributes</code> that can be used for sampling decision during the <code>Span</code>
creation time.</li>
<li>Remove <code>addLink</code> APIs from the <code>Span</code> interface, and allow recording links only during the span
construction time.</li>
</ol>
<h2><a class="header" href="#motivation-4" id="motivation-4">Motivation</a></h2>
<p>Different users of OpenTelemetry, ranging from library developers, packaged infrastructure binary
developers, application developers, operators, and telemetry system owners, have separate use cases
for OpenTelemetry that have gotten muddled in the design of the original Sampling API. Thus, we need
to clarify what APIs each should be able to depend upon, and how they will configure sampling and
OpenTelemetry according to their needs.</p>
<pre><code>
                    +----------+           +-----------+
           grpc     |  Library |           |           |
           Django   |  Devs    +----------&gt;| OTel API  |
           Express  |          |   +------&gt;|           |
                    +----------+   |  +---&gt;+-----------+                  +---------+
                                   |  |          ^                        | OTel    |
                                   |  |          |                     +-&gt;| Proxy   +---+
                                   |  |          |                     |  |         |   |
                    +----------+   |  |    +-----+-----+------------+  |  +---------+   |
                    |          |   |  |    |           | OTel Wire  |  |                |
           Hbase    |  Infra   |   |  |    |           | Export     |+-+                v
           Envoy    |  Binary  +---+  |    |  OTel     |            |  |           +----v-----+
                    |  Devs    |      |    |  SDK      +------------+  |           |          |
                    +----------+----------&gt;|           |            |  +----------&gt;|  Backend |
                                   +------&gt;|           | Custom     |  +----------&gt;|          |
                                   |  |    |           | Export     |  |           +----------+
                    +----------+   |  |    |           |            |+-+             ^
                    |          +---+  |    +-----------+------------+                |
                    |  App     +------+       ^              ^                       |
                    |  Devs    +              |              |          +------------+-+
                    |          |              |              |          |              |
                    +----------+          +---+----+         +----------+   Telemetry  |
                                          |  SRE   |                    |   Owner      |
                                          |        |                    |              |
                                          +--------+                    +--------------+
                                                                          Lightstep
                                                                          Honeycomb

</code></pre>
<h2><a class="header" href="#explanation-3" id="explanation-3">Explanation</a></h2>
<p>We outline five different use cases (who may be overlapping sets of people), and how they should
interact with OpenTelemetry:</p>
<h3><a class="header" href="#library-developer" id="library-developer">Library developer</a></h3>
<p>Examples: gRPC, Express, Django developers.</p>
<ul>
<li>They must only depend upon the OpenTelemetry API and not upon the SDK.
<ul>
<li>For testing only they may depend on the SDK with InMemoryExporter.</li>
</ul>
</li>
<li>They are shipping source code that will be linked into others' applications.</li>
<li>They have no explicit runtime control over the application.</li>
<li>They know some signal about what traces may be interesting (e.g. unusual control plane requests)
or uninteresting (e.g. health-checks), but have to write fully generically.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>For the moment, the OpenTelemetry API will not offer any <code>SamplingHint</code> functionality for the last use case.
This is intentional to avoid premature optimizations, and it is based on the fact that changing an API is
backwards incompatible compared to adding a new API.</li>
</ul>
<h3><a class="header" href="#infrastructure-packagebinary-developer" id="infrastructure-packagebinary-developer">Infrastructure package/binary developer</a></h3>
<p>Examples: HBase, Envoy developers.</p>
<ul>
<li>They are shipping self-contained binaries that may accept YAML or similar run-time configuration,
but are not expected to support extensibility/plugins beyond the default OpenTelemetry SDK,
OpenTelemetry SDKTracer, and OpenTelemetry wire format exporter.</li>
<li>They may have their own recommendations for sampling rates, but don't run the binaries in
production, only provide packaged binaries. So their sampling rate configs, and sampling strategies
need to be a finite &quot;built in&quot; set from OpenTelemetry's SDK.</li>
<li>They need to deal with upstream sampling decisions made by services that call them.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>Allow different sampling strategies by default in OpenTelemetry SDK, all configurable easily via
YAML or feature flags. See <a href="0006-sampling.html#default-samplers">default samplers</a>.</li>
</ul>
<h3><a class="header" href="#application-developer" id="application-developer">Application developer</a></h3>
<p>These are the folks we've been thinking the most about for OpenTelemetry in general.</p>
<ul>
<li>They have full control over the OpenTelemetry implementation or SDK configuration. When using the
SDK they can configure custom exporters, custom code/samplers, etc.</li>
<li>They can choose to implement runtime configuration via a variety of means (e.g. baking in feature
flags, reading YAML files, etc.), or even configure the library in code.</li>
<li>They make heavy usage of OpenTelemetry for instrumenting application-specific behavior, beyond
what may be provided by the libraries they use such as gRPC, Django, etc.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>Allow application developers to link in custom samplers or write their own when using the
official SDK.
<ul>
<li>These might include dynamic per-field sampling to achieve a target rate
(e.g. <a href="https://github.com/honeycombio/dynsampler-go">https://github.com/honeycombio/dynsampler-go</a>)</li>
</ul>
</li>
<li>Sampling decisions are made within the start Span operation, after attributes relevant to the
span have been added to the Span start operation but before a concrete Span object exists (so that
either a NoOpSpan can be made, or an actual Span instance can be produced depending upon the
sampler's decision).</li>
<li>Span.IsRecording() needs to be present to allow costly span attribute/log computation to be
skipped if the span is a NoOp span.</li>
</ul>
<h3><a class="header" href="#application-operator" id="application-operator">Application operator</a></h3>
<p>Often the same people as the application developers, but not necessarily</p>
<ul>
<li>They care about adjusting sampling rates and strategies to meet operational needs, debugging,
and cost.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>Use config files or feature flags written by the application developers to control the
application sampling logic.</li>
<li>Use the config files to configure libraries and infrastructure package behavior.</li>
</ul>
<h3><a class="header" href="#telemetry-infrastructure-owner" id="telemetry-infrastructure-owner">Telemetry infrastructure owner</a></h3>
<p>They are the people who provide an implementation for the OpenTelemetry API by using the SDK with
custom <code>Exporter</code>s, <code>Sampler</code>s, hooks, etc. or by writing a custom implementation, as well as
running the infrastructure for collecting exported traces.</p>
<ul>
<li>They care about a variety of things, including efficiency, cost effectiveness, and being able to
gather spans in a way that makes sense for them.</li>
</ul>
<p><strong>Solution:</strong></p>
<ul>
<li>Infrastructure owners receive information attached to the span, after sampling hooks have already
been run.</li>
</ul>
<h2><a class="header" href="#internal-details-1" id="internal-details-1">Internal details</a></h2>
<p>In Dapper based systems (or systems without a deferred sampling decision) all exported spans are
stored to the backend, thus some of these systems usually don't scale to a high volume of traces,
or the cost to store all the Spans may be too high. In order to support this use-case and to
ensure the quality of the data we send, OpenTelemetry needs to natively support sampling with some
requirements:</p>
<ul>
<li>Send as many complete traces as possible. Sending just a subset of the spans from a trace is
less useful because in this case the interaction between the spans may be missing.</li>
<li>Allow application operator to configure the sampling frequency.</li>
</ul>
<p>For new modern systems that need to collect all the Spans and later may or may not make a deferred
sampling decision, OpenTelemetry needs to natively support a way to configure the library to
collect and export all the Spans. This is possible (even though OpenTelemetry supports sampling) by
setting a default config to always collect all the spans.</p>
<h3><a class="header" href="#sampling-flags" id="sampling-flags">Sampling flags</a></h3>
<p>OpenTelemetry API has two flags/properties:</p>
<ul>
<li><code>RecordEvents</code>
<ul>
<li>This property is exposed in the <code>Span</code> interface (e.g. <code>Span.isRecordingEvents()</code>).</li>
<li>If <code>true</code> the current <code>Span</code> records tracing events (attributes, events, status, etc.),
otherwise all tracing events are dropped.</li>
<li>Users can use this property to determine if expensive trace events can be avoided.</li>
</ul>
</li>
<li><code>SampledFlag</code>
<ul>
<li>This flag is propagated via the <code>TraceOptions</code> to the child Spans (e.g.
<code>TraceOptions.isSampled()</code>). For more details see the w3c definition <a href="https://github.com/w3c/trace-context/blob/master/spec/20-http_request_header_format.md#trace-flags">here</a>.</li>
<li>In Dapper based systems this is equivalent to <code>Span</code> being <code>sampled</code> and exported.</li>
</ul>
</li>
</ul>
<p>The flag combination <code>SampledFlag == false</code> and <code>RecordEvents == true</code> means that the current <code>Span</code>
does record tracing events, but most likely the child <code>Span</code> will not. This combination is
necessary because:</p>
<ul>
<li>Allow users to control recording for individual Spans.</li>
<li>OpenCensus has this to support z-pages, so we need to keep backwards compatibility.</li>
</ul>
<p>The flag combination <code>SampledFlag == true</code> and <code>RecordEvents == false</code> can cause gaps in the
distributed trace, and because of this OpenTelemetry API should NOT allow this combination.</p>
<p>It is safe to assume that users of the API should only access the <code>RecordEvents</code> property when
instrumenting code and never access <code>SampledFlag</code> unless used in context propagators.</p>
<h3><a class="header" href="#sampler-interface" id="sampler-interface">Sampler interface</a></h3>
<p>The interface for the Sampler class that is available only in the OpenTelemetry SDK:</p>
<ul>
<li><code>TraceID</code></li>
<li><code>SpanID</code></li>
<li>Parent <code>SpanContext</code> if any</li>
<li><code>Links</code></li>
<li>Span name</li>
<li><code>SpanKind</code></li>
<li>Initial set of <code>Attributes</code> for the <code>Span</code> being constructed</li>
</ul>
<p>It produces an output called <code>SamplingResult</code> that includes:</p>
<ul>
<li>A <code>SamplingDecision</code> enum [<code>NOT_RECORD</code>, <code>RECORD</code>, <code>RECORD_AND_PROPAGATE</code>].</li>
<li>A set of span Attributes that will also be added to the <code>Span</code>.
<ul>
<li>These attributes will be added after the initial set of <code>Attributes</code>.</li>
</ul>
</li>
<li>(under discussion in separate RFC) the SamplingRate float.</li>
</ul>
<h3><a class="header" href="#default-samplers" id="default-samplers">Default Samplers</a></h3>
<p>These are the default samplers implemented in the OpenTelemetry SDK:</p>
<ul>
<li>ALWAYS_ON</li>
<li>ALWAYS_OFF</li>
<li>ALWAYS_PARENT
<ul>
<li>Trust parent sampling decision (trusting and propagating parent <code>SampledFlag</code>).</li>
<li>For root Spans (no parent available) returns <code>NOT_RECORD</code>.</li>
</ul>
</li>
<li>Probability
<ul>
<li>Allows users to configure to ignore the parent <code>SampledFlag</code>.</li>
<li>Allows users to configure if probability applies only for &quot;root spans&quot;, &quot;root spans and remote
parent&quot;, or &quot;all spans&quot;.
<ul>
<li>Default is to apply only for &quot;root spans and remote parent&quot;.</li>
<li>Remote parent property should be added to the SpanContext see specs <a href="https://github.com/open-telemetry/opentelemetry-specification/pull/216">PR/216</a></li>
</ul>
</li>
<li>Sample with 1/N probability</li>
</ul>
</li>
</ul>
<p><strong>Root Span Decision:</strong></p>
<table><thead><tr><th>Sampler</th><th>RecordEvents</th><th>SampledFlag</th></tr></thead><tbody>
<tr><td>ALWAYS_ON</td><td><code>True</code></td><td><code>True</code></td></tr>
<tr><td>ALWAYS_OFF</td><td><code>False</code></td><td><code>False</code></td></tr>
<tr><td>ALWAYS_PARENT</td><td><code>False</code></td><td><code>False</code></td></tr>
<tr><td>Probability</td><td><code>Same as SampledFlag</code></td><td><code>Probability</code></td></tr>
</tbody></table>
<p><strong>Child Span Decision:</strong></p>
<table><thead><tr><th>Sampler</th><th>RecordEvents</th><th>SampledFlag</th></tr></thead><tbody>
<tr><td>ALWAYS_ON</td><td><code>True</code></td><td><code>True</code></td></tr>
<tr><td>ALWAYS_OFF</td><td><code>False</code></td><td><code>False</code></td></tr>
<tr><td>ALWAYS_PARENT</td><td><code>ParentSampledFlag</code></td><td><code>ParentSampledFlag</code></td></tr>
<tr><td>Probability</td><td><code>Same as SampledFlag</code></td><td><code>ParentSampledFlag OR Probability</code></td></tr>
</tbody></table>
<h3><a class="header" href="#links" id="links">Links</a></h3>
<p>This RFC proposes that Links will be recorded only during the start <code>Span</code> operation, because:</p>
<ul>
<li>Link's <code>SampledFlag</code> can be used in the sampling decision.</li>
<li>OpenTracing supports adding references only during the <code>Span</code> creation.</li>
<li>OpenCensus supports adding links at any moment, but this was mostly used to record child Links
which are not supported in OpenTelemetry.</li>
<li>Allowing links to be recorded after the sampling decision is made will cause samplers to not
work correctly and unexpected behaviors for sampling.</li>
</ul>
<h3><a class="header" href="#when-does-sampling-happen" id="when-does-sampling-happen">When does sampling happen</a></h3>
<p>The sampling decision will happen before a real <code>Span</code> object is returned to the user, because:</p>
<ul>
<li>If child spans are created they need to know the 'SampledFlag'.</li>
<li>If <code>SpanContext</code> is propagated on the wire the 'SampledFlag' needs to be set.</li>
<li>If user records any tracing event the <code>Span</code> object needs to know if the data are kept or not.
It may be possible to always collect all the events until the sampling decision is made but this is
an important optimization.</li>
</ul>
<p>There are two important use-cases to be considered:</p>
<ul>
<li>All information that may be used for sampling decisions are available at the moment when the
logical <code>Span</code> operation should start. This is the most common case.</li>
<li>Some information that may be used for sampling decision are NOT available at the moment when the
logical <code>Span</code> operation should start (e.g. <code>http.route</code> may be determine later).</li>
</ul>
<p>The current <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/trace/api.md#span-creation">span creation logic</a> facilitates the first use-case very well, but
the second use-case requires users to record the logical <code>start_time</code> and collect all the
information necessarily to start the <code>Span</code> in custom objects, then when all the properties are
available call the span creation API.</p>
<p>The RFC proposes that we keep the current <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/trace/api.md#span-creation">span creation logic</a> as it is and we will
address the delayed sampling in a different RFC when that becomes a high priority.</p>
<p>The SDK must call the <code>Sampler</code> every time a <code>Span</code> is created during the start span operation.</p>
<p><strong>Alternatives considerations:</strong></p>
<ul>
<li>We considered, to offer a delayed span construction mechanism:
<ul>
<li>For languages where a <code>Builder</code> pattern is used to construct a <code>Span</code>, to allow users to
create a <code>Builder</code> where the start time of the Span is considered when the <code>Builder</code> is created.</li>
<li>For languages where no intermediate object is used to construct a <code>Span</code>, to allow users maybe
via a <code>StartSpanOption</code> object to start a <code>Span</code>. The <code>StartSpanOption</code> allows users to set all
the start <code>Span</code> properties.</li>
<li>Pros:
<ul>
<li>Would resolve the second use-case posted above.</li>
</ul>
</li>
<li>Cons:
<ul>
<li>We could not identify too many real case examples for the second use-case and decided to
postpone the decision to avoid premature decisions.</li>
</ul>
</li>
</ul>
</li>
<li>We considered, instead of requiring that sampling decision happens before the <code>Span</code> is
created to add an explicit <code>MakeSamplingDecision(SamplingHint)</code> on the <code>Span</code>. Attempts to create
a child <code>Span</code>, or to access the <code>SpanContext</code> would fail if <code>MakeSamplingDecision()</code> had not yet
been run.
<ul>
<li>Pros:
<ul>
<li>Simplifies the case when all the attributes that may be used for sampling are not available
when the logical <code>Span</code> operation should start.</li>
</ul>
</li>
<li>Cons:
<ul>
<li>The most common case would have required an extra API call.</li>
<li>Error prone, users may forget to call the extra API.</li>
<li>Unexpected and hard to find errors if user tries to create a child <code>Span</code> before calling
MakeSamplingDecision().</li>
</ul>
</li>
</ul>
</li>
<li>We considered allowing the sampling decision to be arbitrarily delayed, but guaranteed before
any child <code>Span</code> is created, or <code>SpanContext</code> is accessed, or before <code>Span.end()</code> finished.
<ul>
<li>Pros:
<ul>
<li>Similar and smaller API that supports both use-cases defined ahead.</li>
</ul>
</li>
<li>Cons:
<ul>
<li>If <code>SamplingHint</code> needs to also be delayed recorded then an extra API on Span is required
to set this.</li>
<li>Does not allow optimization to not record tracing events, all tracing events MUST be
recorded before the sampling decision is made.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#prior-art-and-alternatives-4" id="prior-art-and-alternatives-4">Prior art and alternatives</a></h2>
<p>Prior art for Zipkin, and other Dapper based systems: all client-side sampling decisions are made at
head. Thus, we need to retain compatibility with this.</p>
<h2><a class="header" href="#open-questions-3" id="open-questions-3">Open questions</a></h2>
<p>This RFC does not necessarily resolve the question of how to propagate sampling rate values between
different spans and processes. A separate RFC will be opened to cover this case.</p>
<h2><a class="header" href="#future-possibilities" id="future-possibilities">Future possibilities</a></h2>
<p>In the future, we propose that library developers may be able to defer the decision on whether to
recommend the trace be sampled or not sampled until mid-way through execution;</p>
<h2><a class="header" href="#related-issues-1" id="related-issues-1">Related Issues</a></h2>
<ul>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/189">opentelemetry-specification/189</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/187">opentelemetry-specification/187</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/164">opentelemetry-specification/164</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/125">opentelemetry-specification/125</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/87">opentelemetry-specification/87</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/66">opentelemetry-specification/66</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/65">opentelemetry-specification/65</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/53">opentelemetry-specification/53</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/33">opentelemetry-specification/33</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/32">opentelemetry-specification/32</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/31">opentelemetry-specification/31</a></li>
</ul>
<h1><a class="header" href="#remove-support-to-report-out-of-band-telemetry-from-the-api" id="remove-support-to-report-out-of-band-telemetry-from-the-api">Remove support to report out-of-band telemetry from the API</a></h1>
<h2><a class="header" href="#tldr-1" id="tldr-1">TL;DR</a></h2>
<p>This section tries to summarize all the changes proposed in this RFC:</p>
<ol>
<li>Remove API requirement to support reporting out-of-band telemetry.</li>
<li>Move Resource to SDK, API will always report telemetry for the current application so no need to
allow configuring the Resource in any instrumentation.</li>
<li>New APIs should be designed without this requirement.</li>
</ol>
<h2><a class="header" href="#motivation-5" id="motivation-5">Motivation</a></h2>
<p>Currently the API package is designed with a goal to support reporting out-of-band telemetry, but
this requirements forces a lot of trade-offs and unnecessary complicated APIs (e.g. <code>Resource</code> must
be exposed in the API package to allow telemetry to be associated with the source of the telemetry).</p>
<p>Reporting out-of-band telemetry is a required for the OpenTelemetry ecosystem, but this can be done
via a few different other options that does not require to use the API package:</p>
<ul>
<li>The OpenTelemetry Service, users can write a simple <a href="https://github.com/open-telemetry/opentelemetry-service#config-receivers">receiver</a> that parses and
produces the OpenTelemetry data.</li>
<li>Using the SDK's exporter framework, users can write directly OpenTelemetry data.</li>
</ul>
<h2><a class="header" href="#internal-details-2" id="internal-details-2">Internal details</a></h2>
<p>Here is a list of decisions and trade-offs related to supporting out-of-band reporting:</p>
<ol>
<li>Add <code>Resource</code> concept into the API.
<ul>
<li>Example in the create metric we need to allow users to specify the resource, see
<a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/metrics/api.md#create-metric">here</a>. The developer that writes the instrumentation has no knowledge about where
the monitored resource is deployed so there is no way to configure the right resource.</li>
</ul>
</li>
<li><a href="./trace/0002-remove-spandata.html">RFC</a> removes support to report SpanData.
<ul>
<li>This will require that the trace API has to support all the possible fields to be configured
via the API, for example need to allow users to set a pre-generated <code>SpanId</code> that can be avoided
if we do not support out-of-band reporting.</li>
</ul>
</li>
<li>Sampling logic for out-of-band spans will get very complicated because it will be incorrect to
sample these data.</li>
<li>Associating the source of the telemetry with the telemetry data gets very simple. All data
produced by one instance of the API implementation belongs to only one Application.</li>
</ol>
<p>This can be rephrased as &quot;one API implementation instance&quot; can report telemetry about only the
current Application.</p>
<h3><a class="header" href="#resource-changes" id="resource-changes">Resource changes</a></h3>
<p>This RFC does not suggest to remove the <code>Resource</code> concept or to modify any API in this interface,
it only suggests to move this concept to the SDK level.</p>
<p>Every implementation of the API (SDK in OpenTelemetry case) instance will have one <code>Resource</code> that
describes the running Application. There may be cases where in the same binary there are multiple
Application running (e.g. Java application server), every application will have it's own SDK
instance configured with it's own <code>Resource</code>.</p>
<h2><a class="header" href="#related-issues-2" id="related-issues-2">Related Issues</a></h2>
<ul>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/62">opentelemetry-specification/62</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/61">opentelemetry-specification/61</a></li>
</ul>
<h1><a class="header" href="#metrics-observer-specification" id="metrics-observer-specification">Metrics observer specification</a></h1>
<p><strong>Status:</strong> Superceded entirely by <a href="0072-metric-observer.html">0072-metric-observer</a></p>
<p>Propose metric <code>Observer</code> callbacks for context-free access to current Gauge instrument values on demand.</p>
<h2><a class="header" href="#motivation-6" id="motivation-6">Motivation</a></h2>
<p>The current specification describes metric callbacks as an alternate means of generating metrics for the SDK, allowing the application to generate metrics only as often as desired by the monitoring infrastructure.  This proposal limits callback metrics to only support gauge <code>Observer</code> callbacks, arguably the only important case.</p>
<h2><a class="header" href="#explanation-4" id="explanation-4">Explanation</a></h2>
<p>Gauge metric instruments are typically used to reflect properties that are pre-computed by a system, where the measurement interval is arbitrary.  When selecting a gauge, as opposed to the cumulative or measure kind of metric instrument, there could be significant computational cost in computing the current value.  When this is the case, it is understandable that we are interested in computing them on demand to minimize cost.</p>
<p>Why are gauges different than cumulative and measure instruments?  Measure instruments, by definition, carry information in the individual event, so the callback cannot optimize any better than the SDK can in this case.  Cumulative instruments are more commonly used to record amounts that are readily available, such as the number of bytes read or written, and while this may not always be true, recall the special case of <code>NonDescending</code> gauges.</p>
<p><code>NonDescending</code> gauges owe their existence to this case, that we support non-negative cumulative metrics which, being expensive to compute, are recommended for use with <code>Observer</code> callbacks.  For example, if it requires a system call or more to compute a non-descending sum, such as the <em>cpu seconds</em> consumed by the process, we should declare a non-descending gauge <code>Observer</code> for the instrument, instead of a cumulative.  This allows the cost of the metric to be reduced according to the desired monitoring frequency.</p>
<p>One significant difference between gauges that are explicitly <code>Set()</code>, as compared with observer callbacks, is that <code>Set()</code> happens inside a context, whereas the observer callback does not.</p>
<h2><a class="header" href="#details" id="details">Details</a></h2>
<p>Observer callbacks are only supported for gauge metric instruments.  Use the language-specific constructor for an Observer gauge (e.g., <code>metric.NewFloat64Observer()</code>).  Observer gauges support the <code>NonDescending</code> option.</p>
<p>Callbacks return a map from <em>label set</em> to gauge value. Gauges declared with observer callbacks cannot also be <code>Set</code>.</p>
<p>Callbacks should avoid blocking.  The implementation may be required to cancel computation if the callback blocks for too long.</p>
<p>Callbacks must not be called synchronously with application code via any OpenTelemetry API.  Implementations that cannot provide this guarantee should prefer not to implement observer callbacks.</p>
<p>Callbacks may be called synchronously in the SDK on behalf of an exporter.</p>
<p>Callbacks should avoid calling OpenTelemetry APIs, but we recognize this may be impossible to enforce.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-3" id="trade-offs-and-mitigations-3">Trade-offs and mitigations</a></h2>
<p>Callbacks are a relatively dangerous programming pattern, which may require care to avoid deadlocks between the application and the API or the SDK.  Implementations may consider preventing deadlocks through runtime callstack introspection, to make these interfaces absolutely safe.</p>
<h1><a class="header" href="#metric-handle-api-specification" id="metric-handle-api-specification">Metric Handle API specification</a></h1>
<p>Specify the behavior of the Metrics API &quot;Handle&quot; type, for efficient repeated-use of metric instruments.</p>
<h2><a class="header" href="#motivation-7" id="motivation-7">Motivation</a></h2>
<p>The specification currently names this concept &quot;TimeSeries&quot;, the type returned by <code>GetOrCreateTimeseries</code>, which supports binding a metric to a pre-defined set of labels for repeated use.  This proposal renames these &quot;Handle&quot; and <code>GetHandle</code>, respectively, and adds further detail to the API specification for handles.</p>
<h2><a class="header" href="#explanation-5" id="explanation-5">Explanation</a></h2>
<p>The <code>TimeSeries</code> is referred to as a &quot;Handle&quot;, as the former name suggests an implementation, not an API concept. &quot;Handle&quot;, we feel, is more descriptive of the intended use.  Likewise with <code>GetOrCreateTimeSeries</code> to <code>GetHandle</code> and <code>GetDefaultTimeSeries</code> to <code>GetDefaultHandle</code>, these names suggest an implementation and not the intended use.</p>
<p>Applications are encouraged to re-use metric handles for efficiency.</p>
<p>Handles are useful to reduce the cost of repeatedly recording a metric instrument (cumulative, gauge, or measure) with a pre-defined set of label values.</p>
<p><code>GetHandle</code> gets a new handle given a <a href="./0049-metric-label-set.html"><code>LabelSet</code></a>.</p>
<p>As a language-optional feature, the API may provide an <em>ordered</em> form of the API for supplying labels in known order.  The ordered label-value API is provided as a (language-optional) potential optimization that facilitates a simple lookup for the SDK.  In this ordered-value form, the API is permitted to throw an exception or return an error when there is a mismatch in the arguments to <code>GetHandle</code>, although languages without strong type-checking may wish to omit this feature.  When label values are accepted in any order, SDKs may be forced to canonicalize the labels in order to find an existing metrics handle, but they must not throw exceptions.</p>
<p><code>GetHandle</code> supports arbitrary label sets.  There is no requirement that the LabelSet used to construct a handle covers the recommended aggregation keys of a metric instrument.</p>
<h2><a class="header" href="#internal-details-3" id="internal-details-3">Internal details</a></h2>
<p>Because each of the metric kinds supports a different operation (<code>Add()</code>, <code>Set()</code>, and <code>Record()</code>), there are logically distinct kinds of handle.  The names of the distinct handle types should reflect their instrument kind.</p>
<p>The names (<code>Handle</code>, <code>GetHandle</code>, ...) are just language-neutral recommendations.  Language APIs should feel free to choose type and method names with attention to the language's style.</p>
<h3><a class="header" href="#metric-attachment-support" id="metric-attachment-support">Metric <code>Attachment</code> support</a></h3>
<p>OpenCensus has the notion of a metric attachment, allowing the application to include additional information associated with the event, for sampling purposes.  Any label value not used for aggregation may be used as a sample &quot;attachment&quot;, including the OpenTelemetry span context, to associate sample trace context with exported metrics.</p>
<h2><a class="header" href="#issues-addressed-1" id="issues-addressed-1">Issues addressed</a></h2>
<p><a href="https://docs.google.com/document/d/1d0afxe3J6bQT-I6UbRXeIYNcTIyBQv4axfjKF4yvAPA/edit#">Agreements reached on handles and naming in the working group convened on 8/21/2019</a>.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/144"><code>record</code> should take a generic <code>Attachment</code> class instead of having tracing dependency</a></p>
<h1><a class="header" href="#rename-cumulative-to-counter-in-the-metrics-api" id="rename-cumulative-to-counter-in-the-metrics-api">Rename &quot;Cumulative&quot; to &quot;Counter&quot; in the metrics API</a></h1>
<p>Prefer the name &quot;Counter&quot; as opposed to &quot;Cumulative&quot;.</p>
<h2><a class="header" href="#motivation-8" id="motivation-8">Motivation</a></h2>
<p>Informally speaking, it seems that OpenTelemetry community members would prefer to call Cumulative metric instruments &quot;Counters&quot;.  During conversation (e.g., in the 8/21 working session), this has become clear.</p>
<p>Counter is a noun, like the other kinds Gauge and Measure.  Cumulative is an adjective, so while &quot;Cumulative instrument&quot; makes sense, it describes a &quot;Counter&quot;.</p>
<h2><a class="header" href="#explanation-6" id="explanation-6">Explanation</a></h2>
<p>This will eliminate the cognitive cost of mapping &quot;cumulative&quot; to &quot;counter&quot; when speaking about these APIs.</p>
<p>This is the term used for a cumulative metric instrument, for example, in <a href="https://github.com/statsd/statsd/blob/master/docs/metric_types.md">Statsd</a> and <a href="https://prometheus.io/docs/concepts/metric_types/#counter">Prometheus</a>.</p>
<p>However, we have identified important sub-cases of Counter that are treated as follows.  Counters have an option:</p>
<ul>
<li>True-cumulative Counter: By default, <code>Add()</code> arguments must be &gt;= 0.</li>
<li>Bi-directional Counter: As an option, <code>Add()</code> arguments must be +/-0.</li>
</ul>
<p>Gauges are sometimes used to monitoring non-descending quantities (e.g., cpu usage), as an option:</p>
<ul>
<li>Bi-directional Gauge: By default, <code>Set()</code> arguments may by +/- 0.</li>
<li>Uni-directional Gauge: As an option, <code>Set()</code> arguments must change by &gt;= 0.</li>
</ul>
<p>Uni-directional Gauge instruments are typically used in metric <code>Observer</code> callbacks where the observed value is cumulative.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-4" id="trade-offs-and-mitigations-4">Trade-offs and mitigations</a></h2>
<p>Other ways to describe the distinction between true-cumulative and bi-directional Counters are:</p>
<ul>
<li>Additive (vs. Cumulative)</li>
<li>GaugeDelta (vs. Gauge)</li>
</ul>
<p>It is possible that reducing all of these cases into the broad term &quot;Counter&quot; creates more confusion than it addresses.</p>
<h2><a class="header" href="#internal-details-4" id="internal-details-4">Internal details</a></h2>
<p>Simply replace every &quot;Cumulative&quot; with &quot;Counter&quot;, then edit for grammar.</p>
<h2><a class="header" href="#prior-art-and-alternatives-5" id="prior-art-and-alternatives-5">Prior art and alternatives</a></h2>
<p>In a survey of existing metrics libraries, Counter is far more common.</p>
<h1><a class="header" href="#named-tracers-and-meters" id="named-tracers-and-meters">Named Tracers and Meters</a></h1>
<p><em>Associate Tracers and Meters with the name and version of the instrumentation library which reports telemetry data by parameterizing the API which the library uses to acquire the Tracer or Meter.</em></p>
<h2><a class="header" href="#suggested-reading-1" id="suggested-reading-1">Suggested reading</a></h2>
<ul>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/issues/10">Proposal: Tracer Components</a></li>
<li><a href="https://github.com/open-telemetry/opentelemetry-specification/labels/global%20instance">Global Instance discussions</a></li>
<li><a href="https://github.com/open-telemetry/oteps/pull/38">Proposal: Add a version resource</a></li>
</ul>
<h2><a class="header" href="#motivation-9" id="motivation-9">Motivation</a></h2>
<p>The mechanism of &quot;Named Tracers and Meters&quot; proposed here is motivated by the following scenarios:</p>
<h3><a class="header" href="#faulty-or-expensive-instrumentation" id="faulty-or-expensive-instrumentation">Faulty or expensive instrumentation</a></h3>
<p>For an operator of an application using OpenTelemetry, there is currently no way to influence the amount of data produced by instrumentation libraries. Instrumentation libraries can easily &quot;spam&quot; backend systems, deliver bogus data, or -- in the worst case -- crash or slow down applications. These problems might even occur suddenly in production environments because of external factors such as increasing load or unexpected input data.</p>
<h3><a class="header" href="#instrumentation-library-identification" id="instrumentation-library-identification">Instrumentation library identification</a></h3>
<p>If an instrumentation library hasn't implemented <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/overview.md#semantic-conventions">semantic conventions</a> correctly or those conventions change over time, it's currently hard to interpret and sanitize data produced by it selectively. The produced Spans or Metrics cannot later be associated with the library which reported them, either in the processing pipeline or the backend.</p>
<h3><a class="header" href="#disable-instrumentation-of-pre-instrumented-libraries" id="disable-instrumentation-of-pre-instrumented-libraries">Disable instrumentation of pre-instrumented libraries</a></h3>
<p>It is the eventual goal of OpenTelemetry that library vendors implement the OpenTelemetry API, obviating the need to auto-instrument their library. An operator should be able to disable the telemetry that is built into some database driver or other library and provide their own integration if the built-in telemetry is lacking in some way. This should be possible even if the developer of that database driver has not provided a configuration to disable telemetry.</p>
<h2><a class="header" href="#solution" id="solution">Solution</a></h2>
<p>This proposal attempts to solve the stated problems by introducing the concept of:</p>
<ul>
<li><em>Named Tracers and Meters</em> which are associated with the <strong>name</strong> (e.g. <em>&quot;io.opentelemetry.contrib.mongodb&quot;</em>) and <strong>version</strong> (e.g.<em>&quot;semver:1.0.0&quot;</em>) of the library which acquired them.</li>
<li>A <code>TracerProvider</code> / <code>MeterProvider</code> as the only means of acquiring a Tracer or Meter.</li>
</ul>
<p>Based on the name and version, a Provider could provide a no-op Tracer or Meter to specific instrumentation libraries, or a Sampler could be implemented that discards Spans or Metrics from certain libraries. Also, by providing custom Exporters, Span or Metric data could be sanitized before it gets processed in a back-end system. However, this is beyond the scope of this proposal, which only provides the fundamental mechanisms.</p>
<h2><a class="header" href="#explanation-7" id="explanation-7">Explanation</a></h2>
<p>From a user perspective, working with <em>Named Tracers / Meters</em> and <code>TracerProvider</code> / <code>MeterProvider</code> is conceptually similar to how e.g. the <a href="https://docs.oracle.com/javase/7/docs/api/java/util/logging/Logger.html#getLogger(java.lang.String)">Java logging API</a> and logging frameworks like <a href="https://www.slf4j.org/apidocs/org/slf4j/LoggerFactory.html">log4j</a> work. In analogy to requesting Logger objects through LoggerFactories, an instrumentation library would create specific Tracer / Meter objects through a TracerProvider / MeterProvider.</p>
<p>New Tracers or Meters can be created by providing the name and version of an instrumentation library. The version (following the convention proposed in <a href="https://github.com/open-telemetry/oteps/pull/38">https://github.com/open-telemetry/oteps/pull/38</a>) is basically optional but <em>should</em> be supplied since only this information enables following scenarios:</p>
<ul>
<li>Only a specific range of versions of a given instrumentation library need to be suppressed, while other versions are allowed (e.g. due to a bug in those specific versions).</li>
<li>Go modules allow multiple versions of the same middleware in a single build so those need to be determined at runtime.</li>
</ul>
<pre><code class="language-java">// Create a tracer/meter for a given instrumentation library in a specific version.
Tracer tracer = OpenTelemetry.getTracerProvider().getTracer(&quot;io.opentelemetry.contrib.mongodb&quot;, &quot;semver:1.0.0&quot;);
Meter meter = OpenTelemetry.getMeterProvider().getMeter(&quot;io.opentelemetry.contrib.mongodb&quot;, &quot;semver:1.0.0&quot;);
</code></pre>
<p>These factories (<code>TracerProvider</code> and <code>MeterProvider</code>) replace the global <code>Tracer</code> / <code>Meter</code> singleton objects as ubiquitous points to request Tracer and Meter instances.</p>
<p>The <em>name</em> used to create a Tracer or Meter must identify the <em>instrumentation</em> libraries (also referred to as <em>integrations</em>) and not the library being instrumented. These instrumentation libraries could be libraries developed in an OpenTelemetry repository, a 3rd party implementation, or even auto-injected code (see <a href="https://github.com/open-telemetry/oteps/blob/master/text/0001-telemetry-without-manual-instrumentation.md">Open Telemetry Without Manual Instrumentation OTEP</a>). See also the examples for identifiers at the end.
If a library (or application) has instrumentation built-in, it is both the instrumenting and instrumented library and should pass its own name here. In all other cases (and to distinguish them from that case), the distinction between instrumenting and instrumented library is very important. For example, if an HTTP library <code>com.example.http</code> is instrumented by either <code>io.opentelemetry.contrib.examplehttp</code>, then it is important that the Tracer is not named <code>com.example.http</code>, but <code>io.opentelemetry.contrib.examplehttp</code> after the actual instrumentation library.</p>
<p>If no name (null or empty string) is specified, following the suggestions in <a href="https://github.com/open-telemetry/opentelemetry-specification/pull/153">&quot;error handling proposal&quot;</a>, a &quot;smart default&quot; will be applied and a default Tracer / Meter implementation is returned.</p>
<h3><a class="header" href="#examples-of-tracer-and-meter-names" id="examples-of-tracer-and-meter-names">Examples (of Tracer and Meter names)</a></h3>
<p>Since Tracer and Meter names describe the libraries which use those Tracers and Meters, their names should be defined in a way that makes them as unique as possible.
The name of the Tracer / Meter should represent the identity of the library, class or package that provides the instrumentation.</p>
<p>Examples (based on existing contribution libraries from OpenTracing and OpenCensus):</p>
<ul>
<li><code>io.opentracing.contrib.spring.rabbitmq</code></li>
<li><code>io.opentracing.contrib.jdbc</code></li>
<li><code>io.opentracing.thrift</code></li>
<li><code>io.opentracing.contrib.asynchttpclient</code></li>
<li><code>io.opencensus.contrib.http.servlet</code></li>
<li><code>io.opencensus.contrib.spring.sleuth.v1x</code></li>
<li><code>io.opencesus.contrib.http.jaxrs</code></li>
<li><code>github.com/opentracing-contrib/go-amqp</code> (Go)</li>
<li><code>github.com/opentracing-contrib/go-grpc</code> (Go)</li>
<li><code>OpenTracing.Contrib.NetCore.AspNetCore</code> (.NET)</li>
<li><code>OpenTracing.Contrib.NetCore.EntityFrameworkCore</code> (.NET)</li>
</ul>
<h2><a class="header" href="#internal-details-5" id="internal-details-5">Internal details</a></h2>
<p>By providing a <code>TracerProvider</code> / <code>MeterProvider</code> and <em>Named Tracers / Meters</em>, a vendor or OpenTelemetry implementation gains more flexibility in providing Tracers and Meters and which attributes they set in the resulting Spans and Metrics that are produced.</p>
<p>On an SDK level, the SpanData class and its Metrics counterpart are extended with a <code>getLibraryResource</code> function that returns the resource associated with the Tracer / Meter that created it.</p>
<h2><a class="header" href="#glossary-of-terms" id="glossary-of-terms">Glossary of Terms</a></h2>
<h3><a class="header" href="#instrumentation-library" id="instrumentation-library">Instrumentation library</a></h3>
<p>Also known as the trace/metrics reporter, this may be either a library/module/plugin provided by OpenTelemetry that instruments an existing library, a third party integration which instruments some library, or a library that has implemented the OpenTelemetry API in order to instrument itself. In any case, the instrumentation library is the library which provides tracing and metrics data to OpenTelemetry.</p>
<p>examples:</p>
<ul>
<li><code>@opentelemetry/plugin-http</code></li>
<li><code>io.opentelemetry.redis</code></li>
<li><code>redis-client</code> (in this case, <code>redis-client</code> has instrumented itself with the OpenTelemetry API)</li>
</ul>
<h3><a class="header" href="#tracer--meter-name-and-version" id="tracer--meter-name-and-version">Tracer / Meter name and version</a></h3>
<p>When an instrumentation library acquires a Tracer/Meter, it provides its own name and version to the Tracer/Meter Provider. This name/version two-tuple is said to be the Tracer/Meter's <em>name</em> and <em>version</em>. Note that this is the name and version of the library which acquires the Tracer/Meter, and not the library it is monitoring. In cases where the library is instrumenting itself using the OpenTelemetry API, they may be the same.</p>
<p>example: If the <code>http</code> version <code>semver:3.0.0</code> library is being instrumented by a library with the name <code>io.opentelemetry.contrib.http</code> and version <code>semver:1.3.2</code>, then the tracer name and version are also <code>io.opentelemetry.contrib.http</code> and <code>semver:1.3.2</code>. If that same <code>http</code> library has built-in instrumentation through use of the OpenTelemetry API, then the tracer name and version would be <code>http</code> and <code>semver:3.0.0</code>.</p>
<h3><a class="header" href="#meter-namespace" id="meter-namespace">Meter namespace</a></h3>
<p>Meter name is used as a namespace for all metrics created by it. This allows a telemetry library to register a metric using any name, such as <code>latency</code>, without worrying about collisions with a metric registered under the same name by a different library.</p>
<p>example: The libraries <code>redis</code> and <code>io.opentelemetry.redis</code> may both register metrics with the name <code>latency</code>. These metrics can still be uniquely identified even though they have the same name because they are registered under different namespaces (<code>redis</code> and <code>io.opentelemetry.redis</code> respectively). In this case, the operator may disable one of these metrics because they are measuring the same thing.</p>
<h2><a class="header" href="#prior-art-and-alternatives-6" id="prior-art-and-alternatives-6">Prior art and alternatives</a></h2>
<p>This proposal originates from an <code>opentelemetry-specification</code> proposal on <a href="https://github.com/open-telemetry/opentelemetry-specification/issues/10">components</a> since having a concept of named Tracers would automatically enable determining this semantic <code>component</code> property.</p>
<p>Alternatively, instead of having a <code>TracerProvider</code>, existing (global) Tracers could return additional indirection objects (called e.g. <code>TraceComponent</code>), which would be able to produce spans for specifically named traced components.</p>
<pre><code class="language-java">TraceComponent traceComponent = OpenTelemetry.Tracing.getTracer().componentBuilder(&quot;io.opentelemetry.contrib.mongodb&quot;, &quot;semver:1.0.0&quot;);
Span span = traceComponent.spanBuilder(&quot;someMethod&quot;).startSpan();
</code></pre>
<p>Overall, this would not change a lot compared to the <code>TracerProvider</code> since the levels of indirection until producing an actual span are the same.</p>
<p>Instead of setting the <code>component</code> property based on the given Tracer names, those names could also be used as <em>prefixes</em> for produced span names (e.g. <code>&lt;TracerName-SpanName&gt;</code>). However, with regard to data quality and semantic conventions, a dedicated <code>component</code> set on spans is probably preferred.</p>
<p>Instead of using plain strings as an argument for creating new Tracers, a <code>Resource</code> identifying an instrumentation library could be used. Such resources must have a <em>version</em> and a <em>name</em> label (there could be semantic convention definitions for those labels). This implementation alternative mainly depends on the availability of the <code>Resource</code> data type on an API level (see <a href="https://github.com/open-telemetry/opentelemetry-specification/pull/254">https://github.com/open-telemetry/opentelemetry-specification/pull/254</a>).</p>
<pre><code class="language-java">// Create resource for given instrumentation library information (name + version)
Map&lt;String, String&gt; libraryLabels = new HashMap&lt;&gt;();
libraryLabels.put(&quot;name&quot;, &quot;io.opentelemetry.contrib.mongodb&quot;);
libraryLabels.put(&quot;version&quot;, &quot;1.0.0&quot;);
Resource libraryResource = Resource.create(libraryLabels);
// Create tracer for given instrumentation library.
Tracer tracer = OpenTelemetry.getTracerProvider().getTracer(libraryResource);
</code></pre>
<p>Those given alternatives could be applied to Meters and Metrics in the same way.</p>
<h2><a class="header" href="#future-possibilities-1" id="future-possibilities-1">Future possibilities</a></h2>
<p>Based on the Resource information identifying a Tracer or Meter these could be configured (enabled / disabled) programmatically or via external configuration sources (e.g. environment).</p>
<p>Based on this proposal, future &quot;signal producers&quot; (i.e. logs) can use the same or a similar creation approach.</p>
<h1><a class="header" href="#opentelemetry-protocol-specification" id="opentelemetry-protocol-specification">OpenTelemetry Protocol Specification</a></h1>
<p><strong>Author</strong>: Tigran Najaryan, Omnition Inc.</p>
<p>OpenTelemetry Protocol (OTLP) specification describes the encoding, transport and delivery mechanism of telemetry data between telemetry sources, intermediate nodes such as collectors and telemetry backends.</p>
<h2><a class="header" href="#table-of-contents-1" id="table-of-contents-1">Table of Contents</a></h2>
<ul>
<li><a href="0035-opentelemetry-protocol.html#motivation">Motivation</a></li>
<li><a href="0035-opentelemetry-protocol.html#protocol-details">Protocol Details</a>
<ul>
<li><a href="0035-opentelemetry-protocol.html#export-request-and-response">Export Request and Response</a>
<ul>
<li><a href="0035-opentelemetry-protocol.html#otlp-over-grpc">OTLP over gRPC</a></li>
<li><a href="0035-opentelemetry-protocol.html#export-response">Export Response</a></li>
<li><a href="0035-opentelemetry-protocol.html#throttling">Throttling</a></li>
<li><a href="0035-opentelemetry-protocol.html#grpc-service-definition">gRPC Service Definition</a></li>
</ul>
</li>
<li><a href="0035-opentelemetry-protocol.html#other-transports">Other Transports</a></li>
</ul>
</li>
<li><a href="0035-opentelemetry-protocol.html#implementation-recommendations">Implementation Recommendations</a>
<ul>
<li><a href="0035-opentelemetry-protocol.html#multi-destination-exporting">Multi-Destination Exporting</a></li>
</ul>
</li>
<li><a href="0035-opentelemetry-protocol.html#trade-offs-and-mitigations">Trade-offs and mitigations</a>
<ul>
<li><a href="0035-opentelemetry-protocol.html#request-acknowledgements">Request Acknowledgements</a>
<ul>
<li><a href="0035-opentelemetry-protocol.html#duplicate-data">Duplicate Data</a></li>
</ul>
</li>
<li><a href="0035-opentelemetry-protocol.html#partial-success">Partial Success</a></li>
</ul>
</li>
<li><a href="0035-opentelemetry-protocol.html#future-versions-and-interoperability">Future Versions and Interoperability</a></li>
<li><a href="0035-opentelemetry-protocol.html#prior-art-alternatives-and-future-possibilities">Prior Art, Alternatives and Future Possibilities</a></li>
<li><a href="0035-opentelemetry-protocol.html#open-questions">Open Questions</a></li>
<li><a href="0035-opentelemetry-protocol.html#appendix-a---protocol-buffer-definitions">Appendix A - Protocol Buffer Definitions</a></li>
<li><a href="0035-opentelemetry-protocol.html#appendix-b---performance-benchmarks">Appendix B - Performance Benchmarks</a>
<ul>
<li><a href="0035-opentelemetry-protocol.html#throughput---sequential-vs-concurrent">Throughput - Sequential vs Concurrent</a></li>
<li><a href="0035-opentelemetry-protocol.html#cpu-usage---grpc-vs-websocketexperimental">CPU Usage - gRPC vs WebSocket/Experimental</a></li>
<li><a href="0035-opentelemetry-protocol.html#benchmarking-raw-results">Benchmarking Raw Results</a></li>
</ul>
</li>
<li><a href="0035-opentelemetry-protocol.html#glossary">Glossary</a></li>
<li><a href="0035-opentelemetry-protocol.html#acknowledgements">Acknowledgements</a></li>
</ul>
<h2><a class="header" href="#motivation-10" id="motivation-10">Motivation</a></h2>
<p>OTLP is a general-purpose telemetry data delivery protocol designed in the scope of OpenTelemetry project. It is an incremental improvement of OpenCensus protocol. Compared to OpenCensus protocol OTLP has the following improvements:</p>
<ul>
<li>
<p>Ensures high reliability of data delivery and clear visibility when the data cannot be delivered. OTLP uses acknowledgements to implement reliable delivery.</p>
</li>
<li>
<p>It is friendly to Level 7 Load Balancers and allows them to correctly map imbalanced incoming traffic to a balanced outgoing traffic. This allows to efficiently operate large networks of nodes where telemetry data generation rates change over time.</p>
</li>
<li>
<p>Allows backpressure signalling from telemetry data destinations to sources. This is important for implementing reliable multi-hop telemetry data delivery all the way from the source to the destination via intermediate nodes, each having different processing capacity and thus requiring different data transfer rates.</p>
</li>
</ul>
<h2><a class="header" href="#protocol-details" id="protocol-details">Protocol Details</a></h2>
<p>OTLP defines the encoding of telemetry data and the protocol used to exchange data between the client and the server.</p>
<p>This specification defines how OTLP is implemented over <a href="https://grpc.io/">gRPC</a> and specifies corresponding <a href="https://developers.google.com/protocol-buffers/docs/overview">Protocol Buffers</a> schema. Future extensions to OTLP may define implementations over other transports. For details of gRPC service definition see section <a href="0035-opentelemetry-protocol.html#grpc-transport">gRPC Transport</a>.</p>
<p>OTLP is a request/response style protocols: the clients send requests, the server replies with corresponding responses. This document defines one requests and response type: <code>Export</code>.</p>
<h3><a class="header" href="#export-request-and-response" id="export-request-and-response">Export Request and Response</a></h3>
<p>After establishing the underlying transport the client starts sending telemetry data using <code>Export</code> requests.  The client continuously sends a sequence of <code>Export</code> requests to the server and expects to receive a response to each request:</p>
<p><img src="images/otlp-request-response.png" alt="Request-Response" /></p>
<p><em>Note: this protocol is concerned with reliability of delivery between one pair of client/server nodes and aims to ensure that no data is lost in-transit between the client and the server. Many telemetry collection systems have intermediary nodes that the data must travel across until reaching the final destination (e.g. application -&gt; agent -&gt; collector -&gt; backend). End-to-end delivery guarantees in such systems is outside of the scope of OTLP. The acknowledgements described in this protocol happen between a single client/server pair and do not span intermediary nodes in multi-hop delivery paths.</em></p>
<h4><a class="header" href="#otlp-over-grpc" id="otlp-over-grpc">OTLP over gRPC</a></h4>
<p>For gRPC transport OTLP uses Unary RPC to send export requests and receives responses.</p>
<p>After sending the request the client MAY wait until the response is received from the server. In that case there will be at most only one request in flight that is not yet acknowledged by the server.</p>
<p><img src="images/otlp-sequential.png" alt="Unary" /></p>
<p>Sequential operation is recommended when simplicity of implementation is desirable and when the client and the server are connected via very low-latency network, such as for example when the client is an instrumented application and the server is a OpenTelemetry Service running as a local daemon.</p>
<p>The implementations that need to achieve high throughput SHOULD support concurrent Unary calls to achieve higher throughput. The client SHOULD send new requests without waiting for the response to the earlier sent requests, essentially creating a pipeline of requests that are currently in flight that are not acknowledged.</p>
<p><img src="images/otlp-concurrent.png" alt="Streaming" /></p>
<p>The number of concurrent requests SHOULD be configurable.</p>
<p>The maximum achievable throughput is <code>max_concurrent_requests * max_request_size / (network_latency + server_response_time)</code>. For example if the request can contain at most 100 spans, network roundtrip latency is 200ms and server response time is 300 ms, then the maximum achievable throughput with one concurrent request is <code>100 spans / (200ms+300ms)</code> or 200 spans per second. It is easy to see that in high latency networks or when the server response time is high to achieve good throughput the requests need to be very big or a lot concurrent requests must be done.</p>
<p>If the client is shutting down (e.g. when the containing process wants to exit) the client will optionally wait until all pending acknowledgements are received or until an implementation specific timeout expires. This ensures reliable delivery of telemetry data. The client implementation SHOULD expose an option to turn on and off the waiting during shutdown.</p>
<p>If the client is unable to deliver a certain request (e.g. a timer expired while waiting for acknowledgements) the client SHOULD record the fact that the data was not delivered.</p>
<h4><a class="header" href="#export-response" id="export-response">Export Response</a></h4>
<p>The server may respond with either a success or an error to export requests.</p>
<p>The success response indicates telemetry data is successfully processed by the server. If the server receives an empty request (a request that does not carry any telemetry data) the server SHOULD respond with success.</p>
<p>When using gRPC transport, success response is returned via <code>ExportResponse</code> message.</p>
<p>When an error is returned by the server it falls into 2 broad categories: retryable and not-retryable:</p>
<ul>
<li>
<p>Retryable errors indicate that processing of telemetry data failed and the client SHOULD record the error and may retry exporting the same data. This can happen when the server is temporarily unable to process the data.</p>
</li>
<li>
<p>Not-retryable errors indicate that processing of telemetry data failed and the client MUST NOT retry sending the same telemetry data. The telemetry data MUST be dropped. This can happen, for example, when the request contains bad data and cannot be deserialized or otherwise processed by the server. The client SHOULD maintain a counter of such dropped data.</p>
</li>
</ul>
<p>When using gRPC transport the server SHOULD indicate retryable errors using code <a href="https://godoc.org/google.golang.org/grpc/codes">Unavailable</a> and MAY supply additional <a href="https://godoc.org/google.golang.org/grpc/status#Status.WithDetails">details via status</a> using <a href="https://github.com/googleapis/googleapis/blob/6a8c7914d1b79bd832b5157a09a9332e8cbd16d4/google/rpc/error_details.proto#L40">RetryInfo</a> containing 0 value of RetryDelay. Here is a sample Go code to illustrate:</p>
<pre><code class="language-go">  // Do this on server side.
  st, err := status.New(codes.Unavailable, &quot;Server is unavailable&quot;).
    WithDetails(&amp;errdetails.RetryInfo{RetryDelay: &amp;duration.Duration{Seconds: 0}})
  if err != nil {
    log.Fatal(err)
  }

  return st.Err()
</code></pre>
<p>To indicate not-retryable errors the server is recommended to use code <a href="https://godoc.org/google.golang.org/grpc/codes">InvalidArgument</a> and MAY supply additional <a href="https://godoc.org/google.golang.org/grpc/status#Status.WithDetails">details via status</a> using <a href="https://github.com/googleapis/googleapis/blob/6a8c7914d1b79bd832b5157a09a9332e8cbd16d4/google/rpc/error_details.proto#L119">BadRequest</a>. Other gRPC status code may be used if it is more appropriate. Here is a sample Go code to illustrate:</p>
<pre><code class="language-go">  // Do this on server side.
  st, err := status.New(codes.InvalidArgument, &quot;Invalid Argument&quot;).
    WithDetails(&amp;errdetails.BadRequest{})
  if err != nil {
    log.Fatal(err)
  }

  return st.Err()
</code></pre>
<p>The server MAY use other gRPC codes to indicate retryable and not-retryable errors if those other gRPC codes are more appropriate for a particular erroneous situation. The client SHOULD interpret gRPC status codes as retryable or not-retryable according to the following table:</p>
<table><thead><tr><th>gRPC Code</th><th>Retryable?</th></tr></thead><tbody>
<tr><td>CANCELLED</td><td>Yes</td></tr>
<tr><td>UNKNOWN</td><td>No</td></tr>
<tr><td>INVALID_ARGUMENT</td><td>No</td></tr>
<tr><td>DEADLINE_EXCEEDED</td><td>Yes</td></tr>
<tr><td>NOT_FOUND</td><td>No</td></tr>
<tr><td>ALREADY_EXISTS</td><td>No</td></tr>
<tr><td>PERMISSION_DENIED</td><td>No</td></tr>
<tr><td>UNAUTHENTICATED</td><td>No</td></tr>
<tr><td>RESOURCE_EXHAUSTED</td><td>Yes</td></tr>
<tr><td>FAILED_PRECONDITION</td><td>No</td></tr>
<tr><td>ABORTED</td><td>Yes</td></tr>
<tr><td>OUT_OF_RANGE</td><td>Yes</td></tr>
<tr><td>UNIMPLEMENTED</td><td>No</td></tr>
<tr><td>INTERNAL</td><td>No</td></tr>
<tr><td>UNAVAILABLE</td><td>Yes</td></tr>
<tr><td>DATA_LOSS</td><td>Yes</td></tr>
</tbody></table>
<p>When retrying, the client SHOULD implement a backoff strategy. An exception to this is the Throttling case explained below, which provides explicit instructions about retrying interval.</p>
<h4><a class="header" href="#throttling" id="throttling">Throttling</a></h4>
<p>OTLP allows backpressure signalling.</p>
<p>If the server is unable to keep up with the pace of data it receives from the client then it SHOULD signal that fact to the client. The client MUST then throttle itself to avoid overwhelming the server.</p>
<p>To signal backpressure when using gRPC transport, the server SHOULD return an error with code <a href="https://godoc.org/google.golang.org/grpc/codes">Unavailable</a> and MAY supply additional <a href="https://godoc.org/google.golang.org/grpc/status#Status.WithDetails">details via status</a> using <a href="https://github.com/googleapis/googleapis/blob/6a8c7914d1b79bd832b5157a09a9332e8cbd16d4/google/rpc/error_details.proto#L40">RetryInfo</a>. Here is a sample Go code to illustrate:</p>
<pre><code class="language-go">  // Do this on server side.
  st, err := status.New(codes.Unavailable, &quot;Server is unavailable&quot;).
    WithDetails(&amp;errdetails.RetryInfo{RetryDelay: &amp;duration.Duration{Seconds: 30}})
  if err != nil {
    log.Fatal(err)
  }

  return st.Err()
  
  ...

  // Do this on client side.
  st := status.Convert(err)
  for _, detail := range st.Details() {
    switch t := detail.(type) {
    case *errdetails.RetryInfo:
      if t.RetryDelay.Seconds &gt; 0 || t.RetryDelay.Nanos &gt; 0 {
        // Wait before retrying.
      }
    }
  }
</code></pre>
<p>When the client receives this signal it SHOULD follow the recommendations outlined in documentation for <code>RetryInfo</code>:</p>
<pre><code>// Describes when the clients can retry a failed request. Clients could ignore
// the recommendation here or retry when this information is missing from error
// responses.
//
// It's always recommended that clients should use exponential backoff when
// retrying.
//
// Clients should wait until `retry_delay` amount of time has passed since
// receiving the error response before retrying.  If retrying requests also
// fail, clients should use an exponential backoff scheme to gradually increase
// the delay between retries based on `retry_delay`, until either a maximum
// number of retires have been reached or a maximum retry delay cap has been
// reached.
</code></pre>
<p>The value of <code>retry_delay</code> is determined by the server and is implementation dependant. The server SHOULD choose a <code>retry_delay</code> value that is big enough to give the server time to recover, yet is not too big to cause the client to drop data while it is throttled.</p>
<h4><a class="header" href="#grpc-service-definition" id="grpc-service-definition">gRPC Service Definition</a></h4>
<p><code>Export</code> requests and responses are delivered using unary gRPC calls.</p>
<p>This is OTLP over gRPC Service definition:</p>
<pre><code>service UnaryExporter {
  rpc ExportTraces(TraceExportRequest) returns (ExportResponse) {}
  rpc ExportMetrics(MetricExportRequest) returns (ExportResponse) {}
}
</code></pre>
<p>Appendix A contains Protocol Buffer definitions for <code>TraceExportRequest</code>, <code>MetricExportRequest</code> and <code>ExportResponse</code>.</p>
<h3><a class="header" href="#other-transports" id="other-transports">Other Transports</a></h3>
<p>OTLP can work over any other transport that supports message request/response capabilities. Additional transports supported by OTLP can be specified in future RFCs that extend OTLP.</p>
<h2><a class="header" href="#implementation-recommendations" id="implementation-recommendations">Implementation Recommendations</a></h2>
<h3><a class="header" href="#multi-destination-exporting" id="multi-destination-exporting">Multi-Destination Exporting</a></h3>
<p>When the telemetry data from one client must be sent to more than one destination server there is an additional complication that must be accounted for. When one of the servers acknowledges the data and the other server does not (yet) acknowledges the client needs to make a decision about how to move forward.</p>
<p>In such situation the the client SHOULD implement queuing, acknowledgement handling and retrying logic per destination. This ensures that servers do not block each other. The queues SHOULD reference shared, immutable data to be sent, thus minimizing the memory overhead caused by having multiple queues.</p>
<p><img src="images/otlp-multi-destination.png" alt="Multi-Destination Exporting" /></p>
<p>This ensures that all destination servers receive the data regardless of their speed of reception (within the available limits imposed by the size of the client-side queue).</p>
<h2><a class="header" href="#trade-offs-and-mitigations-5" id="trade-offs-and-mitigations-5">Trade-offs and mitigations</a></h2>
<h3><a class="header" href="#request-acknowledgements" id="request-acknowledgements">Request Acknowledgements</a></h3>
<h4><a class="header" href="#duplicate-data" id="duplicate-data">Duplicate Data</a></h4>
<p>In edge cases (e.g. on reconnections, network interruptions, etc) the client has no way of knowing if recently sent data was delivered if no acknowledgement was received yet. The client will typically choose to re-send such data to guarantee delivery, which may result in duplicate data on the server side. This is a deliberate choice and is considered to be the right tradeoff for telemetry data.</p>
<h3><a class="header" href="#partial-success" id="partial-success">Partial Success</a></h3>
<p>The protocol does not attempt to communicate partial reception success from the server to the client (i.e. when part of the data can be received by the server and part of it cannot). Attempting to do so would complicate the protocol and implementations significantly and is left out as a possible future area of work.</p>
<h2><a class="header" href="#future-versions-and-interoperability" id="future-versions-and-interoperability">Future Versions and Interoperability</a></h2>
<p>OTLP will evolve and change over time. Future versions of OTLP must be designed and implemented in a way that ensures that clients and servers that implement different versions of OTLP can interoperate and exchange telemetry data. Old clients must be able to talk to new servers and vice versa. If new versions of OTLP introduce new functionality that cannot be understood and supported by nodes implementing the old versions of OTLP the protocol must regress to the lowest common denominator from functional perspective.</p>
<p>When possible the interoperability SHOULD be ensured between all versions of OTLP that are not declared obsolete.</p>
<p>OTLP does not use explicit protocol version numbering. OTLP's interoperability of clients and servers of different versions is based on the following concepts:</p>
<ol>
<li>
<p>OTLP (current and future versions) defines a set of capabilities, some of which are mandatory, others are optional. Clients and servers must implement mandatory capabilities and can choose implement only a subset of optional capabilities.</p>
</li>
<li>
<p>For minor changes to the protocol future versions and extension of OTLP are encouraged to use the ability of Protocol Buffers to evolve message schema in backwards compatible manner. Newer versions of OTLP may add new fields to messages that will be ignored by clients and servers that do not understand these fields. In many cases careful design of such schema changes and correct choice of default values for new fields is enough to ensure interoperability of different versions without nodes explicitly detecting that their peer node has different capabilities.</p>
</li>
<li>
<p>More significant changes must be explicitly defined as new optional capabilities in future RFCs. Such capabilities SHOULD be discovered by client and server implementations after establishing the underlying transport. The exact discovery mechanism SHOULD be described in future RFCs which define the new capabilities and typically can be implemented by making a discovery request/response message exchange from the client to server. The mandatory capabilities defined by this specification are implied and do not require a discovery. The implementation which supports a new, optional capability MUST adjust its behavior to match the expectation of a peer that does not support a particular capability.</p>
</li>
</ol>
<p>The current version of OTLP is the initial version that describes mandatory capabilities only. Implementations of this specification SHOULD NOT attempt to detect the capabilities of their peers and should operate as defined in this document.</p>
<h2><a class="header" href="#prior-art-alternatives-and-future-possibilities" id="prior-art-alternatives-and-future-possibilities">Prior Art, Alternatives and Future Possibilities</a></h2>
<p>We have considered using gRPC streaming instead of Unary RPC calls. This would require implementations to manually perform stream closing and opening periodically to be L7 Load Balancer friendly. Reference implementation using gRPC Streaming has shown that it results in significantly more complex and error prone code without significant benefits. Because of this Unary RPC was chosen.</p>
<p>OTLP is an evolution of OpenCensus protocol based on the research and testing of its modifications in production at Omnition. The modifications include changes to data formats (see RFC0059), use of Unary PRC and backpressure signaling capability.</p>
<p>OTLP uses Protocol Buffers for data encoding. Two other encodings were considered as alternative approaches: FlatBuffers and Capnproto. Both alternatives were rejected. FlatBuffers was rejected because it lacks required functionality in all languages except C++, particularly lack of verification of decoded data and inability to mutate in-memory data. Capnproto was rejected because it is not yet considered production ready, the API is not yet stable and like FlatBuffers it lacks ability mutate in-memory data.</p>
<p>Both FlatBuffers and Capnproto are worth to be re-evaluated for future versions of OpenTelemetry protocol if they overcome currently known limitations.</p>
<p>It is also worth researching transports other than gRPC. Other transports are not included in this RFC due to time limitations.</p>
<p>Experimental implementation of OTLP over WebSockets exists and was researched as an alternate. WebSockets were not chosen as the primary transport for OTLP due to lack or immaturity of certain capabilities (such as <a href="https://github.com/gorilla/websocket#gorilla-websocket-compared-with-other-packages">lack of universal support</a> for <a href="https://tools.ietf.org/html/rfc7692">RFC 7692</a> message compression extension). Despite limitations the experimental implementation demonstrated good performance and WebSocket transport will be considered for inclusion in a future OTLP Extensions RFC.</p>
<h2><a class="header" href="#open-questions-4" id="open-questions-4">Open Questions</a></h2>
<p>One of the goals for telemetry protocol is reducing CPU usage and memory pressure in garbage collected languages. These goals were not addressed as part of this RFC and remain open. One of the promising future ways to address this is finding a more CPU and memory efficient encoding mechanism.</p>
<p>Another goal for telemetry protocol is achieving high compression ratios for telemetry data while keeping CPU consumption low. OTLP uses compression provided by gRPC transport. No further improvements to compression were considered as part of this RFC and are a future area of work.</p>
<h2><a class="header" href="#appendix-a---protocol-buffer-definitions" id="appendix-a---protocol-buffer-definitions">Appendix A - Protocol Buffer Definitions</a></h2>
<p>This is Protocol Buffers schema for <code>Export</code> request and response:</p>
<pre><code>// A request from client to server containing trace data to export.
message TraceExportRequest {
    // Telemetry data. An array of ResourceSpans.
    repeated ResourceSpans resourceSpans = 2;
}

// A request from client to server containing metric data to export.
message MetricExportRequest {
    // Telemetry data. An array of ResourceMetrics.
    repeated ResourceMetrics resourceMetrics = 2;
}

// A response to ExportRequest.
message ExportResponse {
    // Response in an empty message.
}

// A list of spans from a Resource.
message ResourceSpans {
  Resource resource = 1;
  repeated Span spans = 2;
}

// A list of metrics from a Resource.
message ResourceMetrics {
  Resource resource = 1;
  repeated Metric metrics = 2;
}
</code></pre>
<p><code>Span</code>, <code>Metric</code> and <code>Resource</code> schema definitions are defined in RFCNNNN (RFC number to be defined and linked from here).</p>
<h2><a class="header" href="#appendix-b---performance-benchmarks" id="appendix-b---performance-benchmarks">Appendix B - Performance Benchmarks</a></h2>
<p>Benchmarking of OTLP vs other telemetry protocols was done using <a href="https://github.com/tigrannajaryan/exp-otelproto">reference implementation in Go</a>.</p>
<h3><a class="header" href="#throughput---sequential-vs-concurrent" id="throughput---sequential-vs-concurrent">Throughput - Sequential vs Concurrent</a></h3>
<p>Using 20 concurrent requests shows the following throughput advantage in benchmarks compared to sequential for various values of network roundtrip latency:</p>
<pre><code>+-----------+-----------------------+
+ Latency   | Concurrent/Sequential |
+           |   Throughput Factor   |
+-----------+-----------------------+
+   0.02 ms |          1.7          |
+   2 ms    |          2.1          |
+  20 ms    |          4.9          |
+ 200 ms    |          6.9          |
+-----------+-----------------------+
</code></pre>
<p>Benchmarking is done using Export requests each carrying 500 spans, each span containing 10 small attributes.</p>
<h3><a class="header" href="#cpu-usage---grpc-vs-websocketexperimental" id="cpu-usage---grpc-vs-websocketexperimental">CPU Usage - gRPC vs WebSocket/Experimental</a></h3>
<p>Experimental implementation using WebSocket transport demonstrated about 30% less CPU usage on small batches compared to gRPC transport and about 7% less CPU usage on large batches.</p>
<p>This shows that exploring different transports with less overhead is a promising future direction.</p>
<h3><a class="header" href="#benchmarking-raw-results" id="benchmarking-raw-results">Benchmarking Raw Results</a></h3>
<p>The following is the benchmarking result, running on on a system with i7 7500U processor, 16 GB RAM. (Note that the benchmarking script sets &quot;performance&quot; CPU governor during execution and sets nice value of the process for more consistent results).</p>
<pre><code>====================================================================================
Legend:
GRPC/Stream/LBTimed/Sync    - GRPC, streaming, load balancer friendly, close stream every 30 sec, with ack
GRPC/Stream/LBTimed/Async/N - OTLP Streaming. GRPC, N streams, load balancer friendly, close stream every 30 sec, with async ack
GRPC/Unary                  - OTLP Unary. One request per batch, load balancer friendly, with ack
GRPC/Unary/Async            - GRPC, unary async request per batch, load balancer friendly, with ack
GRPC/OpenCensus             - OpenCensus protocol, streaming, not load balancer friendly, without ack
GRPC/OpenCensusWithAck      - OpenCensus-like protocol, streaming, not load balancer friendly, with ack
GRPC/Stream/NoLB            - GRPC, streaming, not load balancer friendly, with ack
GRPC/Stream/LBAlways/Sync   - GRPC, streaming, load balancer friendly, close stream after every batch, with ack
GRPC/Stream/LBSrv/Async     - OTLP Streaming. Load balancer friendly, server closes stream every 30 sec or 1000 batches, with async ack
WebSocket/Stream/Sync       - WebSocket, streaming, unknown load balancer friendliness, with sync ack
WebSocket/Stream/Async      - WebSocket, streaming, unknown load balancer friendliness, with async ack
WebSocket/Stream/Async/zlib - WebSocket, streaming, unknown load balancer friendliness, with async ack, zlib compression


8000 small batches, 100 spans per batch, 4 attrs per span
GRPC/Stream/LBTimed/Async/1   800000 spans, CPU time  12.4 sec, wall time   5.3 sec, 645.7 batches/cpusec, 1510.0 batches/wallsec
GRPC/Stream/LBTimed/Async/10  800000 spans, CPU time  12.3 sec, wall time   3.9 sec, 650.9 batches/cpusec, 2058.4 batches/wallsec
GRPC/Unary                    800000 spans, CPU time  15.3 sec, wall time   9.5 sec, 523.2 batches/cpusec, 840.0 batches/wallsec
GRPC/Unary/Async              800000 spans, CPU time  14.1 sec, wall time   4.0 sec, 565.8 batches/cpusec, 1986.3 batches/wallsec
GRPC/OpenCensus               800000 spans, CPU time  21.7 sec, wall time  10.6 sec, 368.7 batches/cpusec, 751.5 batches/wallsec
GRPC/OpenCensusWithAck        800000 spans, CPU time  23.4 sec, wall time  19.0 sec, 342.3 batches/cpusec, 420.8 batches/wallsec
GRPC/Stream/NoLB              800000 spans, CPU time  13.6 sec, wall time   9.4 sec, 588.2 batches/cpusec, 848.7 batches/wallsec
GRPC/Stream/LBAlways/Sync     800000 spans, CPU time  16.1 sec, wall time  10.0 sec, 495.7 batches/cpusec, 798.8 batches/wallsec
GRPC/Stream/LBTimed/Sync      800000 spans, CPU time  13.7 sec, wall time   9.5 sec, 585.7 batches/cpusec, 845.1 batches/wallsec
GRPC/Stream/LBSrv/Async       800000 spans, CPU time  12.7 sec, wall time  12.5 sec, 628.9 batches/cpusec, 639.8 batches/wallsec
WebSocket/Stream/Sync         800000 spans, CPU time   8.4 sec, wall time   8.3 sec, 949.0 batches/cpusec, 965.3 batches/wallsec
WebSocket/Stream/Async        800000 spans, CPU time   9.4 sec, wall time   5.4 sec, 852.0 batches/cpusec, 1492.0 batches/wallsec
WebSocket/Stream/Async/zlib   800000 spans, CPU time  23.3 sec, wall time  16.5 sec, 343.8 batches/cpusec, 484.0 batches/wallsec

800 large batches, 500 spans per batch, 10 attrs per span
GRPC/Stream/LBTimed/Async/1   400000 spans, CPU time  11.4 sec, wall time   7.1 sec, 70.2 batches/cpusec, 113.1 batches/wallsec
GRPC/Stream/LBTimed/Async/10  400000 spans, CPU time  12.2 sec, wall time   5.8 sec, 65.8 batches/cpusec, 138.4 batches/wallsec
GRPC/Unary                    400000 spans, CPU time  10.7 sec, wall time   9.6 sec, 74.7 batches/cpusec, 83.2 batches/wallsec
GRPC/Unary/Async              400000 spans, CPU time  11.9 sec, wall time   5.6 sec, 67.0 batches/cpusec, 141.8 batches/wallsec
GRPC/OpenCensus               400000 spans, CPU time  23.9 sec, wall time  14.1 sec, 33.5 batches/cpusec, 56.8 batches/wallsec
GRPC/OpenCensusWithAck        400000 spans, CPU time  22.0 sec, wall time  21.1 sec, 36.4 batches/cpusec, 38.0 batches/wallsec
GRPC/Stream/NoLB              400000 spans, CPU time  10.7 sec, wall time   9.8 sec, 74.9 batches/cpusec, 81.8 batches/wallsec
GRPC/Stream/LBAlways/Sync     400000 spans, CPU time  11.5 sec, wall time  10.2 sec, 69.9 batches/cpusec, 78.2 batches/wallsec
GRPC/Stream/LBTimed/Sync      400000 spans, CPU time  11.1 sec, wall time  10.2 sec, 71.9 batches/cpusec, 78.4 batches/wallsec
GRPC/Stream/LBSrv/Async       400000 spans, CPU time  11.3 sec, wall time   7.0 sec, 70.5 batches/cpusec, 113.6 batches/wallsec
WebSocket/Stream/Sync         400000 spans, CPU time  10.3 sec, wall time  10.1 sec, 78.0 batches/cpusec, 79.4 batches/wallsec
WebSocket/Stream/Async        400000 spans, CPU time  10.5 sec, wall time   7.2 sec, 76.2 batches/cpusec, 111.2 batches/wallsec
WebSocket/Stream/Async/zlib   400000 spans, CPU time  29.0 sec, wall time  22.1 sec, 27.6 batches/cpusec, 36.1 batches/wallsec

2ms network roundtrip latency
800 large batches, 500 spans per batch, 10 attrs per span
GRPC/Stream/LBTimed/Async/1   400000 spans, CPU time  11.1 sec, wall time   7.0 sec, 71.9 batches/cpusec, 114.9 batches/wallsec
GRPC/Stream/LBTimed/Async/10  400000 spans, CPU time  11.4 sec, wall time   5.4 sec, 70.5 batches/cpusec, 148.0 batches/wallsec
GRPC/Unary                    400000 spans, CPU time  11.5 sec, wall time  11.8 sec, 69.5 batches/cpusec, 68.1 batches/wallsec
GRPC/Unary/Async              400000 spans, CPU time  11.3 sec, wall time   5.3 sec, 70.5 batches/cpusec, 150.4 batches/wallsec
GRPC/OpenCensus               400000 spans, CPU time  23.1 sec, wall time  13.6 sec, 34.6 batches/cpusec, 58.7 batches/wallsec
GRPC/OpenCensusWithAck        400000 spans, CPU time  21.9 sec, wall time  22.6 sec, 36.6 batches/cpusec, 35.4 batches/wallsec
GRPC/Stream/NoLB              400000 spans, CPU time  11.1 sec, wall time  11.6 sec, 72.3 batches/cpusec, 69.2 batches/wallsec
GRPC/Stream/LBAlways/Sync     400000 spans, CPU time  11.5 sec, wall time  11.6 sec, 69.8 batches/cpusec, 68.9 batches/wallsec
GRPC/Stream/LBTimed/Sync      400000 spans, CPU time  11.3 sec, wall time  11.7 sec, 71.0 batches/cpusec, 68.2 batches/wallsec
GRPC/Stream/LBSrv/Async       400000 spans, CPU time  11.1 sec, wall time   6.9 sec, 72.0 batches/cpusec, 115.1 batches/wallsec
WebSocket/Stream/Sync         400000 spans, CPU time  10.8 sec, wall time  12.0 sec, 74.1 batches/cpusec, 66.5 batches/wallsec
WebSocket/Stream/Async        400000 spans, CPU time  10.6 sec, wall time   7.2 sec, 75.5 batches/cpusec, 111.8 batches/wallsec
WebSocket/Stream/Async/zlib   400000 spans, CPU time  28.6 sec, wall time  21.9 sec, 27.9 batches/cpusec, 36.6 batches/wallsec

20ms network roundtrip latency
400 large batches, 500 spans per batch, 10 attrs per span
GRPC/Stream/LBTimed/Async/1   200000 spans, CPU time   6.2 sec, wall time   4.1 sec, 64.9 batches/cpusec, 96.7 batches/wallsec
GRPC/Stream/LBTimed/Async/10  200000 spans, CPU time   6.2 sec, wall time   3.0 sec, 64.0 batches/cpusec, 132.9 batches/wallsec
GRPC/Unary                    200000 spans, CPU time   6.2 sec, wall time  13.5 sec, 64.3 batches/cpusec, 29.6 batches/wallsec
GRPC/Unary/Async              200000 spans, CPU time   5.9 sec, wall time   3.0 sec, 68.0 batches/cpusec, 132.9 batches/wallsec
GRPC/OpenCensus               200000 spans, CPU time  12.6 sec, wall time   7.5 sec, 31.8 batches/cpusec, 53.3 batches/wallsec
GRPC/OpenCensusWithAck        200000 spans, CPU time  12.0 sec, wall time  19.5 sec, 33.4 batches/cpusec, 20.5 batches/wallsec
GRPC/Stream/NoLB              200000 spans, CPU time   5.9 sec, wall time  13.3 sec, 68.3 batches/cpusec, 30.0 batches/wallsec
GRPC/Stream/LBAlways/Sync     200000 spans, CPU time   5.9 sec, wall time  13.3 sec, 68.0 batches/cpusec, 30.2 batches/wallsec
GRPC/Stream/LBTimed/Sync      200000 spans, CPU time   5.8 sec, wall time  13.3 sec, 69.3 batches/cpusec, 30.1 batches/wallsec
GRPC/Stream/LBSrv/Async       200000 spans, CPU time   5.5 sec, wall time   3.7 sec, 73.4 batches/cpusec, 107.3 batches/wallsec
WebSocket/Stream/Sync         200000 spans, CPU time   5.8 sec, wall time  14.6 sec, 69.4 batches/cpusec, 27.4 batches/wallsec
WebSocket/Stream/Async        200000 spans, CPU time   5.5 sec, wall time   3.9 sec, 72.3 batches/cpusec, 102.1 batches/wallsec
WebSocket/Stream/Async/zlib   200000 spans, CPU time  14.7 sec, wall time  11.2 sec, 27.3 batches/cpusec, 35.7 batches/wallsec

200ms network roundtrip latency
40 large batches, 500 spans per batch, 10 attrs per span
GRPC/Stream/LBTimed/Async/1    20000 spans, CPU time   0.5 sec, wall time   3.1 sec, 74.1 batches/cpusec, 12.7 batches/wallsec
GRPC/Stream/LBTimed/Async/10   20000 spans, CPU time   0.7 sec, wall time   3.1 sec, 61.5 batches/cpusec, 12.8 batches/wallsec
GRPC/Unary                     20000 spans, CPU time   0.6 sec, wall time   9.9 sec, 65.6 batches/cpusec,  4.0 batches/wallsec
GRPC/Unary/Async               20000 spans, CPU time   0.6 sec, wall time   3.6 sec, 65.6 batches/cpusec, 11.1 batches/wallsec
GRPC/OpenCensus                20000 spans, CPU time   1.1 sec, wall time   3.5 sec, 35.1 batches/cpusec, 11.3 batches/wallsec
GRPC/OpenCensusWithAck         20000 spans, CPU time   1.2 sec, wall time  10.2 sec, 32.8 batches/cpusec,  3.9 batches/wallsec
GRPC/Stream/NoLB               20000 spans, CPU time   0.6 sec, wall time   9.5 sec, 67.8 batches/cpusec,  4.2 batches/wallsec
GRPC/Stream/LBAlways/Sync      20000 spans, CPU time   0.6 sec, wall time   9.5 sec, 63.5 batches/cpusec,  4.2 batches/wallsec
GRPC/Stream/LBTimed/Sync       20000 spans, CPU time   0.6 sec, wall time   9.5 sec, 66.7 batches/cpusec,  4.2 batches/wallsec
GRPC/Stream/LBSrv/Async        20000 spans, CPU time   0.5 sec, wall time   3.3 sec, 74.1 batches/cpusec, 12.0 batches/wallsec
WebSocket/Stream/Sync          20000 spans, CPU time   0.6 sec, wall time  13.5 sec, 69.0 batches/cpusec,  3.0 batches/wallsec
WebSocket/Stream/Async         20000 spans, CPU time   0.5 sec, wall time   6.1 sec, 74.1 batches/cpusec,  6.5 batches/wallsec
WebSocket/Stream/Async/zlib    20000 spans, CPU time   1.5 sec, wall time   2.0 sec, 26.3 batches/cpusec, 19.8 batches/wallsec


400 large batches, 500 spans per batch, 10 attrs per span
200ms network roundtrip latency
GRPC/OpenCensus               200000 spans, CPU time  11.9 sec, wall time  10.1 sec, 33.6 batches/cpusec, 39.6 batches/wallsec
GRPC/Stream/LBTimed/Async/1   200000 spans, CPU time   5.3 sec, wall time   9.5 sec, 76.0 batches/cpusec, 41.9 batches/wallsec
GRPC/Stream/LBTimed/Async/10  200000 spans, CPU time   6.4 sec, wall time   8.9 sec, 62.3 batches/cpusec, 44.7 batches/wallsec
GRPC/Unary/Async              200000 spans, CPU time   5.8 sec, wall time  12.0 sec, 68.6 batches/cpusec, 33.3 batches/wallsec
WebSocket/Stream/Async        200000 spans, CPU time   5.3 sec, wall time  11.2 sec, 75.3 batches/cpusec, 35.7 batches/wallsec
WebSocket/Stream/Async/zlib   200000 spans, CPU time  15.1 sec, wall time  12.0 sec, 26.5 batches/cpusec, 33.4 batches/wallsec
====================================================================================
</code></pre>
<h2><a class="header" href="#glossary" id="glossary">Glossary</a></h2>
<p>There are 2 parties involved in telemetry data exchange. In this document the party that is the source of telemetry data is called the <code>Client</code>, the party that is the destination of telemetry data is called the <code>Server</code>.</p>
<p><img src="images/otlp-client-server.png" alt="Client-Server" /></p>
<p>Examples of a Client are instrumented applications or sending side of telemetry collectors, examples of Servers are telemetry backends or receiving side of telemetry collectors (so a Collector is typically both a Client and a Server depending on which side you look from).</p>
<p>Both the Client and the Server are also a <code>Node</code>. This term is used in the document when referring to either one.</p>
<h2><a class="header" href="#acknowledgements" id="acknowledgements">Acknowledgements</a></h2>
<p>Special thanks to Owais Lone who helped to conduct experiments with Load Balancers, to Paulo Janotti, Bogdan Drutu and Yuri Shkuro for thoughtful discussions around the protocol.</p>
<h1><a class="header" href="#version-semantic-attribute" id="version-semantic-attribute">Version Semantic Attribute</a></h1>
<p>Add a standard <code>version</code> semantic attribute.</p>
<h2><a class="header" href="#motivation-11" id="motivation-11">Motivation</a></h2>
<p>When creating trace data or metrics, it can be extremely useful to know the specific version that
emitted the iota of span or measurement being viewed. However, versions can mean different things
to different systems and users. In addition, downstream analysis systems may wish to expose
functionality related to the type of a version (such as detecting when versions are newer or older).
To support this, we should standardize a <code>version</code> attribute with optional hints as to the type of the
version.</p>
<h2><a class="header" href="#explanation-8" id="explanation-8">Explanation</a></h2>
<p>A <code>version</code> is a semantic attribute that can be applied to other resources, such as <code>Service</code>,
<code>Component</code>, <code>Library</code>, <code>Device</code>, <code>Platform</code>, etc. A <code>version</code> attribute is optional, but recommended.
The definition of a <code>version</code> is a key-value attribute pair of <code>string</code> to <code>string</code>, with naming schemas
available to hint at the type of a version, such as the following:</p>
<p><code>version=semver:1.2.3</code> (a semantic version)
<code>version=git:8ae73a</code> (a git sha hash)
<code>version=0.0.4.2.20190921</code> (a untyped version)</p>
<h2><a class="header" href="#internal-details-6" id="internal-details-6">Internal details</a></h2>
<p>Since this is just an attribute pair, no special handling is required, although SDKs may provide helper methods
to construct schema-appropriate values.</p>
<h2><a class="header" href="#prior-art-and-alternatives-7" id="prior-art-and-alternatives-7">Prior art and alternatives</a></h2>
<p>Tagging service resources with their version is generally suggested by analysis tools -- see <a href="https://www.jaegertracing.io/docs/1.8/client-features/">JAEGER_TAGS</a> for an example -- but lacks standardization.</p>
<h1><a class="header" href="#metric-labelset-specification" id="metric-labelset-specification">Metric <code>LabelSet</code> specification</a></h1>
<p>Introduce a first-class <code>LabelSet</code> API type as a handle on a pre-defined set of labels for the Metrics API.</p>
<h2><a class="header" href="#motivation-12" id="motivation-12">Motivation</a></h2>
<p>Labels are the term for key-value pairs used in the OpenTelemetry Metrics API.  Treatment of labels in the Metrics API is especially important for performance across a variety of export strategies.</p>
<p>Label serialization is often one of the most expensive tasks when processing metric events. Creating a <code>LabelSet</code> once and re-using it many times can greatly reduce the overall cost of processing many events.</p>
<p>The Metrics API supports three calling conventions: the Handle convention, the Direct convention, and the Batch convention. Each of these conventions stands to benefit when a <code>LabelSet</code> is re-used, as it allows the SDK to process the label set once instead of once per call.  Whenever more than one handle will be created with the same labels, more than one instrument will be called directly with the same labels, or more than one batch of metric events will be recorded with the same labels, re-using a <code>LabelSet</code> makes it possible for the SDK to improve performance.</p>
<h2><a class="header" href="#explanation-9" id="explanation-9">Explanation</a></h2>
<p>Metric instrument APIs which presently take labels in the form <code>{ Key: Value, ... }</code> will be updated to take an explicit <code>LabelSet</code>.  The <code>Meter.Labels()</code> API method supports getting a <code>LabelSet</code> from the API, allowing the programmer to acquire a pre-defined label set.  Here are several examples of <code>LabelSet</code> re-use.  Assume we have two instruments:</p>
<pre><code class="language-golang">var (
    cumulative = metric.NewFloat64Cumulative(&quot;my_counter&quot;)
    gauge      = metric.NewFloat64Gauge(&quot;my_gauge&quot;)
)
</code></pre>
<p>Use a <code>LabelSet</code> to construct multiple Handles:</p>
<pre><code class="language-golang">var (
    labels  = meter.Labels({ &quot;required_key1&quot;: value1, &quot;required_key2&quot;: value2 })
    chandle = cumulative.GetHandle(labels)
    ghandle = gauge.GetHandle(labels)
)
for ... {
   // ...
   chandle.Add(...)
   ghandle.Set(...)
}
</code></pre>
<p>Use a <code>LabelSet</code> to make multiple Direct calls:</p>
<pre><code class="language-golang">labels := meter.Labels({ &quot;required_key1&quot;: value1, &quot;required_key2&quot;: value2 })
cumulative.Add(quantity, labels)
gauge.Set(quantity, labels)
</code></pre>
<p>Of course, repeated calls to <code>Meter.RecordBatch()</code> could re-use a <code>LabelSet</code> as well.</p>
<h3><a class="header" href="#ordered-labelset-option" id="ordered-labelset-option">Ordered <code>LabelSet</code> option</a></h3>
<p>As a language-level decision, APIs may support <em>ordered</em> LabelSet
construction, in which a pre-defined set of ordered label keys is
defined such that values can be supplied in order.  This allows a
faster code path to construct the <code>LabelSet</code>.  For example,</p>
<pre><code class="language-golang">
var rpcLabelKeys = meter.OrderedLabelKeys(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)

for _, input := range stream {
    labels := rpcLabelKeys.Values(1, 2, 3)  // a=1, b=2, c=3

    // ...
}
</code></pre>
<p>This is specified as a language-optional feature because its safety,
and therefore its value as an input for monitoring, depends on the
availability of type-checking in the source language.  Passing
unordered labels (i.e., a list of bound keys and values) to
<code>Meter.Labels(...)</code> is considered the safer alternative.</p>
<h3><a class="header" href="#interaction-with-named-meters" id="interaction-with-named-meters">Interaction with &quot;Named&quot; Meters</a></h3>
<p>LabelSet values may be used with any named Meter originating from the
same Meter provider.  That is, LabelSets acquired through a named
Meter may be used by any Meter from the same Meter provider.</p>
<h2><a class="header" href="#internal-details-7" id="internal-details-7">Internal details</a></h2>
<p>Metric SDKs that do not or cannot take advantage of the LabelSet optimizations are not especially burdened by having to support these APIs.  It is trivial to supply an implementation of <code>LabelSet</code> that simply stores a list of labels.  This may not be acceptable in performance-critical applications, but this is the common case in many metrics and diagnostics APIs today.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-6" id="trade-offs-and-mitigations-6">Trade-offs and mitigations</a></h2>
<p>In languages where overloading is a standard convenience, the metrics API may elect to offer alternate forms that elide the call to <code>Meter.Labels()</code>, for example:</p>
<pre><code>instrument.GetHandle({ Key: Value, ... })
</code></pre>
<p>as opposed to this:</p>
<pre><code>instrument.GetHandle(meter.Labels({ Key: Value, ... }))
</code></pre>
<p>A key distinction between <code>LabelSet</code> and similar concepts in existing metrics libraries is that it is a <em>write-only</em> structure.  <code>LabelSet</code> allows the developer to input metric labels without being able to read them back.  This avoids forcing the SDK to retain a reference to memory that is not required.</p>
<h2><a class="header" href="#prior-art-and-alternatives-8" id="prior-art-and-alternatives-8">Prior art and alternatives</a></h2>
<p>Some existing metrics APIs support this concept.  For example, see <code>Scope</code> in the <a href="https://godoc.org/github.com/uber-go/tally#Scope">Tally metric API for Go</a>.</p>
<p>Some libraries take <code>LabelSet</code> one step further.  In the future, we may add to the the <code>LabelSet</code> API a method to extend the label set with additional labels.  For example:</p>
<pre><code>serviceLabels := meter.Labels({ &quot;k1&quot;: &quot;v1&quot;, &quot;k2&quot;: &quot;v2&quot; })
// ...
requestLabels := serviceLabels.With({ &quot;k3&quot;: &quot;v3&quot;, &quot;k4&quot;: &quot;v4&quot; })
</code></pre>
<h1><a class="header" href="#otlp-trace-data-format" id="otlp-trace-data-format">OTLP Trace Data Format</a></h1>
<p><strong>Author</strong>: Tigran Najaryan, Splunk</p>
<p>OTLP Trace Data Format specification describes the structure of the trace data that is transported by OpenTelemetry Protocol (RFC0035).</p>
<h2><a class="header" href="#motivation-13" id="motivation-13">Motivation</a></h2>
<p>This document is a continuation of OpenTelemetry Protocol RFC0035 and is necessary part of OTLP specification.</p>
<h2><a class="header" href="#explanation-10" id="explanation-10">Explanation</a></h2>
<p>OTLP Trace Data Format is primarily inherited from OpenCensus protocol. Several changes are introduced with the goal of more efficient serialization. Notable differences from OpenCensus protocol are:</p>
<ol>
<li>Removed <code>Node</code> as a concept.</li>
<li>Extended <code>Resource</code> to better describe the source of the telemetry data.</li>
<li>Replaced attribute maps by lists of key/value pairs.</li>
<li>Eliminated unnecessary additional nesting in various values.</li>
</ol>
<p>Changes 1-2 are conceptual, changes 3-4 improve performance.</p>
<h2><a class="header" href="#internal-details-8" id="internal-details-8">Internal details</a></h2>
<p>This section specifies data format in Protocol Buffers.</p>
<h3><a class="header" href="#resource" id="resource">Resource</a></h3>
<pre><code class="language-protobuf">// Resource information. This describes the source of telemetry data.
message Resource {
  // labels is a collection of attributes that describe the resource. See OpenTelemetry
  // specification semantic conventions for standardized label names:
  // https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/data-resource-semantic-conventions.md
  repeated AttributeKeyValue labels = 1;

  // dropped_labels_count is the number of dropped labels. If the value is 0, then
  // no labels were dropped.
  int32 dropped_labels_count = 2;
}
</code></pre>
<h3><a class="header" href="#span" id="span">Span</a></h3>
<pre><code class="language-protobuf">// Span represents a single operation within a trace. Spans can be
// nested to form a trace tree. Spans may also be linked to other spans
// from the same or different trace and form graphs. Often, a trace
// contains a root span that describes the end-to-end latency, and one
// or more subspans for its sub-operations. A trace can also contain
// multiple root spans, or none at all. Spans do not need to be
// contiguous - there may be gaps or overlaps between spans in a trace.
//
// The next field id is 18.
message Span {
  // trace_id is the unique identifier of a trace. All spans from the same trace share
  // the same `trace_id`. The ID is a 16-byte array. An ID with all zeroes
  // is considered invalid.
  //
  // This field is semantically required. If empty or invalid trace_id is received:
  // - The receiver MAY reject the invalid data and respond with the appropriate error
  //   code to the sender.
  // - The receiver MAY accept the invalid data and attempt to correct it.
  bytes trace_id = 1;

  // span_id is a unique identifier for a span within a trace, assigned when the span
  // is created. The ID is an 8-byte array. An ID with all zeroes is considered
  // invalid.
  //
  // This field is semantically required. If empty or invalid span_id is received:
  // - The receiver MAY reject the invalid data and respond with the appropriate error
  //   code to the sender.
  // - The receiver MAY accept the invalid data and attempt to correct it.
  bytes span_id = 2;

  // TraceStateEntry is the entry that is repeated in tracestate field (see below).
  message TraceStateEntry {
    // key must begin with a lowercase letter, and can only contain
    // lowercase letters 'a'-'z', digits '0'-'9', underscores '_', dashes
    // '-', asterisks '*', and forward slashes '/'.
    string key = 1;

    // value is opaque string up to 256 characters printable ASCII
    // RFC0020 characters (i.e., the range 0x20 to 0x7E) except ',' and '='.
    // Note that this also excludes tabs, newlines, carriage returns, etc.
    string value = 2;
  }

  // tracestate conveys information about request position in multiple distributed tracing graphs.
  // It is a collection of TracestateEntry with a maximum of 32 members in the collection.
  //
  // See the https://github.com/w3c/distributed-tracing for more details about this field.
  repeated TraceStateEntry tracestate = 3;

  // parent_span_id is the `span_id` of this span's parent span. If this is a root span, then this
  // field must be omitted. The ID is an 8-byte array.
  bytes parent_span_id = 4;

  // resource that is associated with this span. Optional. If not set, this span
  // should be part of a ResourceSpans message that does include the resource information,
  // unless resource information is unknown.
  Resource resource = 5;

  // name describes the span's operation.
  //
  // For example, the name can be a qualified method name or a file name
  // and a line number where the operation is called. A best practice is to use
  // the same display name at the same call point in an application.
  // This makes it easier to correlate spans in different traces.
  //
  // This field is semantically required to be set to non-empty string.
  //
  // This field is required.
  string name = 6;

  // SpanKind is the type of span. Can be used to specify additional relationships between spans
  // in addition to a parent/child relationship.
  enum SpanKind {
    // Unspecified. Do NOT use as default.
    // Implementations MAY assume SpanKind to be INTERNAL when receiving UNSPECIFIED.
    SPAN_KIND_UNSPECIFIED = 0;

    // Indicates that the span represents an internal operation within an application,
    // as opposed to an operations happening at the boundaries. Default value.
    INTERNAL = 1;

    // Indicates that the span covers server-side handling of an RPC or other
    // remote network request.
    SERVER = 2;

    // Indicates that the span describes a request to some remote service.
    CLIENT = 3;

    // Indicates that the span describes a producer sending a message to a broker.
    // Unlike CLIENT and SERVER, there is often no direct critical path latency relationship
    // between producer and consumer spans. A PRODUCER span ends when the message was accepted
    // by the broker while the logical processing of the message might span a much longer time.
    PRODUCER = 4;

    // Indicates that the span describes consumer receiving a message from a broker.
    // Like the PRODUCER kind, there is often no direct critical path latency relationship
    // between producer and consumer spans.
    CONSUMER = 5;
  }

  // kind field distinguishes between spans generated in a particular context. For example,
  // two spans with the same name may be distinguished using `CLIENT` (caller)
  // and `SERVER` (callee) to identify network latency associated with the span.
  SpanKind kind = 7;

  // start_time_unixnano is the start time of the span. On the client side, this is the time
  // kept by the local machine where the span execution starts. On the server side, this
  // is the time when the server's application handler starts running.
  //
  // This field is semantically required and it is expected that end_time &gt;= start_time.
  //
  // This field is required.
  int64 start_time_unixnano = 8;

  // end_time_unixnano is the end time of the span. On the client side, this is the time
  // kept by the local machine where the span execution ends. On the server side, this
  // is the time when the server application handler stops running.
  //
  // This field is semantically required and it is expected that end_time &gt;= start_time.
  //
  // This field is required.
  int64 end_time_unixnano = 9;

  // attributes is a collection of key/value pairs. The value can be a string,
  // an integer, a double or the Boolean values `true` or `false`. Note, global attributes
  // like server name can be set using the resource API. Examples of attributes:
  //
  //     &quot;/http/user_agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36&quot;
  //     &quot;/http/server_latency&quot;: 300
  //     &quot;abc.com/myattribute&quot;: true
  //     &quot;abc.com/score&quot;: 10.239
  repeated AttributeKeyValue attributes = 10;

  // dropped_attributes_count is the number of attributes that were discarded. Attributes
  // can be discarded because their keys are too long or because there are too many
  // attributes. If this value is 0, then no attributes were dropped.
  int32 dropped_attributes_count = 11;

  // TimedEvent is a time-stamped annotation of the span, consisting of either
  // user-supplied key-value pairs, or details of a message sent/received between Spans.
  message TimedEvent {
    // time_unixnano is the time the event occurred.
    int64 time_unixnano = 1;

    // name is a user-supplied description of the event.
    string name = 2;

    // attributes is a collection of attribute key/value pairs on the event.
    repeated AttributeKeyValue attributes = 3;

    // dropped_attributes_count is the number of dropped attributes. If the value is 0,
    // then no attributes were dropped.
    int32 dropped_attributes_count = 4;
  }

  // timed_events is a collection of TimedEvent items.
  repeated TimedEvent timed_events = 12;

  // dropped_timed_events_count is the number of dropped timed events. If the value is 0,
  // then no events were dropped.
  int32 dropped_timed_events_count = 13;

  // Link is a pointer from the current span to another span in the same trace or in a
  // different trace. For example, this can be used in batching operations,
  // where a single batch handler processes multiple requests from different
  // traces or when the handler receives a request from a different project.
  // See also Links specification:
  // https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/overview.md#links-between-spans
  message Link {
    // trace_id is a unique identifier of a trace that this linked span is part of.
    // The ID is a 16-byte array.
    bytes trace_id = 1;

    // span_id is a unique identifier for the linked span. The ID is an 8-byte array.
    bytes span_id = 2;

    // tracestate is the trace state associated with the link.
    repeated TraceStateEntry tracestate = 3;

    // attributes is a collection of attribute key/value pairs on the link.
    repeated AttributeKeyValue attributes = 4;

    // dropped_attributes_count is the number of dropped attributes. If the value is 0,
    // then no attributes were dropped.
    int32 dropped_attributes_count = 5;
  }

  // links is a collection of Links, which are references from this span to a span
  // in the same or different trace.
  repeated Link links = 14;

  // dropped_links_count is the number of dropped links after the maximum size was
  // enforced. If this value is 0, then no links were dropped.
  int32 dropped_links_count = 15;

  // status is an optional final status for this span. Semantically when status
  // wasn't set it is means span ended without errors and assume Status.Ok (code = 0).
  Status status = 16;

  // child_span_count is an optional number of local child spans that were generated while this
  // span was active. If set, allows an implementation to detect missing child spans.
  int32 child_span_count = 17;
}

// The Status type defines a logical error model that is suitable for different
// programming environments, including REST APIs and RPC APIs.
message Status {

  // StatusCode mirrors the codes defined at
  // https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/api-tracing.md#statuscanonicalcode
  enum StatusCode {
    Ok                 = 0;
    Cancelled          = 1;
    UnknownError       = 2;
    InvalidArgument    = 3;
    DeadlineExceeded   = 4;
    NotFound           = 5;
    AlreadyExists      = 6;
    PermissionDenied   = 7;
    ResourceExhausted  = 8;
    FailedPrecondition = 9;
    Aborted            = 10;
    OutOfRange         = 11;
    Unimplemented      = 12;
    InternalError      = 13;
    Unavailable        = 14;
    DataLoss           = 15;
    Unauthenticated    = 16;
  };

  // The status code. This is optional field. It is safe to assume 0 (OK)
  // when not set.
  StatusCode code = 1;

  // A developer-facing human readable error message.
  string message = 2;
}
</code></pre>
<h3><a class="header" href="#attributekeyvalue" id="attributekeyvalue">AttributeKeyValue</a></h3>
<pre><code class="language-protobuf">// AttributeKeyValue is a key-value pair that is used to store Span attributes, Resource
// labels, etc.
message AttributeKeyValue {
  // ValueType is the enumeration of possible types that value can have.
  enum ValueType {
    STRING  = 0;
    BOOL    = 1;
    INT64   = 2;
    DOUBLE  = 3;
  };

  // key part of the key-value pair.
  string key = 1;

  // The type of the value.
  ValueType type = 2;

  // Only one of the following fields is supposed to contain data (determined by `type` field value).
  // This is deliberately not using Protobuf `oneof` for performance reasons (verified by benchmarks).

  // A string value.
  string string_value = 3;
  // A 64-bit signed integer.
  int64 int64_value = 4;
  // A Boolean value represented by `true` or `false`.
  bool bool_value = 5;
  // A double value.
  double double_value = 6;
}
</code></pre>
<h2><a class="header" href="#trade-offs-and-mitigations-7" id="trade-offs-and-mitigations-7">Trade-offs and mitigations</a></h2>
<p>Timestamps were changed from google.protobuf.Timestamp to a int64 representation in Unix epoch nanoseconds. This change reduces the type-safety but benchmarks show that for small spans there is 15-20% encoding/decoding CPU speed gain. This is the right trade-off to make because encoding/decoding CPU consumption tends to dominate many workloads (particularly in OpenTelemetry Service).</p>
<h2><a class="header" href="#prior-art-and-alternatives-9" id="prior-art-and-alternatives-9">Prior art and alternatives</a></h2>
<p>OpenCensus and Jaeger protocol buffer data schemas were used as the inspiration for this specification. OpenCensus was the starting point, Jaeger provided performance improvement ideas.</p>
<h2><a class="header" href="#open-questions-5" id="open-questions-5">Open questions</a></h2>
<p>A follow up RFC is required to define the data format for metrics.</p>
<p>One of the original aspiring goals for OTLP was to <em>&quot;support very fast pass-through mode (when no modifications to the data are needed), fast augmenting or tagging of data and partial inspection of data&quot;</em>. This particular goal was not met directly (although performance improvements over OpenCensus encoding make OTLP more suitable for these tasks). This goal remains a good direction of future research and improvement.</p>
<h2><a class="header" href="#appendix-a---benchmarking" id="appendix-a---benchmarking">Appendix A - Benchmarking</a></h2>
<p>The following shows <a href="https://github.com/tigrannajaryan/exp-otelproto/">benchmarking of encoding/decoding in Go</a> using various schemas.</p>
<p>Legend:</p>
<ul>
<li>OpenCensus    - OpenCensus protocol schema.</li>
<li>OTLP/AttrMap  - OTLP schema using map for attributes.</li>
<li>OTLP/AttrList - OTLP schema using list of key/values for attributes and with reduced nesting for values.</li>
<li>OTLP/AttrList/TimeWrapped - Same as OTLP/AttrList, except using google.protobuf.Timestamp instead of int64 for timestamps.</li>
</ul>
<p>Suffixes:</p>
<ul>
<li>Attributes - a span with 3 attributes.</li>
<li>TimedEvent - a span with 3 timed events.</li>
</ul>
<pre><code>BenchmarkEncode/OpenCensus/Attributes-8                                 10  605614915 ns/op
BenchmarkEncode/OpenCensus/TimedEvent-8                                 10 1025026687 ns/op
BenchmarkEncode/OTLP/AttrAsMap/Attributes-8                             10  519539723 ns/op
BenchmarkEncode/OTLP/AttrAsMap/TimedEvent-8                             10  841371163 ns/op
BenchmarkEncode/OTLP/AttrAsList/Attributes-8                            50  128790429 ns/op
BenchmarkEncode/OTLP/AttrAsList/TimedEvent-8                            50  175874878 ns/op
BenchmarkEncode/OTLP/AttrAsList/TimeWrapped/Attributes-8                50  153184772 ns/op
BenchmarkEncode/OTLP/AttrAsList/TimeWrapped/TimedEvent-8                30  232705272 ns/op
BenchmarkDecode/OpenCensus/Attributes-8                                 10  644103382 ns/op
BenchmarkDecode/OpenCensus/TimedEvent-8                                  5 1132059855 ns/op
BenchmarkDecode/OTLP/AttrAsMap/Attributes-8                             10  529679038 ns/op
BenchmarkDecode/OTLP/AttrAsMap/TimedEvent-8                             10  867364162 ns/op
BenchmarkDecode/OTLP/AttrAsList/Attributes-8                            50  228834160 ns/op
BenchmarkDecode/OTLP/AttrAsList/TimedEvent-8                            20  321160309 ns/op
BenchmarkDecode/OTLP/AttrAsList/TimeWrapped/Attributes-8                30  277597851 ns/op
BenchmarkDecode/OTLP/AttrAsList/TimeWrapped/TimedEvent-8                20  443386880 ns/op
</code></pre>
<p>The benchmark encodes/decodes 1000 batches of 100 spans, each span containing 3 attributes or 3 timed events. The total uncompressed, encoded size of each batch is around 20KBytes.</p>
<p>The results show OTLP/AttrList is 5-6 times faster than OpenCensus in encoding and about 3 times faster in decoding.</p>
<p>Using google.protobuf.Timestamp instead of int64-encoded unix timestamp results in 1.18-1.32 times slower encoding and 1.21-1.38 times slower decoding (depending on what the span contains).</p>
<h1><a class="header" href="#context-propagation-a-layered-approach" id="context-propagation-a-layered-approach">Context Propagation: A Layered Approach</a></h1>
<ul>
<li><a href="0066-separate-context-propagation.html#Motivation">Motivation</a></li>
<li><a href="0066-separate-context-propagation.html#OpenTelemetry-layered-architecture">OpenTelemetry layered architecture</a>
<ul>
<li><a href="0066-separate-context-propagation.html#Cross-Cutting-Concerns">Cross-Cutting Concerns</a>
<ul>
<li><a href="0066-separate-context-propagation.html#Observability-API">Observability API</a></li>
<li><a href="0066-separate-context-propagation.html#Correlations-API">Correlations API</a></li>
</ul>
</li>
<li><a href="0066-separate-context-propagation.html#Context-Propagation">Context Propagation</a>
<ul>
<li><a href="0066-separate-context-propagation.html#Context-API">Context API</a></li>
<li><a href="0066-separate-context-propagation.html#Propagation-API">Propagation API</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="0066-separate-context-propagation.html#Prototypes">Prototypes</a></li>
<li><a href="0066-separate-context-propagation.html#Examples">Examples</a>
<ul>
<li><a href="0066-separate-context-propagation.html#Global-initialization">Global initialization</a></li>
<li><a href="0066-separate-context-propagation.html#Extracting-and-injecting-from-HTTP-headers">Extracting and injecting from HTTP headers</a></li>
<li><a href="0066-separate-context-propagation.html#Simplify-the-API-with-automated-context-propagation">Simplify the API with automated context propagation</a></li>
<li><a href="0066-separate-context-propagation.html#Implementing-a-propagator">Implementing a propagator</a></li>
<li><a href="0066-separate-context-propagation.html#Implementing-a-concern">Implementing a concern</a></li>
<li><a href="0066-separate-context-propagation.html#The-scope-of-current-context">The scope of current context</a></li>
<li><a href="0066-separate-context-propagation.html#Referencing-multiple-contexts">Referencing multiple contexts</a></li>
<li><a href="0066-separate-context-propagation.html#Falling-back-to-explicit-contexts">Falling back to explicit contexts</a></li>
</ul>
</li>
<li><a href="0066-separate-context-propagation.html#Internal-details">Internal details</a></li>
<li><a href="0066-separate-context-propagation.html#faq">FAQ</a></li>
</ul>
<p><img src="img/0066_context_propagation_overview.png" alt="drawing" /></p>
<p>A proposal to refactor OpenTelemetry into a set of separate cross-cutting concerns which
operate on a shared context propagation mechanism.</p>
<h2><a class="header" href="#motivation-14" id="motivation-14">Motivation</a></h2>
<p>This RFC addresses the following topics:</p>
<h3><a class="header" href="#separation-of-concerns" id="separation-of-concerns">Separation of concerns</a></h3>
<ul>
<li>Cleaner package layout results in an easier to learn system. It is possible to
understand Context Propagation without needing to understand Observability.</li>
<li>Allow for multiple types of context propagation, each self contained with
different rules. For example, TraceContext may be sampled, while
CorrelationContext never is.</li>
<li>Allow the Observability and Context Propagation to have different defaults.
The Observability systems ships with a no-op implementation and a pluggable SDK,
the context propagation system ships with a canonical, working implementation.</li>
</ul>
<h3><a class="header" href="#extensibility" id="extensibility">Extensibility</a></h3>
<ul>
<li>A clean separation allows the context propagation mechanisms to be used on
their own, so they may be consumed by other systems which do not want to
depend on an observability tool for their non-observability concerns.</li>
<li>Allow developers to create new applications for context propagation. For
example: A/B testing, authentication, and network switching.</li>
</ul>
<h2><a class="header" href="#opentelemetry-layered-architecture" id="opentelemetry-layered-architecture">OpenTelemetry layered architecture</a></h2>
<p>The design of OpenTelemetry is based on the principles of <a href="https://en.wikipedia.org/wiki/Aspect-oriented_programming">aspect-oriented
programming</a>,
adopted to the needs of distributed systems.</p>
<p>Some concerns &quot;cut across&quot; multiple abstractions in a program. Logging
exemplifies aspect orientation because a logging strategy necessarily affects
every logged part of the system. Logging thereby &quot;cross-cuts&quot; across all logged
classes and methods. Distributed tracing takes this strategy to the next level,
and cross-cuts across all classes and methods in all services in the entire
transaction. This requires a distributed form of the same aspect-oriented
programming principles in order to be implemented cleanly.</p>
<p>OpenTelemetry approaches this by separating it's design into two layers. The top
layer contains a set of independent <strong>cross-cutting concerns</strong>, which intertwine
with a program's application logic and cannot be cleanly encapsulated. All
concerns share an underlying distributed <strong>context propagation</strong> layer, for
storing state and accessing data across the lifespan of a distributed
transaction.</p>
<h2><a class="header" href="#cross-cutting-concerns" id="cross-cutting-concerns">Cross-Cutting Concerns</a></h2>
<h3><a class="header" href="#observability-api" id="observability-api">Observability API</a></h3>
<p>Distributed tracing is one example of a cross-cutting concern. Tracing code is
interleaved with regular code, and ties together independent code modules which
would otherwise remain encapsulated. Tracing is also distributed, and requires
transaction-level context propagation in order to execute correctly.</p>
<p>The various observability APIs are not described here directly. However, in this new
design, all observability APIs would be modified to make use of the generalized
context propagation mechanism described below, rather than the tracing-specific
propagation system it uses today.</p>
<p>Note that OpenTelemetry APIs calls should <em>always</em> be given access to the entire
context object, and never just a subset of the context, such as the value in a
single key. This allows the SDK to make improvements and leverage additional
data that may be available, without changes to all of the call sites.</p>
<p>The following are notes on the API, and not meant as final.</p>
<p><strong><code>StartSpan(context, options) -&gt; context</code></strong>
When a span is started, a new context is returned, with the new span set as the
current span.</p>
<p><strong><code>GetSpanPropagator() -&gt; (HTTP_Extractor, HTTP_Injector)</code></strong><br />
When a span is extracted, the extracted value is stored in the context seprately
from the current span.</p>
<h3><a class="header" href="#correlations-api" id="correlations-api">Correlations API</a></h3>
<p>In addition to trace propagation, OpenTelemetry provides a simple mechanism for
propagating indexes, called the Correlations API. Correlations are
intended for indexing observability events in one service with attributes
provided by a prior service in the same transaction. This helps to establish a
causal relationship between these events. For example, determining that a
particular browser version is associated with a failure in an image processing
service.</p>
<p>The Correlations API is based on the <a href="https://w3c.github.io/correlation-context/">W3C Correlation-Context specification</a>,
and implements the protocol as it is defined in that working group. There are
few details provided here as it is outside the scope of this OTEP to finalize
this API.</p>
<p>While Correlations can be used to prototype other cross-cutting concerns, this
mechanism is primarily intended to convey values for the OpenTelemetry
observability systems.</p>
<p>For backwards compatibility, OpenTracing Baggage is propagated as Correlations
when using the OpenTracing bridge. New concerns with different criteria should
be modeled separately, using the same underlying context propagation layer as
building blocks.</p>
<p>The following is an example API, and not meant as final.</p>
<p><strong><code>GetCorrelation(context, key) -&gt; value</code></strong><br />
To access the value for a label set by a prior event, the Correlations API
provides a function which takes a context and a key as input, and returns a
value.</p>
<p><strong><code>SetCorrelation(context, key, value) -&gt; context</code></strong><br />
To record the value for a label, the Correlations API provides a function which
takes a context, a key, and a value as input, and returns an updated context
which contains the new value.</p>
<p><strong><code>RemoveCorrelation(context, key) -&gt; context</code></strong><br />
To delete a label, the Correlations API provides a function
which takes a context and a key as input, and returns an updated context which
no longer contains the selected key-value pair.</p>
<p><strong><code>ClearCorrelations(context) -&gt; context</code></strong><br />
To avoid sending any labels to an untrusted process, the Correlation API
provides a function to remove all Correlations from a context.</p>
<p><strong><code>GetCorrelationPropagator() -&gt; (HTTP_Extractor, HTTP_Injector)</code></strong><br />
To deserialize the previous labels set by prior processes, and to serialize the
current total set of labels and send them to the next process, the Correlations
API provides a function which returns a Correlation-specific implementation of
the <code>HTTPExtract</code> and <code>HTTPInject</code> functions found in the Propagation API.</p>
<h2><a class="header" href="#context-propagation" id="context-propagation">Context Propagation</a></h2>
<h3><a class="header" href="#context-api" id="context-api">Context API</a></h3>
<p>Cross-cutting concerns access data in-process using the same, shared context
object. Each concern uses its own namespaced set of keys in the context,
containing all of the data for that cross-cutting concern.</p>
<p>The following is an example API, and not meant as final.</p>
<p><strong><code>CreateKey(name) -&gt; key</code></strong>
To allow concerns to control access to their data, the Context API uses keys
which cannot be guessed by third parties which have not been explicitly handed
the key. It is recommended that concerns mediate data access via an API, rather
than provide direct public access to their keys.</p>
<p><strong><code>GetValue(context, key) -&gt; value</code></strong><br />
To access the local state of an concern, the Context API provides a function
which takes a context and a key as input, and returns a value.</p>
<p><strong><code>SetValue(context, key, value) -&gt; context</code></strong><br />
To record the local state of a cross-cutting concern, the Context API provides a
function which takes a context, a key, and a value as input, and returns a
new context which contains the new value. Note that the new value is not present
in the old context.</p>
<p><strong><code>RemoveValue(context, key) -&gt; context</code></strong>
RemoveValue returns a new context with the key cleared. Note that the removed
value still remains present in the old context.</p>
<h4><a class="header" href="#optional-automated-context-management" id="optional-automated-context-management">Optional: Automated Context Management</a></h4>
<p>When possible, the OpenTelemetry context should automatically be associated
with the program execution context. Note that some languages do not provide any
facility for setting and getting a current context. In these cases, the user is
responsible for managing the current context.</p>
<p><strong><code>GetCurrent() -&gt; context</code></strong><br />
To access the context associated with program execution, the Context API
provides a function which takes no arguments and returns a Context.</p>
<p><strong><code>SetCurrent(context)</code></strong><br />
To associate a context with program execution, the Context API provides a
function which takes a Context.</p>
<h3><a class="header" href="#propagation-api" id="propagation-api">Propagation API</a></h3>
<p>Cross-cutting concerns send their state to the next process via propagators:
functions which read and write context into RPC requests. Each concern creates a
set of propagators for every type of supported medium - currently only HTTP
requests.</p>
<p>The following is an example API, and not meant as final.</p>
<p><strong><code>Extract(context, []http_extractor, headers) -&gt; context</code></strong><br />
In order to continue transmitting data injected earlier in the transaction,
the Propagation API provides a function which takes a context, a set of
HTTP_Extractors, and a set of HTTP headers, and returns a new context which
includes the state sent from the prior process.</p>
<p><strong><code>Inject(context, []http_injector, headers) -&gt; headers</code></strong><br />
To send the data for all concerns to the next process in the transaction, the
Propagation API provides a function which takes a context, a set of
HTTP_Injectors, and adds the contents of the context in to HTTP headers to
include an HTTP Header representation of the context.</p>
<p><strong><code>HTTP_Extractor(context, headers) -&gt; context</code></strong><br />
Each concern must implement an HTTP_Extractor, which can locate the headers
containing the http-formatted data, and then translate the contents into an
in-memory representation, set within the returned context object.</p>
<p><strong><code>HTTP_Injector(context, headers) -&gt; headers</code></strong><br />
Each concern must implement an HTTP_Injector, which can take the in-memory
representation of its data from the given context object, and add it to an
existing set of HTTP headers.</p>
<h4><a class="header" href="#optional-global-propagators" id="optional-global-propagators">Optional: Global Propagators</a></h4>
<p>It may be convenient to create a list of propagators during program
initialization, and then access these propagators later in the program.
To facilitate this, global injectors and extractors are optionally available.
However, there is no requirement to use this feature.</p>
<p><strong><code>GetExtractors() -&gt; []http_extractor</code></strong><br />
To access the global extractor, the Propagation API provides a function which
returns an extractor.</p>
<p><strong><code>SetExtractors([]http_extractor)</code></strong><br />
To update the global extractor, the Propagation API provides a function which
takes an extractor.</p>
<p><strong><code>GetInjectors() -&gt; []http_injector</code></strong><br />
To access the global injector, the Propagation API provides a function which
returns an injector.</p>
<p><strong><code>SetInjectors([]http_injector)</code></strong><br />
To update the global injector, the Propagation API provides a function which
takes an injector.</p>
<h2><a class="header" href="#prototypes" id="prototypes">Prototypes</a></h2>
<p><strong>Erlang:</strong> <a href="https://github.com/open-telemetry/opentelemetry-erlang-api/pull/4">https://github.com/open-telemetry/opentelemetry-erlang-api/pull/4</a><br />
<strong>Go:</strong> <a href="https://github.com/open-telemetry/opentelemetry-go/pull/381">https://github.com/open-telemetry/opentelemetry-go/pull/381</a><br />
<strong>Java:</strong> <a href="https://github.com/open-telemetry/opentelemetry-java/pull/655">https://github.com/open-telemetry/opentelemetry-java/pull/655</a><br />
<strong>Python:</strong> <a href="https://github.com/open-telemetry/opentelemetry-python/pull/325">https://github.com/open-telemetry/opentelemetry-python/pull/325</a><br />
<strong>Ruby:</strong> <a href="https://github.com/open-telemetry/opentelemetry-ruby/pull/147">https://github.com/open-telemetry/opentelemetry-ruby/pull/147</a><br />
<strong>C#/.NET:</strong> <a href="https://github.com/open-telemetry/opentelemetry-dotnet/pull/399">https://github.com/open-telemetry/opentelemetry-dotnet/pull/399</a></p>
<h2><a class="header" href="#examples" id="examples">Examples</a></h2>
<p>It might be helpful to look at some examples, written in pseudocode. Note that
the pseudocode only uses simple functions and immutable values. Most mutable,
object-orient languages will use objects, such as a Span object, to encapsulate
the context object and hide it from the user in most cases.</p>
<p>Let's describe
a simple scenario, where <code>service A</code> responds to an HTTP request from a <code>client</code>
with the result of a request to <code>service B</code>.</p>
<pre><code>client -&gt; service A -&gt; service B
</code></pre>
<p>Now, let's assume the <code>client</code> in the above system is version 1.0. With version
v2.0 of the <code>client</code>, <code>service A</code> must call <code>service C</code> instead of <code>service B</code>
in order to return the correct data.</p>
<pre><code>client -&gt; service A -&gt; service C
</code></pre>
<p>In this example, we would like <code>service A</code> to decide on which backend service
to call, based on the client version. We would also like to trace the entire
system, in order to understand if requests to <code>service C</code> are slower or faster
than <code>service B</code>. What might <code>service A</code> look like?</p>
<h3><a class="header" href="#global-initialization" id="global-initialization">Global initialization</a></h3>
<p>First, during program initialization, <code>service A</code> configures correlation and tracing
propagation, and include them in the global list of injectors and extractors.
Let's assume this tracing system is configured to use B3, and has a specific
propagator for that format. Initializing the propagators might look like this:</p>
<pre><code class="language-php">func InitializeOpentelemetry() {
  // create the propagators for tracing and correlations.
  bagExtract, bagInject = Correlations::HTTPPropagator()
  traceExtract, traceInject = Tracer::B3Propagator()
  
  // add the propagators to the global list.
  Propagation::SetExtractors(bagExtract, traceExtract)
  Propagation::SetInjectors(bagInject, traceInject)
}
</code></pre>
<h3><a class="header" href="#extracting-and-injecting-from-http-headers" id="extracting-and-injecting-from-http-headers">Extracting and injecting from HTTP headers</a></h3>
<p>These propagators can then be used in the request handler for <code>service A</code>. The
tracing and correlations concerns use the context object to handle state without
breaking the encapsulation of the functions they are embedded in.</p>
<pre><code class="language-php">func ServeRequest(context, request, project) -&gt; (context) {
  // Extract the context from the HTTP headers. Because the list of
  // extractors includes a trace extractor and a correlations extractor, the
  // contents for both systems are included in the  request headers into the
  // returned context.
  extractors = Propagation::GetExtractors()
  context = Propagation::Extract(context, extractors, request.Headers)

  // Start a span, setting the parent to the span context received from
  // the client process. The new span will then be in the returned context.
  context = Tracer::StartSpan(context, [span options])
  
  // Determine the version of the client, in order to handle the data
  // migration and allow new clients access to a data source that older
  // clients are unaware of.
  version = Correlations::GetCorrelation( context, &quot;client-version&quot;)

  switch( version ){
    case &quot;v1.0&quot;:
      data, context = FetchDataFromServiceB(context)
    case &quot;v2.0&quot;:
      data, context = FetchDataFromServiceC(context)
  }

  context = request.Response(context, data)

  // End the current span
  Tracer::EndSpan(context)

  return context
}

func FetchDataFromServiceB(context) -&gt; (context, data) {
  request = NewRequest([request options])
  
  // Inject the contexts to be propagated. Note that there is no direct
  // reference to tracing or correlations.
  injectors = Propagation::GetInjectors()
  request.Headers = Propagation::Inject(context, injectors, request.Headers)

  // make an http request
  data = request.Do()

  return data
}
</code></pre>
<h3><a class="header" href="#simplify-the-api-with-automated-context-propagation" id="simplify-the-api-with-automated-context-propagation">Simplify the API with automated context propagation</a></h3>
<p>In this version of pseudocode above, we assume that the context object is
explicit, and is pass and returned from every function as an ordinary parameter.
This is cumbersome, and in many languages, a mechanism exists which allows
context to be propagated automatically.</p>
<p>In this version of pseudocode, assume that the current context can be stored as
a thread local, and is implicitly passed to and returned from every function.</p>
<pre><code class="language-php">func ServeRequest(request, project) {
  extractors = Propagation::GetExtractors()
  Propagation::Extract(extractors, request.Headers)
  
  Tracer::StartSpan([span options])
  
  version = Correlations::GetCorrelation(&quot;client-version&quot;)
  
  switch( version ){
    case &quot;v1.0&quot;:
      data = FetchDataFromServiceB()
    case &quot;v2.0&quot;:
      data = FetchDataFromServiceC()
  }

  request.Response(data)
  
  Tracer::EndSpan()
}

func FetchDataFromServiceB() -&gt; (data) {
  request = newRequest([request options])
  
  injectors = Propagation::GetInjectors()
  Propagation::Inject(request.Headers)
  
  data = request.Do()

  return data
}
</code></pre>
<h3><a class="header" href="#implementing-a-propagator" id="implementing-a-propagator">Implementing a propagator</a></h3>
<p>Digging into the details of the tracing system, what might the internals of a
span context propagator look like? Here is a crude example of extracting and
injecting B3 headers, using an explicit context.</p>
<pre><code class="language-php">  func B3Extractor(context, headers) -&gt; (context) {
    context = Context::SetValue( context,
                                 &quot;trace.parentTraceID&quot;,
                                 headers[&quot;X-B3-TraceId&quot;])
    context = Context::SetValue( context,
                                &quot;trace.parentSpanID&quot;,
                                 headers[&quot;X-B3-SpanId&quot;])
    return context
  }

  func B3Injector(context, headers) -&gt; (headers) {
    headers[&quot;X-B3-TraceId&quot;] = Context::GetValue( context, &quot;trace.parentTraceID&quot;)
    headers[&quot;X-B3-SpanId&quot;] = Context::GetValue( context, &quot;trace.parentSpanID&quot;)

    return headers
  }
</code></pre>
<h3><a class="header" href="#implementing-a-concern" id="implementing-a-concern">Implementing a concern</a></h3>
<p>Now, have a look at a crude example of how StartSpan might make use of the
context. Note that this code must know the internal details about the context
keys in which the propagators above store their data. For this pseudocode, let's
assume again that the context is passed implicitly in a thread local.</p>
<pre><code class="language-php">  func StartSpan(options) {
    spanData = newSpanData()

    spanData.parentTraceID = Context::GetValue( &quot;trace.parentTraceID&quot;)
    spanData.parentSpanID = Context::GetValue( &quot;trace.parentSpanID&quot;)

    spanData.traceID = newTraceID()
    spanData.spanID = newSpanID()

    Context::SetValue( &quot;trace.parentTraceID&quot;, spanData.traceID)
    Context::SetValue( &quot;trace.parentSpanID&quot;, spanData.spanID)

    // store the spanData object as well, for in-process propagation. Note that
    // this key will not be propagated, it is for local use only.
    Context::SetValue( &quot;trace.currentSpanData&quot;, spanData)

    return
  }
</code></pre>
<h3><a class="header" href="#the-scope-of-current-context" id="the-scope-of-current-context">The scope of current context</a></h3>
<p>Let's look at a couple other scenarios related to automatic context propagation.</p>
<p>When are the values in the current context available? Scope management may be
different in each language, but as long as the scope does not change (by
switching threads, for example) the current context follows the execution of
the program. This includes after a function returns. Note that the context
objects themselves are immutable, so explicit handles to prior contexts will not
be updated when the current context is changed.</p>
<pre><code class="language-php">func Request() {
  emptyContext = Context::GetCurrent()
  
  Context::SetValue( &quot;say-something&quot;, &quot;foo&quot;)
  secondContext = Context::GetCurrent()
  
  print(Context::GetValue(&quot;say-something&quot;)) // prints &quot;foo&quot;
  
  DoWork()
  
  thirdContext = Context::GetCurrent()
  
  print(Context::GetValue(&quot;say-something&quot;)) // prints &quot;bar&quot;

  print( emptyContext.GetValue(&quot;say-something&quot;) )  // prints &quot;&quot;
  print( secondContext.GetValue(&quot;say-something&quot;) ) // prints &quot;foo&quot;
  print( thirdContext.GetValue(&quot;say-something&quot;) )  // prints &quot;bar&quot;
}

func DoWork(){
  Context::SetValue( &quot;say-something&quot;, &quot;bar&quot;)
}
</code></pre>
<h3><a class="header" href="#referencing-multiple-contexts" id="referencing-multiple-contexts">Referencing multiple contexts</a></h3>
<p>If context propagation is automatic, does the user ever need to reference a
context object directly? Sometimes. Even when automated context propagation is
an available option, there is no restriction which says that concerns must only
ever access the current context.</p>
<p>For example, if a concern wanted to merge the data between two contexts, at
least one of them will not be the current context.</p>
<pre><code class="language-php">mergedContext = MergeCorrelations( Context::GetCurrent(), otherContext)
Context::SetCurrent(mergedContext)
</code></pre>
<h3><a class="header" href="#falling-back-to-explicit-contexts" id="falling-back-to-explicit-contexts">Falling back to explicit contexts</a></h3>
<p>Sometimes, suppling an additional version of a function which uses explicit
contexts is necessary, in order to handle edge cases. For example, in some cases
an extracted context is not intended to be set as current context. An
alternate extract method can be added to the API to handle this.</p>
<pre><code class="language-php">// Most of the time, the extract function operates on the current context.
Extract(headers)

// When a context needs to be extracted without changing the current
// context, fall back to the explicit API.
otherContext = ExtractWithContext(Context::GetCurrent(), headers)
</code></pre>
<h2><a class="header" href="#internal-details-9" id="internal-details-9">Internal details</a></h2>
<p><img src="img/0066_context_propagation_details.png" alt="drawing" /></p>
<h3><a class="header" href="#example-package-layout" id="example-package-layout">Example Package Layout</a></h3>
<pre><code>  Context
    ContextAPI
  Observability
    Correlations
      CorrelationAPI
      HttpInjector
      HttpExtractor
    Metrics
      MetricAPI
    Trace
      TracerAPI
      HttpInjector
      HttpExtractor
  Propagation
    Registry
    HttpInjectorInterface
    HttpExtractorInterface
</code></pre>
<h3><a class="header" href="#edge-cases" id="edge-cases">Edge Cases</a></h3>
<p>There are some complications that can arise when managing a span context extracted off the wire and in-process spans for tracer operations that operate on an implicit parent. In order to ensure that a context key references an object of the expected type and that the proper implicit parent is used, the following conventions have been established:</p>
<h3><a class="header" href="#extract" id="extract">Extract</a></h3>
<p>When extracting a remote context, the extracted span context MUST be stored separately from the current span.</p>
<h3><a class="header" href="#default-span-parentage" id="default-span-parentage">Default Span Parentage</a></h3>
<p>When a new span is created from a context, a default parent for the span can be assigned. The order is of assignment is as follows:</p>
<ul>
<li>The current span.</li>
<li>The extracted span.</li>
<li>The root span.</li>
</ul>
<h3><a class="header" href="#inject" id="inject">Inject</a></h3>
<p>When injecting a span to send over the wire, the default order is of
assignment is as follows:</p>
<ul>
<li>The current span.</li>
<li>The extracted span.</li>
</ul>
<h3><a class="header" href="#default-http-headers" id="default-http-headers">Default HTTP headers</a></h3>
<p>OpenTelemetry currently uses two standard header formats for context propagation.
Their properties and requirements are integrated into the OpenTelemetry APIs.</p>
<p><strong>Span Context -</strong> The OpenTelemetry Span API is modeled on the <code>traceparent</code>
and <code>tracestate</code> headers defined in the <a href="https://www.w3.org/TR/trace-context/">W3C Trace Context specification</a>.</p>
<p><strong>Correlation Context -</strong> The OpenTelemetry Correlations API is modeled on the
<code>Correlation-Context</code> headers defined in the <a href="https://w3c.github.io/correlation-context/">W3C Correlation Context specification</a>.</p>
<h3><a class="header" href="#context-management-and-in-process-propagation" id="context-management-and-in-process-propagation">Context management and in-process propagation</a></h3>
<p>In order for Context to function, it must always remain bound to the execution
of code it represents. By default, this means that the programmer must pass a
Context down the call stack as a function parameter. However, many languages
provide automated context management facilities, such as thread locals.
OpenTelemetry should leverage these facilities when available, in order to
provide automatic context management.</p>
<h3><a class="header" href="#pre-existing-context-implementations" id="pre-existing-context-implementations">Pre-existing context implementations</a></h3>
<p>In some languages, a single, widely used context implementation exists. In other
languages, there many be too many implementations, or none at all. For example,
Go has a the <code>context.Context</code> object, and widespread conventions for how to
pass it down the call stack. Java has MDC, along with several other context
implementations, but none are so widely used that their presence can be
guaranteed or assumed.</p>
<p>In the cases where an extremely clear, pre-existing option is not available,
OpenTelemetry should provide its own context implementation.</p>
<h2><a class="header" href="#faq" id="faq">FAQ</a></h2>
<h3><a class="header" href="#what-about-complex-propagation-behavior" id="what-about-complex-propagation-behavior">What about complex propagation behavior</a></h3>
<p>Some OpenTelemetry proposals have called for more complex propagation behavior.
For example, falling back to extracting B3 headers if W3C Trace-Context headers
are not found. &quot;Fallback propagators&quot; and other complex behavior can be modeled as
implementation details behind the Propagator interface. Therefore, the
propagation system itself does not need to provide an mechanism for chaining
together propagators or other additional facilities.</p>
<h2><a class="header" href="#prior-art-and-alternatives-10" id="prior-art-and-alternatives-10">Prior art and alternatives</a></h2>
<p>Prior art:</p>
<ul>
<li>OpenTelemetry distributed context</li>
<li>OpenCensus propagators</li>
<li>OpenTracing spans</li>
<li>gRPC context</li>
</ul>
<h2><a class="header" href="#risks" id="risks">Risks</a></h2>
<p>The Correlations API is related to the <a href="https://w3c.github.io/correlation-context/">W3C Correlation-Context</a>
specification. Work on this specification has begun, but is not complete. While
unlikely, it is possible that this W3C specification could diverge from the
design or guarantees needed by the Correlations API.</p>
<h2><a class="header" href="#future-possibilities-2" id="future-possibilities-2">Future possibilities</a></h2>
<p>Cleanly splitting OpenTelemetry into Aspects and Context Propagation layer may
allow us to move the Context Propagation layer into its own, stand-alone
project. This may facilitate adoption, by allowing us to share Context
Propagation with gRPC and other projects.</p>
<h1><a class="header" href="#rename-metric-instrument-handles-to-bound-instruments" id="rename-metric-instrument-handles-to-bound-instruments">Rename metric instrument Handles to &quot;Bound Instruments&quot;</a></h1>
<p>The OpenTelemetry metrics API specification refers to a concept known
as <a href="0009-metric-handles.html">&quot;metric handles&quot;</a>, which is a metric
instrument bound to a <code>LabelSet</code>.  This OTEP proposes to change that
term to &quot;bound instruments&quot; to avoid the more-generic term &quot;handle&quot;.</p>
<p>The corresponding method to create a bound instrument will be renamed
&quot;Bind&quot; as opposed to &quot;GetHandle&quot;.</p>
<h2><a class="header" href="#motivation-15" id="motivation-15">Motivation</a></h2>
<p>The term &quot;Handle&quot; is widely seen as too general for its purpose in the
metrics API.  Rather than re-use a widely-used noun for this concept,
we instead will re-use the metric &quot;instrument&quot; noun and apply an
adjective, &quot;bound&quot; to convey that it has been bound to a <code>LabelSet</code>.</p>
<h2><a class="header" href="#explanation-11" id="explanation-11">Explanation</a></h2>
<p>&quot;Handle&quot; has been confusing from the start. However it was preceded by
other potentially confusing terms (e.g., &quot;TimeSeries&quot;, &quot;Entry&quot;).  The
term &quot;Bound instrument&quot; was initially suggested
<a href="https://github.com/open-telemetry/opentelemetry-specification/pull/299#discussion_r334211154">here</a>
and widely accepted.</p>
<h2><a class="header" href="#internal-details-10" id="internal-details-10">Internal details</a></h2>
<p>This is a simple renaming.  All uses of &quot;handle&quot; will be replaced by
&quot;bound instrument&quot; in the specification.  All uses of the <code>GetHandle</code>
method become <code>Bind</code>.</p>
<p>Note that the phrase &quot;bound instrument&quot; may not appear directly in the
user-facing API, nor is it required to, whereas the method <code>GetHandle</code>
is a specified method on metric instruments.</p>
<p>The newly-named <code>Bind()</code> method returns a bound instrument type.  The
name of the returned type may simply take the name of its instrument
with the prefix <code>Bound</code>.  For example, an <code>Int64Counter</code> instrument's
<code>Bind()</code> method should return a <code>BoundInt64Counter</code> type.</p>
<p>As usual, the spelling and capitalization of these names are just
recommendations, individual language committees should select names
that are well suited to their language and existing API style.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-8" id="trade-offs-and-mitigations-8">Trade-offs and mitigations</a></h2>
<p>This is widely seen as an improvement, based on informal discussions.</p>
<h2><a class="header" href="#prior-art-and-alternatives-11" id="prior-art-and-alternatives-11">Prior art and alternatives</a></h2>
<p>The OpenCensus libraries named this concept &quot;Entries&quot;, with a
<code>GetEntry</code> method, as they are entries some kind of map.</p>
<p>The earliest appearance in OpenTelemetry renamed these &quot;TimeSeries&quot;,
hoping to improve matters, but &quot;TimeSeries&quot; more commonly refers to
the output the bound instruments, after aggregation.  &quot;Handle&quot; was
decided upon in an August 2019 working group on metrics.</p>
<p>The Prometheus library refers to unbound instruments as &quot;Vectors&quot; and
supports a variety of &quot;With&quot; methods to bind labels with the vector to
yield a bound instrument.</p>
<h1><a class="header" href="#metric-observer-specification-refinement" id="metric-observer-specification-refinement">Metric observer specification (refinement)</a></h1>
<p>The metric observer gauge was described in <a href="0008-metric-observer.html">OTEP
0008</a> but left out of the current metrics
specification because the prior OTEP did not clarify the valid calling
conventions for observer gauge metric instruments.  This proposal
completely replaces OTEP 0008.</p>
<h2><a class="header" href="#motivation-16" id="motivation-16">Motivation</a></h2>
<p>An <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/597718b3fcfaf10bcf45d93f99b66f94a28048cb/specification/api-metrics.md">earlier version of the metrics specification</a>
described metric callbacks as an alternate means of generating metric
events, allowing the application to generate metric events only as
often as desired by the collection interval.  It specified this
support for all instrument kinds.</p>
<p>This proposal restores the ability to use callbacks only with a
dedicated <code>Observer</code> kind of instrument with the same semantics as
Gauge instruments.  Like a Gauge instrument, Observer instruments are
used to report the current value of a variable.</p>
<p>We may ask, why should Observer instruments be a first-class part of
the API, as opposed to simply registering non-instrument-specific
callbacks to call user-level code on the metrics collection interval?
That would permit the use of ordinary Gauge instruments as a stand-in
for the Observer instrument proposed here.  The approach proposed here
is more flexible because it permits the Meter implementation to
control the collection interval on a per-instrument basis as well as
to disable instruments.</p>
<h2><a class="header" href="#explanation-12" id="explanation-12">Explanation</a></h2>
<p>Gauge metric instruments are typically used to reflect properties that
are pre-computed or instantaneously read by a system, where the
measurement interval is arbitrary.  When selecting a Gauge, as opposed
to the Counter or measure kind of metric instrument, there could be
significant computational cost in computing or reading the current
value.  When this is the case, it is understandable that we are
interested in providing values on demand, as an optimization.</p>
<p>The optimization aspect of Observer instruments is critical to their
purpose.  If the simpler alternative suggested above--registering
non-instrument-specific callbacks--were implemented instead, callers
would demand a way to ask whether an instrument was &quot;recording&quot; or
not, similar to the <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/trace/api.md#isrecording"><code>Span.IsRecording</code>
API</a>.</p>
<p>Observer instruments are semantically equivalent to gauge instruments,
except they support callbacks instead of a <code>Set()</code> operation.
Observer callbacks support <code>Observe()</code> instead.  Why support callbacks
with Gauge semantics but not do the same for Counter and Measure
semantics?</p>
<h3><a class="header" href="#why-not-measure-callbacks" id="why-not-measure-callbacks">Why not Measure callbacks?</a></h3>
<p>Measure instruments, by definition, carry information about the
individual measurements, so there is no benefit to be had in deferring
evaluation to a callback.  Observer callbacks are designed to reduce
the number of measurements, which is incompatible with the semantics
of Measure instruments.</p>
<h3><a class="header" href="#why-not-counter-callbacks" id="why-not-counter-callbacks">Why not Counter callbacks?</a></h3>
<p>Counter instruments can be expressed as Observer instruments when they
are expensive to pre-compute or will be instantaneously read.  There
are two ways these can be treated using Observer instrument semantics.</p>
<p>Observer instruments, like Gauge instruments, use a &quot;last value&quot;
aggregation by default.  With this default interpretation in mind, a
monotonic Counter can be expressed as a monotonic Observer instrument
simply by reporting the current sum from <code>Observe()</code>, in which case
the &quot;last value&quot; may be interpreted directly as a sum.  Systems with
support for rate calculations over current sums (e.g., Prometheus)
will be able to use these metrics directly.  Non-monotonic Counters
may be expressed as their current value, but they cannot meaningfully
be aggregated in this way.</p>
<p>The preferred way to <code>Observe()</code> Counter-like data from an Observer
instrument callback is to report deltas in the callback and configure
a Sum aggregation in the exporter.  Data reported in this way will
support rate calculations just as if they were true Counters.</p>
<h3><a class="header" href="#differences-between-gauge-and-observer" id="differences-between-gauge-and-observer">Differences between Gauge and Observer</a></h3>
<p>One significant difference between gauges that are explicitly <code>Set()</code>,
as compared with observer callbacks, is that <code>Set()</code> happens inside a
context (i.e., its distributed context), whereas the observer callback
does not execute with any distributed context.</p>
<p>Whereas Gauge values do have context at the moment <code>Set()</code> is called,
Observer callbacks do not.  Observer instruments are appropriate for
reporting values that are not request specific.</p>
<h2><a class="header" href="#details-1" id="details-1">Details</a></h2>
<p>Observer instruments are semantically equivalent to Gauge instruments
but use different calling conventions.  Use the language-specific
constructor for an Observer instrument (e.g.,
<code>metric.NewFloat64Observer()</code>).  Observer instruments support the
<code>Monotonic</code> and <code>NonMonotonic</code> options, the same as Gauge instruments.</p>
<p>Callbacks should avoid blocking.  The implementation may be required
to cancel computation if the callback blocks for too long.</p>
<p>Callbacks must not be called synchronously with application code via
any OpenTelemetry API.  This prevents the application from potentially
deadlocking itself by being called synchronously from its own thread.
Implementations that cannot provide this guarantee should prefer not
to implement Observer instrsuments.</p>
<p>Callbacks may be called synchronously in the SDK on behalf of an
exporter, provided it does not contradict the requirement above.</p>
<p>Callbacks should avoid calling OpenTelemetry APIs other than the
interface provided to <code>Observe()</code>.  This prevents the SDK from
potentially deadlocking itself by being called synchronously from its
own thread.  We recognize this may be impossible or expensive to
enforce.  SDKs should document how they respond to such attempts at
re-entry.</p>
<h3><a class="header" href="#observer-calling-conventions" id="observer-calling-conventions">Observer calling conventions</a></h3>
<p>Observer callbacks are called with an <code>ObserverResult</code>, an interface
that supports capturing events directly in the callback, as follows.</p>
<p>To capture an observation with a specific <code>LabelSet</code>, call the
<code>ObserverResult</code> directly using <code>ObserverResult.Observe(value, LabelSet)</code>.</p>
<p>There is no equivalent of a &quot;bound&quot; observer instrument as there is
with Counter, Gauge, and Measure instruments.  A bound calling
convention is not needed for Observer instruments because there is
little if any performance benefit in doing so (as Observer instruments
are called during collection, there is no need to maintain &quot;active&quot;
records concurrent with collection).</p>
<p>Multiple observations are permitted in a single callback invocation.</p>
<p>The <code>ObserverResult</code> passed to a callback should not be used outside the
invocation to which it is passed.</p>
<h4><a class="header" href="#one-callback-per-instrument" id="one-callback-per-instrument">One callback per instrument</a></h4>
<p>The API <em>could</em> support registering independent callbacks tied to
registered (&quot;bound&quot;) label sets, instead it takes the approach of
supporting one callback per instrument.  There are two cases to
consider: (a) where the source of an instrument's values provides one
value at a time, (b) where the source of an instrument's values
provides several values at once.</p>
<p>The decision to support one callback per instrument is justified
because it is relatively easy in case (a) above to call the source
multiple times for multiple values, while it is relatively difficult
in case (b) above to call the source once and report values from
multiple callbacks.</p>
<h3><a class="header" href="#pseudocode" id="pseudocode">Pseudocode</a></h3>
<p>An example:</p>
<pre><code>class YourClass {
  private static final Meter meter = ...;
  private static final ObserverDouble cpuLoad = ...;

  void init() {
    LabelSet labelSet = meter.createLabelSet(&quot;low_power&quot;, isLowPowerMode());
    cpuLoad.setCallback(
        new ObserverDouble.Callback&lt;ObserverDouble.Result&gt;() {
          @Override
          public void update(Result result) {
              result.Observe(getCPULoad(), labelSet);
        });
  }
}
</code></pre>
<h2><a class="header" href="#trade-offs-and-mitigations-9" id="trade-offs-and-mitigations-9">Trade-offs and mitigations</a></h2>
<p>Callbacks are a relatively dangerous programming pattern, which may
require care to avoid deadlocks between the application and the API or
the SDK.  Implementations SHOULD consider preventing deadlocks through
any means that are safe and economical.</p>
<h1><a class="header" href="#remove-the-metric-api-gauge-instrument" id="remove-the-metric-api-gauge-instrument">Remove the Metric API Gauge instrument</a></h1>
<p>The <a href="./0072-metric-observer.html">Observer instrument</a> is semantically
identical to the metric Gauge instrument, only it is reported via a
callback instead of synchronous API calls.  Implementation has shown
that Gauge instruments are difficult to reason about because the
semantics of a &quot;last value&quot; Aggregator have to address questions about
statefulness--the SDK's ability to recall old values.  Observer
instruments avoid some of these concerns because they are reported
once per collection period, making it easier to reason about &quot;all
values&quot; in an aggregator.</p>
<h2><a class="header" href="#motivation-17" id="motivation-17">Motivation</a></h2>
<p>Observer instruments improve on our ability to compute well-defined
sum and average-value aggregations over a set of last-value aggregated
data, compared with the existing Gauge instrument.  Using data from an
Observer instrument, we are easily able to pose queries about the
current sum of all current values as well as the number of distinct
values, which together define the average value.</p>
<p>To do the same with synchronous Gauge instruments, the SDK would
potentially be required to maintain state outside a single collection
window, which complicates memory management.  The SDK is required to
maintain state about all distinct label sets over the query evaluation
interval.</p>
<p>The question is: how long should the SDK remember a gauge value?
Observer instruments do not pose this complication, because
observations are synchronized with collection instead of with the
application.</p>
<p>Unlike with Gauge instruments, Observer instruments naturally define
the current set of all values for a single collection period, making
sum and average-value aggregations possible without mention of the
query evaluation interval, and without the implied additional state
management.</p>
<h2><a class="header" href="#explanation-13" id="explanation-13">Explanation</a></h2>
<p>The Gauge instrument's most significant feature is that its
measurement interval is arbitrary -- controlled by the application
through explicit, synchronous calls to <code>Set()</code>.  It is used to report
a current value in a synchronous context, meaning the metric event is
associated with a label set determined by some &quot;request&quot;.</p>
<p>This proposal recommends that synchronously reporting Gauge values can
always be accomplished using one of the three other kinds of
instrument.</p>
<p>It was <em>already</em> recommended in the specification that if the
instrument reports values you would naturally sum, you should have
used a Counter in the first place.  These are not really &quot;current&quot;
values when reported, they are current contributions to the sum.  We
still recommend Counters in this case.</p>
<p>If the gauge reports values, where you would naturally average the
last value across distinct label sets, use a Measure instrument.
Configure the instrument for last-value aggregation.  Since last-value
aggregation is not the default for Measure instruments, this will be
non-standard and require extra configuration.</p>
<p>If the gauge reports values, where you would naturally sum the last
value across distinct label sets, use an Observer instrument.  The
current set of entities (e.g., shards, active users, etc) constributes
a last value that should be summed.  These are different from Counter
instruments because we are not interested in a sum across time, we are
interested in a sum across distinct instances.</p>
<h3><a class="header" href="#example-reporting-per-request-cpu-usage" id="example-reporting-per-request-cpu-usage">Example: Reporting per-request CPU usage</a></h3>
<p>Use a counter to report a quantity that is naturally summed over time,
such as CPU usage.</p>
<h3><a class="header" href="#example-reporting-per-shard-memory-holdings" id="example-reporting-per-shard-memory-holdings">Example: Reporting per-shard memory holdings</a></h3>
<p>There are a number of current shards holding variable amounts of
memory by a widely-used library.  Observe the current allocation per
shard using an Observer instrument.  These can be aggregated across
hosts to compute cluster-wide memory holdings by shard, for example.</p>
<p>It does not make sense to compute a sum of memory holdings over
multiple periods, as these are not additive quantities.  It does makes
sense to sum the last value across hosts.</p>
<h3><a class="header" href="#example-reporting-a-per-request-finishing-account-balance" id="example-reporting-a-per-request-finishing-account-balance">Example: Reporting a per-request finishing account balance</a></h3>
<p>There's a number that rises and falls such as a bank account balance.
This was being <code>Set()</code> at the finish of all transactions.  Replace it
with a Measure instrument and <code>Record()</code> the last value.</p>
<p>Similar cases: report a cpu load, specific temperature, fan speed, or
altitude measurement associated with a request.</p>
<h2><a class="header" href="#internal-details-11" id="internal-details-11">Internal details</a></h2>
<p>The Gauge instrument will be removed from the specification at the
same time the Observer instrument is added.  This will make the
transition easier because in many cases, Observer instruments simply
replace Gauge instruments in the text.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-10" id="trade-offs-and-mitigations-10">Trade-offs and mitigations</a></h2>
<p>Not much is lost to the user from removing Gauge instruments.</p>
<p>There may be situations where an Observer instrument is the natural
choice but it is undesirable to be interrupted by the Metric SDK in
order to execute an Observer callback.  Situations where Observer
semantics are correct (not Counter, not Measure) but a synchronous API
is more acceptable are expected to be very rare.</p>
<p>To address such rare cases, here are two possibilities:</p>
<ol>
<li>Implement a Gauge Set instrument backed by an Observer instrument.
The Gauge Set's job is to maintain the current set of label sets
(e.g., explicitly managed or by time-limit) and their last value, to
be reported by the Observer at each collection interval.</li>
<li>Implement an application-specific metric collection API that would
allow the application to synchronize with the SDK on collection
intervals.  For example, a transactional API allowing the application
to BEGIN and END synchronously reporting Observer instrument
observations.</li>
</ol>
<h2><a class="header" href="#prior-art-and-alternatives-12" id="prior-art-and-alternatives-12">Prior art and alternatives</a></h2>
<p>Many existing Metric libraries support both synchronous and
asynchronous Gauge-like instruments.</p>
<p>See the initial discussion in <a href="https://github.com/open-telemetry/opentelemetry-specification/issues/412">Spec issue
412</a>.</p>
<h1><a class="header" href="#instrumentationlibrary" id="instrumentationlibrary">InstrumentationLibrary</a></h1>
<p>Introducing the notion of <code>InstrumentationLibrary</code> as a first class concept in
OpenTelemetry which describes the named <code>Tracer|Meter</code> in the data model.</p>
<h2><a class="header" href="#motivation-18" id="motivation-18">Motivation</a></h2>
<p>The main motivation for this is to better model telemetry coming from different
instrumentation libraries inside an Application (<code>Resource</code>). This change
affects only the OpenTelemetry protocol and exporters, and it does not require
any API changes.</p>
<p>The proposal is to make <code>InstrumentationLibrary</code> a first class concept in
OpenTelemetry, with a scope defined by the <code>named Tracer|Meter</code>. It describes
all telemetry generated by one <code>named Tracer|Meter</code>.</p>
<h2><a class="header" href="#explanation-14" id="explanation-14">Explanation</a></h2>
<pre><code>                                Application
+--------------------------------------------------------------------------+
|                         TracerProvider(Resource)                         |
|                         MeterProvider(Resource)                          |
|                                                                          |
|      Instrumentation Library 1           Instrumentation Library 2       |
|  +--------------------------------+  +--------------------------------+  |
|  | Tracer(InstrumentationLibrary) |  | Tracer(InstrumentationLibrary) |  |
|  | Meter(InstrumentationLibrary)  |  | Meter(InstrumentationLibrary)  |  |
|  +--------------------------------+  +--------------------------------+  |
|                                                                          |
|      Instrumentation Library 3           Instrumentation Library 4       |
|  +--------------------------------+  +--------------------------------+  |
|  | Tracer(InstrumentationLibrary) |  | Tracer(InstrumentationLibrary) |  |
|  | Meter(InstrumentationLibrary)  |  | Meter(InstrumentationLibrary)  |  |
|  +--------------------------------+  +--------------------------------+  |
|                                                                          |
+--------------------------------------------------------------------------+
</code></pre>
<p>Every application has one <code>TracerProvider</code> and one <code>MeterProvider</code> which have a
<code>Resource</code> associated with them that describes the Application.</p>
<p>Every instrumentation library has one <code>Tracer</code> and one <code>Meter</code> which have a
<code>InstrumentationLibrary</code> associated with them that describes the instrumentation
library.</p>
<p>In case of multi-application deployments like Spring boot (or old Java
Application servers) every Application will have it's own <code>TracerProvider</code> and
<code>MeterProvider</code> instances.</p>
<h2><a class="header" href="#internal-details-12" id="internal-details-12">Internal details</a></h2>
<p>This proposal affects only the OpenTelemtry protocol, and proposes a way to
represent the telemetry data in a structured way.
For example, here is the protobuf definition for metrics:
metrics:</p>
<pre><code class="language-proto">// Resource information.
message Resource {
  // Set of labels that describe the resource.
  repeated opentelemetry.proto.common.v1.AttributeKeyValue attributes = 1;
}

// InstrumentationLibrary is a message representing the instrumentation library
// informations such as the fully qualified name and version.
message InstrumentationLibrary {
  string name = 1;
  string version = 2;
  // Can be extended with attributes.
}

// A collection of InstrumentationLibraryMetrics from a Resource.
message ResourceMetrics {
  // The Resource for the metrics in this message.
  // If this field is not set then no Resource is known.
  Resource resource = 1;

  // A list of metrics that originate from a Resource.
  repeated InstrumentationLibraryMetrics instrumentation_library_metrics = 2;
}

// A collection of Metrics from a InstrumentationLibrary.
message InstrumentationLibraryMetrics {
  // The Instrumentation library information for the metrics in this message.
  // If this field is not set then no InstrumentationLibrary is known.
  InstrumentationLibrary instrumentation_library = 1;

  // A list of metrics that originate from a Instrumentation library.
  repeated Metric metrics = 2;
}
</code></pre>
<h2><a class="header" href="#trade-offs-and-mitigations-11" id="trade-offs-and-mitigations-11">Trade-offs and mitigations</a></h2>
<p>Adding a new concept into the OpenTelemetry protocol and the exporter framework
may be overkill, but this concept maps easily to an already existing concept
in the API, and it is easy to explain to users.</p>
<h2><a class="header" href="#prior-art-and-alternatives-13" id="prior-art-and-alternatives-13">Prior art and alternatives</a></h2>
<p>An alternative approach was proposed in the proto <a href="https://github.com/open-telemetry/opentelemetry-proto/pull/94#discussion_r369952371">PR comment</a>
which suggested to enclose <code>Resource</code> at multiple levels including the
named <code>Tracer|Meter</code>.</p>
<h2><a class="header" href="#open-questions-6" id="open-questions-6">Open questions</a></h2>
<ol>
<li>Should we support more than name and version for the InstrumentationLibrary
now?</li>
</ol>
<h2><a class="header" href="#future-possibilities-3" id="future-possibilities-3">Future possibilities</a></h2>
<p>In the future the <code>InstrumentationLibrary</code> can be extended to support multiple
properties (attributes) that apply to the specific instance of the
instrumenting library.
Also, information about the instrumented library could be added, but that will require additional consideration about grouping, like grouping by the pair (instrumenting lib, instrumenting lib) instead of just by instrumenting lib.</p>
<h1><a class="header" href="#metric-instruments" id="metric-instruments">Metric Instruments</a></h1>
<p>Removes the optional semantic declarations <code>Monotonic</code> and <code>Absolute</code>
for metric instruments, declares the Measure and Observer instruments
as <em>foundational</em>, and introduces a process for standardizing new
instrument <em>refinements</em>.</p>
<p>Note that <a href="https://github.com/open-telemetry/oteps/pull/93">OTEP 93</a>
contains a final proposal for the set of instruments, of which there
are seven.  Note that <a href="https://github.com/open-telemetry/oteps/pull/96">OTEP
96</a> contains a final
proposal for the names of the seven standard instruments.  These three
OTEPs will be applied as a group to the specification, using the names
finalized in OTEP 96.</p>
<h2><a class="header" href="#motivation-19" id="motivation-19">Motivation</a></h2>
<p>With the removal of Gauge instruments and the addition of Observer
instruments in the specification, the existing <code>Monotonic</code> and
<code>Absolute</code> options began to create confusion.  For example, a Counter
instrument is used for capturing changes in a Sum, and we could say
that non-negative-valued metric events define a monotonic Counter, in
the sense that its Sum is monotonic.  The confusion arises, in this
case, because <code>Absolute</code> refers to the captured values, whereas
<code>Monotonic</code> refers to the semantic output.</p>
<p>From a different perspective, Counter instruments might be treated as
refinements of the Measure instrument.  Whereas the Measure instrument
is used for capturing all-purpose synchronous measurements, the
Counter instrument is used specifically for synchronously capturing
measurements of changes in a sum, therefore it uses <code>Add()</code> instead of
<code>Record()</code>, and it specifies <code>Sum</code> as the standard aggregation.</p>
<p>What this illustrates is that we have modeled this space poorly.  This
proposal does not propose to change any existing metrics APIs, only
our understanding of the three instruments currently included in the
specification: Measure, Observer, and Counter.</p>
<h2><a class="header" href="#explanation-15" id="explanation-15">Explanation</a></h2>
<p>The Measure and Observer instrument are defined as <em>foundational</em>
here, in the sense that any kind of metric instrument must reduce to
one of these.  The foundational instruments are unrestricted, in the
sense that metric events support any numerical value, positive or
negative, zero or infinity.</p>
<p>The distinction between the two foundational instruments is whether
they are synchronous.  Measure instruments are called synchronously by
the user, while Observer instruments are called asynchronously by the
implementation.  Synchronous instruments (Measure and its refinements)
have three calling patterns (<em>Bound</em>, <em>Unbound</em>, and <em>Batch</em>) to
capture measurements.  Asynchronous instruments (Observer and its
refinements) use callbacks to capture measurements.</p>
<p>All measurement APIs produce metric events consisting of <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/metrics/api.md#metric-event-format">timestamp,
instrument descriptor, label set, and numerical
value</a>.  Synchronous instrument
events additionally have <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/context/context.md">Context</a>, describing
properties of the associated trace and distributed correlation values.</p>
<h3><a class="header" href="#terminology-kinds-of-aggregation" id="terminology-kinds-of-aggregation">Terminology: Kinds of Aggregation</a></h3>
<p><em>Aggregation</em> refers to the technique used to summarize many
measurements and/or observations into <em>some</em> kind of summary of the
data.  As detailed in the <a href="https://github.com/open-telemetry/opentelemetry-specification/pull/347/files?short_path=5b01bbf#diff-5b01bbf3430dde7fc5789b5919d03001">metric SDK specification (TODO:
WIP)</a>,
there are generally two relevant modes of aggregation:</p>
<ol>
<li>Within one collection interval, for one label set, the SDK's
<code>Aggregator.Add()</code> interface method incorporates one new measurement
value into the current aggregation value.  This happens at run time,
therefore is referred to as <em>temporal aggregation</em>.  This mode applies
only to Measure instruments.</li>
<li>Within one collection interval, when combining label sets, the
SDK's <code>Aggregator.Merge()</code> interface method incorporates two
aggregation values into one aggregation value.  This is referred to as
<em>spatial aggregation</em>.  This mode applies to both Measure and Observer
instruments.</li>
</ol>
<p>As discussed below, we are especially interested in aggregating rate
information, which sometimes requires that temporal and spatial
aggregation be treated differently.</p>
<h3><a class="header" href="#last-value-relationship" id="last-value-relationship">Last-value relationship</a></h3>
<p>Observer instruments have a well-defined <em>Last Value</em> measured by the
instrument, that can be useful in defining aggregations.  The Last
Value of an Observer instrument is the value that was captured during
the last-completed collection interval, and it is a useful
relationship because it is defined without relation to collection
interval timing.  The Last Value of an Observer is determined by the
single most-recently completed collection interval--it is not
necessary to consider prior collection intervals.  The Last Value of
an Observer is undefined when it is not observed during a collection
interval.</p>
<p>To maintain this property, we impose a requirement: two or more
<code>Observe()</code> calls with an identical LabelSet during a single Observer
callback invocation are treated as duplicates of each other, where the
last call to <code>Observe()</code> wins.</p>
<p>Based on the Last Value relationship, we can ask and answer questions
such as &quot;what is the average last value of a metric at a point in
time?&quot;.  Observer instruments define the Last Value relationship
without referring to the collection interval and without ambiguity.</p>
<h3><a class="header" href="#last-value-and-measure-instruments" id="last-value-and-measure-instruments">Last-value and Measure instruments</a></h3>
<p>Measure instruments do not define a Last Value relationship.  One
reason is that <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/metrics/api.md#time">synchronous events can happen
simultaneously</a>.</p>
<p>For Measure instruments, it is possible to compute an aggregation that
computes the last-captured value in a collection interval, but it is
potentially not unique and the result will vary depending on the
timing of the collection interval.  For example, a synchronous metric
event that last took place one minute ago will appear as the last
value for collection intervals one minute or longer, but the last
value will be undefined if the collection interval is shorter than one
minute.</p>
<h3><a class="header" href="#aggregating-changes-to-a-sum-rate-calculation" id="aggregating-changes-to-a-sum-rate-calculation">Aggregating changes to a sum: Rate calculation</a></h3>
<p>The former <code>Monotonic</code> option had been introduced in order to support
reporting of a current sum, such that a rate calculation is implied.
Here we defined <em>Rate</em> as an aggregation, defined for a subset of
instruments, that may be calculated differently depending on how the
instrument is defined.  The rate aggregation outputs the amount of
change in a quantity divided by the amount of change in time.</p>
<p>A rate can be computed from values that are reported as differences,
referred to as <em>delta</em> reporting here, or as sums, referred to as
<em>cumulative</em> reporting here.  The primary goal of the instrument
refinements introduced in this proposal is to facilitate rate
calculations in more than one way.</p>
<p>When delta reporting, a rate is calculated by summing individual
measurements or observations.  When cumulative reporting, a rate is
calculated by computing a difference between individual values.</p>
<p>Note that cumulative-reported metric data requires special treatment
of the time dimension when computing rates.  When aggregating across
the time dimension, the difference should be computed.  When
aggregating across spatial dimensions, the sum should be computed.</p>
<h3><a class="header" href="#standard-implementation-of-measure-and-observer" id="standard-implementation-of-measure-and-observer">Standard implementation of Measure and Observer</a></h3>
<p>OpenTelemetry specifies how the default SDK should treat metric
events, by default, when asked to export data from an instrument.
Measure and Observer instruments compute <code>Sum</code> and <code>Count</code>
aggregations, by default, in the standard implementation.  This pair
of measurements, of course, defines an average value.  There are no
restrictions placed on the numerical value in an event for the two
foundational instruments.</p>
<h3><a class="header" href="#refinements-to-measure-and-observer" id="refinements-to-measure-and-observer">Refinements to Measure and Observer</a></h3>
<p>The <code>Monotonic</code> and <code>Absolute</code> options were removed in the 0.3
specification.  Here, we propose to regain the equivalent effects
through instrument refinements.  Instrument refinements are added to
the foundational instruments, yielding new instruments with the same
calling patterns as the foundational instrument they refine.  These
refinements support adding either a different standard implementation
or a restriction of the input domain to the instrument.</p>
<p>We have done away with instrument options, in other words, in favor of
optional metric instruments.  Here we discuss four significant
instrument refinements.</p>
<h4><a class="header" href="#non-negative" id="non-negative">Non-negative</a></h4>
<p>For some instruments, such as those that measure real quantities,
negative values are meaningless.  For example, it is impossible for a
person to weigh a negative amount.</p>
<p>A non-negative instrument refinement accepts only non-negative values.
For instruments with this property, negative values are considered
measurement errors.  Both Measure and Observer instruments support
non-negative refinements.</p>
<h4><a class="header" href="#sum-only" id="sum-only">Sum-only</a></h4>
<p>A sum-only instrument is one where only the sum is considered to be of
interest.  For a sum-only instrument refinement, we have a semantic
property that two events with numeric values <code>M</code> and <code>N</code> are
semantically equivalent to a single event with value <code>M+N</code>.  For
example, in a sum-only count of users arriving by bus to an event, we
are not concerned with the number of buses that arrived.</p>
<p>A sum-only instrument is one where the number of events is not
counted, only the <code>Sum</code>.  A key property of sum-only instruments is
that they always support a Rate aggregation, whether reporting delta-
or cumulative-values.  Both Measure and Observer instruments support
sum-only refinements.</p>
<h4><a class="header" href="#precomputed-sum" id="precomputed-sum">Precomputed-sum</a></h4>
<p>A precomputed-sum refinement indicates that values reported through an
instrument are observed or measured in terms of a sum that changes
over time.  Pre-computed sum instruments support cumulative reporting,
meaning the rate aggregation is defined by computing a difference
across timestamps or collection intervals.</p>
<p>A precomputed sum refinement implies a sum-only refinement.  Note that
values associated with a precomputed sum are still sums.  Precomputed
sum values are combined using addition, when aggregating over the
spatial dimensions; only the time dimension receives special treatment.</p>
<h4><a class="header" href="#non-negative-rate" id="non-negative-rate">Non-negative-rate</a></h4>
<p>A non-negative-rate instrument refinement states that rate aggregation
produces only non-negative results.  There are non-negative-rate cases
of interest for delta reporting and cumulative reporting, as follows.</p>
<p>For delta reporting, any non-negative and sum-only instrument is also
a non-negative-rate instrument.</p>
<p>For cumulative reporting, a sum-only and pre-computed sum instrument
does not necessarily have a non-negative rate, but adding an explicit
non-negative-rate refinement makes it the equivalent of <code>Monotonic</code> in
the 0.2 specification.</p>
<p>For example, the CPU time used by a process, as read in successive
collection intervals, cannot change by a negative amount, because it
is impossible to use a negative amount of CPU time.  CPU time a
typical value to report through an Observer instrument, so the rate
for a specific set of labels is defined by subtracting the prior
observation from the current observation.  Using a non-negative-rate
refinement asserts that the values increases by a non-negative amount
on subsequent collection intervals.</p>
<h4><a class="header" href="#discussion-additive-vs-non-additive-numbers" id="discussion-additive-vs-non-additive-numbers">Discussion: Additive vs. Non-Additive numbers</a></h4>
<p>The refinements proposed above may leave us wondering about the
distinction between an unrefined Measure and the
<em>UpDownCumulativeCounter</em>.  Both values are unrestricted, in terms of
range, so why should they be treated differently?</p>
<p>The <em>UpDownCumulativeCounter</em> has sum-only and precomputed-sum
refinements, which indicate that the numbers being observed are the
result of addition.  These instruments have the additive property that
observing <code>N</code> and <code>M</code> separately is equivalent to observing <code>N+M</code>.
When performing spatial aggregation over data with these additive
properties, it is natural to compute the sum.</p>
<p>When performing spatial aggregation over data without additive
properties, it is natural to combine the distributions.  The
distinction is about how we interpret the values when aggregating.
Use one of the sum-only refinments to report a sum in the default
configuration, otherwise use one of the non-sum-only instruments to
report a distribution.</p>
<h4><a class="header" href="#language-level-refinements" id="language-level-refinements">Language-level refinements</a></h4>
<p>OpenTelemetry implementations may wish to add instrument refinements
to accommodate built-in types.  Languages with distinct integer and
floating point should offer instrument refinements for each, leading
to type names like <code>Int64Measure</code> and <code>Float64Measure</code>.</p>
<p>A language with support for unsigned integer types may wish to create
dedicated instruments to report these values, leading to type names
like <code>UnsignedInt64Observer</code> and <code>UnsignedFloat64Observer</code>.  These
would naturally apply a non-negative refinment.</p>
<p>Other uses for built-in type refinements involve the type for duration
measurements.  For example, where there is built-in type for the
difference between two clock measurements, OpenTelemetry APIs should
offer a refinement to automatically apply the correct unit of time to
the measurement.</p>
<h3><a class="header" href="#counter-refinement" id="counter-refinement">Counter refinement</a></h3>
<p>Counter is a sum-only, non-negative, thus non-negative-rate refinement
of the Measure instrument.</p>
<h3><a class="header" href="#standardizing-new-instruments" id="standardizing-new-instruments">Standardizing new instruments</a></h3>
<p>With these refinements we can exhaustively list each distinct kind of
instrument.  There are a total of twelve hypothetical instruments
listed in the table below, of which only one has been standardized.
Hypothetical future instrument names are <em>italicized</em>.</p>
<table><thead><tr><th>Foundation instrument</th><th>Sum-only?</th><th>Precomputed-sum?</th><th>Non-negative?</th><th>Non-negative-rate?</th><th>Instrument name <em>(hyptothetical)</em></th></tr></thead><tbody>
<tr><td>Measure</td><td>sum-only</td><td></td><td>non-negative</td><td>non-negative-rate</td><td>Counter</td></tr>
<tr><td>Measure</td><td>sum-only</td><td>precomputed-sum</td><td></td><td>non-negative-rate</td><td><em>CumulativeCounter</em></td></tr>
<tr><td>Measure</td><td>sum-only</td><td></td><td></td><td></td><td><em>UpDownCounter</em></td></tr>
<tr><td>Measure</td><td>sum-only</td><td>precomputed-sum</td><td></td><td></td><td><em>UpDownCumulativeCounter</em></td></tr>
<tr><td>Measure</td><td></td><td></td><td>non-negative</td><td></td><td><em>AbsoluteDistribution</em></td></tr>
<tr><td>Measure</td><td></td><td></td><td></td><td></td><td><em>Distribution</em></td></tr>
<tr><td>Observer</td><td>sum-only</td><td></td><td>non-negative</td><td>non-negative-rate</td><td><em>DeltaObserver</em></td></tr>
<tr><td>Observer</td><td>sum-only</td><td>precomputed-sum</td><td></td><td>non-negative-rate</td><td><em>CumulativeObserver</em></td></tr>
<tr><td>Observer</td><td>sum-only</td><td></td><td></td><td></td><td><em>UpDownDeltaObserver</em></td></tr>
<tr><td>Observer</td><td>sum-only</td><td>precomputed-sum</td><td></td><td></td><td><em>UpDownCumulativeObserver</em></td></tr>
<tr><td>Observer</td><td></td><td></td><td>non-negative</td><td></td><td><em>AbsoluteLastValueObserver</em></td></tr>
<tr><td>Observer</td><td></td><td></td><td></td><td></td><td><em>LastValueObserver</em></td></tr>
</tbody></table>
<p>To arrive at this listing, several assumptions have been made.  For
example, the precomputed-sum and non-negative-rate refeinments are
only applicable in conjunction with a sum-only refinement.</p>
<p>For the precomputed-sum instruments, we technically do not care
whether the inputs are non-negative, because rate aggregation computes
differences.  However, it is useful for other aggregations to assume
that precomputed sums start at zero, and we will ignore the case where
a precomputed sum has an initial value other than zero.</p>
<h4><a class="header" href="#gauge-instrument" id="gauge-instrument">Gauge instrument</a></h4>
<p>A Measure instrument with a default Last Value aggregation could be
defined, hypothetically named a <em>Gauge</em> instrument.  This would offer
convenience for users that want this behavior, for there is otherwise
no standard Measure refinement with Last Value aggregation.</p>
<p>Sum-only uses for this hypothetical instrument should instead use
either <em>CumulativeCounter</em> or <em>UpDownCumulativeCounter</em>, since they
are reporting a sum.  This (hypothetical) <em>Gauge</em> instrument would be
useful when a value is time-dependent and the average value is not of
interest.</p>
<h2><a class="header" href="#internal-details-13" id="internal-details-13">Internal details</a></h2>
<p>This is a change of understanding.  It does not request any new
instruments be created or APIs be changed, but it does specify how we
should think about adding new instruments.</p>
<p>No API changes are called for in this proposal.</p>
<h3><a class="header" href="#translation-into-well-known-systems" id="translation-into-well-known-systems">Translation into well-known systems</a></h3>
<h4><a class="header" href="#prometheus" id="prometheus">Prometheus</a></h4>
<p>The Prometheus system defines four kinds of <a href="https://prometheus.io/docs/concepts/metric_types">synchronous metric
instrument</a>.</p>
<table><thead><tr><th>System</th><th>Metric Kind</th><th>Operation</th><th>Aggregation</th><th>Notes</th></tr></thead><tbody>
<tr><td>Prometheus</td><td>Counter</td><td>Inc()</td><td>Sum</td><td>Sum of positive deltas</td></tr>
<tr><td>Prometheus</td><td>Counter</td><td>Add()</td><td>Sum</td><td>Sum of positive deltas</td></tr>
<tr><td>Prometheus</td><td>Gauge</td><td>Set()</td><td>Last Value</td><td>Non-additive or monotonic cumulative</td></tr>
<tr><td>Prometheus</td><td>Gauge</td><td>Inc()/Dec()</td><td>Sum</td><td>Sum of deltas</td></tr>
<tr><td>Prometheus</td><td>Gauge</td><td>Add()/Sub()</td><td>Sum</td><td>Sum of deltas</td></tr>
<tr><td>Prometheus</td><td>Histogram</td><td>Observe()</td><td>Histogram</td><td>Non-negative values</td></tr>
<tr><td>Prometheus</td><td>Summary</td><td>Observe()</td><td>Summary</td><td>Aggregation does not merge</td></tr>
</tbody></table>
<p>Note that the Prometheus Gauge supports five methods (<code>Set</code>, <code>Inc</code>,
<code>Dec</code>, <code>Add</code>, and <code>Sub</code>), one which sets the last value while the
others modify the last value.  This interface is not compatible with
OpenTelemetry, because it requires the SDK to maintain long-lived
state about Gauge values in order to compute the last value following
one of the additive methods (<code>Inc</code>, <code>Dec</code>, <code>Add</code>, and <code>Sub</code>).</p>
<p>If we restrict Prometheus Gauges to support only a <code>Set</code> method, or to
support only the additive methods, then we can model these two
instruments seprately, in a way that is compatible with OpenTelemetry.
A Prometheus Gauge that is used exclusively with <code>Set()</code> can be
modeled as a Measure instrument with Last Value aggregation.  A
Prometheus Gauge that is used exclusively with the additive methods be
modeled as a <code>UpDownCounter</code></p>
<p>Prometheus has support for asynchronous reporting via the &quot;Collector&quot;
interface, but this is a low-level API to support directly exporting
encoded metric data.  The Prometheus &quot;Collector&quot; interface could be
used to implement Observer-like instruments, but they are not natively
supported in Prometheus.</p>
<h4><a class="header" href="#statsd" id="statsd">Statsd</a></h4>
<p>The Statsd system supports only synchronous reporting.</p>
<table><thead><tr><th>System</th><th>Metric Event</th><th>Operation</th><th>Aggregation</th><th>Notes</th></tr></thead><tbody>
<tr><td>Statsd</td><td>Count</td><td>Count()</td><td>Sum</td><td>Sum of deltas</td></tr>
<tr><td>Statsd</td><td>Gauge</td><td>Gauge()</td><td>Last Value</td><td></td></tr>
<tr><td>Statsd</td><td>Histogram</td><td>Histogram()</td><td>Histogram</td><td></td></tr>
<tr><td>Statsd</td><td>Distribution</td><td>Distribution()</td><td><em>Not specified</em></td><td>A distribution summary</td></tr>
<tr><td>Statsd</td><td>Timing</td><td>Timing()</td><td><em>Not specified</em></td><td>Non-negative, distribution summary, Millisecond units</td></tr>
<tr><td>Statsd</td><td>Set</td><td>Set()</td><td>Cardinality</td><td>Unique value count</td></tr>
</tbody></table>
<p>The Statsd Count operation translates into either a Counter, if
increments are non-negative, or an <em>UpDownCounter</em> if values may be
negative.  The Statsd Gauge operation translates into a Measure
instrument configured with Last Value aggregation.</p>
<p>The Histogram, Distribution, and Timing operations are semantically
identical, but have different units and default behavior in statsd
systems.  Each of these distribution-valued instruments can be
replaced using a Measure with a distribution-valued aggregation such
as MinMaxSumCount, Histogram, Exact, or Summary.</p>
<p>The Set operation does not have a direct replacement in OpenTelemetry,
however one can be constructed using a Measure or Observer instrument
and a dummy value.  Each distinct label set is naturally output each
collection interval, whether reported synchronously or asynchronously,
so the set size can be computed by using a metric label as the unique
element and no aggregation operator.</p>
<h4><a class="header" href="#opencensus" id="opencensus">OpenCensus</a></h4>
<p>The OpenCensus system defines three kinds of instrument:</p>
<table><thead><tr><th>System</th><th>Metric Kind</th><th>Operation</th><th>Aggregation</th><th>Notes</th></tr></thead><tbody>
<tr><td>OpenCensus</td><td>Cumulative</td><td>Inc()</td><td>Sum</td><td>Positive deltas</td></tr>
<tr><td>OpenCensus</td><td>Gauge</td><td>Set()</td><td>LastValue</td><td></td></tr>
<tr><td>OpenCensus</td><td>Gauge</td><td>Add()</td><td>Sum</td><td>Deltas</td></tr>
<tr><td>OpenCensus</td><td>Raw-Stats</td><td>Record()</td><td>Sum, Count, Mean, or Distribution</td><td></td></tr>
</tbody></table>
<p>OpenCensus departed from convention with the introduction of a Views
API, which makes it possible to support fewer kinds of instrument
directly, since they can be configured in multiple ways.</p>
<p>Like Prometheus, the combination of multiple APIs in the Gauge
instrument is not compatible with OpenTelemetry.  A Gauge used with
Set() generally implies last-value aggregation, whereas a Gauge used
with Add() is additive and uses Sum aggregation.</p>
<p>Raw statstistics can be aggregated using any aggregation, and all the
OpenCensus aggregations have equivalents in OpenTelemetry.</p>
<p>OpenCensus supported callback-oriented asynchronous forms of both
Cumulative and Gauge instruments.  An asynchronous Cumulative
instrument would be replaced by a CumulativeObserver in OpenTelemetry.
An asynchronous Last-value Gauge would be replaced by AbsoluteObserver
or just the unrestricted Observer.  An asynchronous Additive Gauge
would be replaced by a DeltaObserver.</p>
<h3><a class="header" href="#sample-proposal" id="sample-proposal">Sample Proposal</a></h3>
<p>The the information above will be used to propose a set of refinements
for both synchronous and asynchronous instruments in a follow-on OTEP.
What follows is a sample of the forthcoming proposal, to motivate the
discussion here.</p>
<h4><a class="header" href="#synchronous-instruments" id="synchronous-instruments">Synchronous instruments</a></h4>
<p>The foundational <code>Measure</code> instrument without refinements or
restrictions will be called a <code>Distribution</code> instrument.</p>
<p>Along with <code>Counter</code> and <code>Distribution</code>, we recognize several less-common
but still important cases and reasons why they should be standardized:</p>
<ul>
<li><em>UpDownCounter</em>: Support Prometheus additive Gauge instrument use</li>
<li><em>Timing</em>: Support Prometheus and Statsd timing measurements.</li>
</ul>
<p>Instruments that are not standardized but may be in the future (and why):</p>
<ul>
<li><em>CumulativeCounter</em>: Support a synchronous monotonic cumulative instrument</li>
<li><em>AbsoluteDistribution</em>: Support non-negative valued distributions</li>
</ul>
<p>Instruments that are probably not seen as widely useful:</p>
<ul>
<li><em>UpDownCumulativeCounter</em>: We believe this is better handled asynchronously.</li>
</ul>
<h4><a class="header" href="#observer-instruments" id="observer-instruments">Observer instruments</a></h4>
<p>The foundational <code>Observer</code> instrument without refinements or
restrictions shall be called a <code>LastValueObserver</code> instrument.</p>
<p>We have identified important cases that should be standardized:</p>
<ul>
<li><em>CumulativeObserver</em>: Support a cumulative monotone counter</li>
<li><em>DeltaObserver</em>: Support an asynchronous delta counter.</li>
</ul>
<p>Observer refinements that could be standardized in the future:</p>
<ul>
<li><em>UpDownCumulativeObserver</em>: Observe a non-monotonic cumluative counter</li>
<li><em>UpDownDeltaObserver</em>: Observe positive and negative deltas</li>
<li><em>AbsoluteLastValueObserver</em>: Observe non-negative current values.</li>
</ul>
<h2><a class="header" href="#example-observer-aggregation" id="example-observer-aggregation">Example: Observer aggregation</a></h2>
<p>Suppose you wish to capture the CPU usage of a process broken down by
the CPU core ID.  The operating system provides a mechanism to read
the current usage from the <code>/proc</code> file system, which will be reported
once per collection interval using an Observer instrument.  Because
this is a precomputed sum with a non-negative rate, use a
<em>CumulativeObserver</em> to report this quantity with a metric label
indicating the CPU core ID.</p>
<p>It will be common to compute a rate of CPU usage over this data.  The
rate can be calculated for an individual CPU core by computing a
difference between the value of two metric events.  To compute the
aggregate rate across all cores–a spatial aggregation–these
differences are added together.</p>
<h2><a class="header" href="#open-questions-7" id="open-questions-7">Open Questions</a></h2>
<p>Are there still questions surrounding the former Monotonic refinement?</p>
<p>Should the <em>CumulativeObserver</em> instrument be named
<em>MonotonicObserver</em>?  In this proposal, we prefer <em>Cumulative</em> and
<em>UpDownCumulative</em>.  <em>Cumulative</em> is a good descriptive term in this
setting (i.e., some additive values are <em>cumulative</em>, some are
<em>delta</em>).  Being <em>Cumulative</em> and not <em>UpDownCumulative</em> implies
monotonicity in this proposal.</p>
<p>For synchronous instruments, this proposals does not standardize
<em>CumulativeCounter</em>. Such an instrument might be named
<em>MonotonicCounter</em>.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-12" id="trade-offs-and-mitigations-12">Trade-offs and mitigations</a></h2>
<p>The trade-off explicitly introduced here is that we should prefer to
create new instrument refinements, each for a dedicated purpose,
rather than create generic instruments with support for multiple
semantic options.</p>
<h2><a class="header" href="#prior-art-and-alternatives-14" id="prior-art-and-alternatives-14">Prior art and alternatives</a></h2>
<p>The optional behaviors <code>Monotonic</code> and <code>Absolute</code> were first discussed
in the August 2019 Metrics working group meeting.</p>
<h2><a class="header" href="#future-possibilities-4" id="future-possibilities-4">Future possibilities</a></h2>
<p>A future OTEP will request the introduction of two standard
refinements for the 0.4 API specification.  This will be the
<code>CumulativeObserver</code> instrument described above plus a synchronous
timing instrument named <code>TimingMeasure</code> that is equivalent to
<em>AbsoluteMeasure</em> with the correct unit and a language-specific
duration type for measuring time.</p>
<p>If the above open question is decided in favor of treating the
foundational instruments as abstract, instrument names like
<em>NonAbsoluteMeasure</em> and <em>NonAbsoluteCounter</em> will need to be
standardized.</p>
<h1><a class="header" href="#remove-the-labelset-object-from-the-metrics-api" id="remove-the-labelset-object-from-the-metrics-api">Remove the LabelSet object from the metrics API</a></h1>
<p>The proposal is to remove the current <a href="./0049-metric-label-set.html"><code>LabelSet</code></a>
API and change all the current APIs that accept LabelSet to accept directly the
labels (list of key-values, or a map of key-values based on the language
capabilities).</p>
<h2><a class="header" href="#motivation-20" id="motivation-20">Motivation</a></h2>
<p>The <a href="./0049-metric-label-set.html"><code>LabelSet</code></a> API type was added to serve as a
handle on a pre-defined set of labels for the Metrics API.</p>
<p>This API represents an optimization for the current metrics API that allows the
implementations to avoid encoding and checking labels restrictions multiple
times for the same set of lables. Usages and implementations of the metrics API
have shown that LabelSet adds extra unnecessary complexity with little benefit.</p>
<p>Some users prefer to avoid this performance optimization for the benefit of a
cleaner code and OpenTelemetry needs to address them as well, so this means that
it is important for OpenTelemetry to support record APIs where users can pass
directly the labels.</p>
<p>OpenTelementry can always add this optimization later (backwards compatible
change) if we determine that it is very important to have.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-13" id="trade-offs-and-mitigations-13">Trade-offs and mitigations</a></h2>
<p>In case where performance matters, here are the ways to achieve almost the same performance:</p>
<ul>
<li>In the current API if a <code>LabelSet</code> is reused across multiple individual
records across different instruments (one record to every instrument) then user
can use the batch recording mechanism, so internally the SDK can do the labels
encoding once.</li>
<li>In the current API if a <code>LabelSet</code> is used multiple times to record to the
same instrument then user can use instrument bindings.</li>
<li>In the current API if a <code>LabelSet</code> is used across multiple batch recordings,
and this pattern becomes very important, then OpenTelemetry can add support for
batches to accept bindings.</li>
</ul>
<p>To ensure that the current batch recording can help in scenarios where there are
some local conditions that control which measurements to be recorded, the
recommendation is to have the <code>newBatchRecorder</code> return an interface called
<code>BatchRecorder</code> that can be used to add <code>measurement</code> and when all entries are
added call <code>record</code> to record all the <code>measurements</code>.</p>
<h2><a class="header" href="#prior-art-and-alternatives-15" id="prior-art-and-alternatives-15">Prior art and alternatives</a></h2>
<p>Almost all the existing Metric libraries do not require users to create
something like LabelSet when recording a value.</p>
<h1><a class="header" href="#logs-vocabulary" id="logs-vocabulary">Logs: Vocabulary</a></h1>
<p>This documents defines the vocabulary for logs to be used across OpenTelemetry project.</p>
<h2><a class="header" href="#motivation-21" id="motivation-21">Motivation</a></h2>
<p>We need a common language and common understanding of terms that we use to
avoid the chaos experienced by the builders of the Tower of Babel.</p>
<h2><a class="header" href="#proposal-1" id="proposal-1">Proposal</a></h2>
<p>OpenTelemetry specification already contains a <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/overview.md">vocabulary</a>
for Traces, Metrics and other relevant concepts.</p>
<p>This proposal is to add the following concepts to the vocabulary.</p>
<h3><a class="header" href="#log-record" id="log-record">Log Record</a></h3>
<p>A recording of an event. Typically the record includes a timestamp indicating
when the event happened as well as other data that describes what happened,
where it happened, etc.</p>
<p>Also known as Log Entry.</p>
<h3><a class="header" href="#log" id="log">Log</a></h3>
<p>Sometimes used to refer to a collection of Log Records. May be ambiguous, since
people also sometimes use <code>Log</code> to refer to a single <code>Log Record</code>, thus this
term should be used carefully and in the context where ambiguity is possible
additional qualifiers should be used (e.g. <code>Log Record</code>).</p>
<h3><a class="header" href="#embedded-log" id="embedded-log">Embedded Log</a></h3>
<p><code>Log Records</code> embedded inside a <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/trace/api.md#span">Span</a>
object, in the <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/trace/api.md#add-events">Events</a> list.</p>
<h3><a class="header" href="#standalone-log" id="standalone-log">Standalone Log</a></h3>
<p><code>Log Records</code> that are not embedded inside a <code>Span</code> and are recorded elsewhere.</p>
<h3><a class="header" href="#log-attributes" id="log-attributes">Log Attributes</a></h3>
<p>Key/value pairs contained in a <code>Log Record</code>.</p>
<h3><a class="header" href="#structured-logs" id="structured-logs">Structured Logs</a></h3>
<p>Logs that are recorded in a format which has a well-defined structure that allows
to differentiate between different elements of a Log Record (e.g. the Timestamp,
the Attributes, etc). The <em>Syslog protocol</em> (<a href="https://tools.ietf.org/html/rfc5424">RFC 5425</a>),
for example, defines a <code>structured-data</code> format.</p>
<h3><a class="header" href="#flat-file-logs" id="flat-file-logs">Flat File Logs</a></h3>
<p>Logs recorded in text files, often one line per log record (although multiline
records are possible too). There is no common industry agreement whether
logs written to text files in more structured formats (e.g. JSON files)
are considered Flat File Logs or not. Where such distinction is important it is
recommended to call it out specifically.</p>
<h1><a class="header" href="#opentelemetry-logs-vision" id="opentelemetry-logs-vision">OpenTelemetry Logs Vision</a></h1>
<p>The following are high-level items that define our long-term vision for
Logs support in OpenTelemetry project, what we aspire to achieve.</p>
<p>This a vision document that reflects our current desires. It is not a commitment
to implement everything precisely as listed. The primary purpose of this
document is to ensure all contributors work in alignment. As our vision changes
over time maintainers reserve the right to add, modify, and remove items from
this document.</p>
<p>This document uses vocabulary introduced in <a href="https://github.com/open-telemetry/oteps/pull/91">https://github.com/open-telemetry/oteps/pull/91</a>.</p>
<h2><a class="header" href="#first-class-citizen" id="first-class-citizen">First-class Citizen</a></h2>
<p>Logs are a first-class citizen in observability, along with traces and metrics.
We will aim to have best-in-class support for logs at OpenTelemetry.</p>
<h2><a class="header" href="#correlation" id="correlation">Correlation</a></h2>
<p>OpenTelemetry will define how logs will be correlated with traces and metrics
and how this correlation information will be stored.</p>
<p>Correlation will work across 2 major dimensions:</p>
<ul>
<li>To correlate telemetry emitted for the same request (also known as Request
or Trace Context Correlation),</li>
<li>To correlate telemetry emitted from the same source (also known as Resource
Context Correlation).</li>
</ul>
<h2><a class="header" href="#logs-data-model" id="logs-data-model">Logs Data Model</a></h2>
<p>We will design a Log Data model that will aim to correctly represent all types
of logs. The purpose of the data model is to have a common understanding of what
a log record is, what data needs to be recorded, transferred, stored and
interpreted by a logging system.</p>
<p>Existing log formats can be unambiguously mapped to this data model. Reverse
mapping from this data model is also possible to the extent that the target log
format has equivalent capabilities.</p>
<p>We will produce mapping recommendations for commonly used log formats.</p>
<h2><a class="header" href="#log-protocol" id="log-protocol">Log Protocol</a></h2>
<p>Armed with the Log Data model we will aim to design a high performance protocol
for logs, which will pursue the same <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/protocol/design-goals.md">design goals</a>
as we had for the traces and metrics protocol.</p>
<p>Most notably the protocol will aim to be highly reliable, have low resource
consumption, be suitable for all participant nodes, ensure high throughput,
allow backpressure signalling and be load-balancer friendly (see the design
goals link above for clarifications).</p>
<p>The reason for this design is to have a single OpenTelemetry protocol that can
deliver logs, traces and metrics via one connection and satisfy all design
goals.</p>
<h2><a class="header" href="#unified-collection" id="unified-collection">Unified Collection</a></h2>
<p>We aim to have high-performance, unified
<a href="https://github.com/open-telemetry/opentelemetry-collector/">Collector</a> that
support logs, traces and metrics in one package, symmetrically and uniformly for
all 3 types of telemetry data (see also
<a href="https://github.com/open-telemetry/opentelemetry-collector/blob/8310e665ec1babfd56ca5b1cfec91c1f997f4f2c/docs/vision.md">Collector vision</a>).</p>
<p>The unified Collector will support multiple log protocols including the newly
designed OpenTelemetry log protocol.</p>
<p>Unified collection is important for the following reasons:</p>
<ul>
<li>One agent (or one collector) to deploy and manage.</li>
<li>One place of configuration for target endpoints, authentication tokens, etc.</li>
<li>Uniform tagging of all 3 types of telemetry data (enrichment by attributes
of resources where the data comes from or by user-defined attributes),
enabling correct correlation across Resource dimensions later on the backend.</li>
</ul>
<h2><a class="header" href="#cloud-native" id="cloud-native">Cloud Native</a></h2>
<p>We will have best-in-class support for logs emitted in cloud native environments
(e.g. Kubernetes, serverless, etc), including legacy applications running
in such environments. This is in line with our CNCF mission.</p>
<h2><a class="header" href="#support-legacy" id="support-legacy">Support Legacy</a></h2>
<p>We will produce guidelines on how legacy applications can emit logs in a
manner that makes them compatible with OpenTelemetry's approach and enables
telemetry data correlation. We will also have a reasonable story around
logs that are emitted by sources over which we may have no control and which
emit logs in pre-defined formats via pre-defined mediums (e.g. flat file logs,
Syslog, etc).</p>
<p>We will have technical solutions or guidelines for using popular logging
libraries in a OpenTelemetry-compatible manner and we may produce logging
libraries for languages where gaps exist.</p>
<p>This is important because we believe software that was created before
OpenTelemetry should not be disregarded and should benefit from OpenTelemetry
efforts where possible.</p>
<h3><a class="header" href="#auto-instrumentation" id="auto-instrumentation">Auto-instrumentation</a></h3>
<p>To enable functionality that requires modification of how logs are emitted we
will work on auto-instrumenting solutions. This will reduce the adoption barrier
for existing deployments.</p>
<h3><a class="header" href="#applicable-to-all-log-sources" id="applicable-to-all-log-sources">Applicable to All Log Sources</a></h3>
<p>Logging support at OpenTelemetry will be applicable to all sorts of log sources:
system logs, infrastructure logs, third-party and first-party application logs.</p>
<h3><a class="header" href="#standalone-and-embedded-logs" id="standalone-and-embedded-logs">Standalone and Embedded Logs</a></h3>
<p>OpenTelemetry will support both logs embedded inside <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/trace/api.md#span">Spans</a>
and standalone logs recorded elsewhere. The support of embedded logs is
important for OpenTelemetry's primary use cases, where errors and exceptions
need to be embedded in Spans. The support of standalone logs is important for
legacy applications which may not emit Spans at all.</p>
<h2><a class="header" href="#legacy-use-cases" id="legacy-use-cases">Legacy Use Cases</a></h2>
<p>Logging technology has a decades-long history. There exists a large number of
logging libraries, collection agents, network protocols, open-source and
proprietary backends. We recognize this fact and aim to make our proposals in a
manner that honours valid legacy use-cases, while at the same time suggests
better solutions where they are due.</p>
<h1><a class="header" href="#log-data-model" id="log-data-model">Log Data Model</a></h1>
<p>Introduce Data Model for Log Records as it is understood by OpenTelemetry.</p>
<ul>
<li><a href="0097-log-data-model.html#motivation">Motivation</a></li>
<li><a href="0097-log-data-model.html#design-notes">Design Notes</a>
<ul>
<li><a href="0097-log-data-model.html#requirements">Requirements</a></li>
<li><a href="0097-log-data-model.html#field-kinds">Field Kinds</a></li>
</ul>
</li>
<li><a href="0097-log-data-model.html#log-and-event-record-definition">Log and Event Record Definition</a>
<ul>
<li><a href="0097-log-data-model.html#field-timestamp">Field: <code>Timestamp</code></a></li>
<li><a href="0097-log-data-model.html#trace-context-fields">Trace Context Fields</a>
<ul>
<li><a href="0097-log-data-model.html#field-traceid">Field: <code>TraceId</code></a></li>
<li><a href="0097-log-data-model.html#field-spanid">Field: <code>SpanId</code></a></li>
<li><a href="0097-log-data-model.html#field-traceflags">Field: <code>TraceFlags</code></a></li>
</ul>
</li>
<li><a href="0097-log-data-model.html#severity-fields">Severity Fields</a>
<ul>
<li><a href="0097-log-data-model.html#field-severitytext">Field: <code>SeverityText</code></a></li>
<li><a href="0097-log-data-model.html#field-severitynumber">Field: <code>SeverityNumber</code></a></li>
<li><a href="0097-log-data-model.html#mapping-of-severitynumber">Mapping of <code>SeverityNumber</code></a></li>
<li><a href="0097-log-data-model.html#reverse-mapping">Reverse Mapping</a></li>
<li><a href="0097-log-data-model.html#error-semantics">Error Semantics</a></li>
<li><a href="0097-log-data-model.html#displaying-severity">Displaying Severity</a></li>
<li><a href="0097-log-data-model.html#comparing-severity">Comparing Severity</a></li>
</ul>
</li>
<li><a href="0097-log-data-model.html#field-name">Field: <code>Name</code></a></li>
<li><a href="0097-log-data-model.html#field-body">Field: <code>Body</code></a></li>
<li><a href="0097-log-data-model.html#field-resource">Field: <code>Resource</code></a></li>
<li><a href="0097-log-data-model.html#field-attributes">Field: <code>Attributes</code></a></li>
</ul>
</li>
<li><a href="0097-log-data-model.html#example-log-records">Example Log Records</a></li>
<li><a href="0097-log-data-model.html#questions-resolved-during-otep-discussion">Questions Resolved during OTEP discussion</a>
<ul>
<li><a href="0097-log-data-model.html#traceflags-vs-traceparent-and-tracestate">TraceFlags vs TraceParent and TraceState</a></li>
<li><a href="0097-log-data-model.html#severity-fields-1">Severity Fields</a></li>
<li><a href="0097-log-data-model.html#timestamp-requirements">Timestamp Requirements</a></li>
<li><a href="0097-log-data-model.html#security-logs">Security Logs</a></li>
</ul>
</li>
<li><a href="0097-log-data-model.html#alternate-design">Alternate Design</a></li>
<li><a href="0097-log-data-model.html#prior-art">Prior Art</a>
<ul>
<li><a href="0097-log-data-model.html#rfc5424-syslog">RFC5424 Syslog</a></li>
<li><a href="0097-log-data-model.html#fluentd-forward-protocol-model">Fluentd Forward Protocol Model</a></li>
</ul>
</li>
<li><a href="0097-log-data-model.html#appendix-a-example-mappings">Appendix A. Example Mappings</a>
<ul>
<li><a href="0097-log-data-model.html#rfc5424-syslog-1">RFC5424 Syslog</a></li>
<li><a href="0097-log-data-model.html#windows-event-log">Windows Event Log</a></li>
<li><a href="0097-log-data-model.html#signalfx-events">SignalFx Events</a></li>
<li><a href="0097-log-data-model.html#splunk-hec">Splunk HEC</a></li>
<li><a href="0097-log-data-model.html#log4j">Log4j</a></li>
<li><a href="0097-log-data-model.html#zap">Zap</a></li>
<li><a href="0097-log-data-model.html#apache-http-server-access-log">Apache HTTP Server access log</a></li>
<li><a href="0097-log-data-model.html#cloudtrail-log-event">CloudTrail Log Event</a></li>
<li><a href="0097-log-data-model.html#google-cloud-logging">Google Cloud Logging</a></li>
</ul>
</li>
<li><a href="0097-log-data-model.html#elastic-common-schema">Elastic Common Schema</a></li>
<li><a href="0097-log-data-model.html#appendix-b-severitynumber-example-mappings">Appendix B: <code>SeverityNumber</code> example mappings</a></li>
<li><a href="0097-log-data-model.html#references">References</a></li>
</ul>
<h2><a class="header" href="#motivation-22" id="motivation-22">Motivation</a></h2>
<p>This is a proposal of a data model and semantic conventions that allow to
represent logs from various sources: application log files, machine generated
events, system logs, etc. Existing log formats can be unambiguously mapped to
this data model. Reverse mapping from this data model is also possible to the
extent that the target log format has equivalent capabilities.</p>
<p>The purpose of the data model is to have a common understanding of what a log
record is, what data needs to be recorded, transferred, stored and interpreted
by a logging system.</p>
<p>This proposal defines a data model for <a href="https://github.com/open-telemetry/oteps/blob/master/text/logs/0091-logs-vocabulary.md#standalone-log">Standalone
Logs</a>.
Relevant parts of it may be adopted for
<a href="https://github.com/open-telemetry/oteps/blob/master/text/logs/0091-logs-vocabulary.md#embedded-log">Embedded Logs</a>
in a future OTEP.</p>
<h2><a class="header" href="#design-notes" id="design-notes">Design Notes</a></h2>
<h3><a class="header" href="#requirements-1" id="requirements-1">Requirements</a></h3>
<p>The Data Model was designed to satisfy the following requirements:</p>
<ul>
<li>
<p>It should be possible to unambiguously map existing log formats to this Data
Model. Translating log data from an arbitrary log format to this Data Model
and back should ideally result in identical data.</p>
</li>
<li>
<p>Mappings of other log formats to this Data Model should be semantically
meaningful. The Data Model must preserve the semantics of particular elements
of existing log formats.</p>
</li>
<li>
<p>Translating log data from an arbitrary log format A to this Data Model and
then translating from the Data Model to another log format B ideally must
result in a meaningful translation of log data that is no worse than a
reasonable direct translation from log format A to log format B.</p>
</li>
<li>
<p>It should be possible to efficiently represent the Data Model in concrete
implementations that require the data to be stored or transmitted. We
primarily care about 2 aspects of efficiency: CPU usage for
serialization/deserialization and space requirements in serialized form. This
is an indirect requirement that is affected by the specific representation of
the Data Model rather than the Data Model itself, but is still useful to keep
in mind.</p>
</li>
</ul>
<p>The Data Model aims to successfully represent 3 sorts of logs and events:</p>
<ul>
<li>
<p>System Formats. These are logs and events generated by the operating system
and over which we have no control - we cannot change the format or affect what
information is included (unless the data is generated by an application which
we can modify). An example of system format is Syslog.</p>
</li>
<li>
<p>Third-party Applications. These are generated by third-party applications. We
may have certain control over what information is included, e.g. customize the
format. An example is Apache log file.</p>
</li>
<li>
<p>First-party Applications. These are applications that we develop and we have
some control over how the logs and events are generated and what information
we include in the logs. We can likely modify the source code of the
application if needed.</p>
</li>
</ul>
<h3><a class="header" href="#field-kinds" id="field-kinds">Field Kinds</a></h3>
<p>This Data Model defines a logical model for a log record (irrespective of the
physical format and encoding of the record). Each record contains 2 kinds of
fields:</p>
<ul>
<li>
<p>Named top-level fields of specific type and meaning.</p>
</li>
<li>
<p>Fields stored in the key/value pair lists, which can contain arbitrary values
of different types. The keys and values for well-known fields follow semantic
conventions for key names and possible values that allow all parties that work
with the field to have the same interpretation of the data. See references to
semantic conventions for <code>Resource</code> and <code>Attributes</code> fields and examples in
<a href="0097-log-data-model.html#appendix-a-example-mappings">Appendix A</a>.</p>
</li>
</ul>
<p>The reasons for having these 2 kinds of fields are:</p>
<ul>
<li>
<p>Ability to efficiently represent named top-level fields, which are almost
always present (e.g. when using encodings like Protocol Buffers where fields
are enumerated but not named on the wire).</p>
</li>
<li>
<p>Ability to enforce types of named fields, which is very useful for compiled
languages with type checks.</p>
</li>
<li>
<p>Flexibility to represent less frequent data via key/value pair lists. This
includes well-known data that has standardized semantics as well as arbitrary
custom data that the application may want to include in the logs.</p>
</li>
</ul>
<p>When designing this data model we followed the following reasoning to make a
decision about when to use a top-level named field:</p>
<ul>
<li>
<p>The field needs to be either mandatory for all records or be frequently
present in well-known log and event formats (such as <code>Timestamp</code>) or is
expected to be often present in log records in upcoming logging systems (such
as <code>TraceId</code>).</p>
</li>
<li>
<p>The field’s semantics must be the same for all known log and event formats and
can be mapped directly and unambiguously to this data model.</p>
</li>
</ul>
<p>Both of the above conditions were required to give the field a place in the
top-level structure of the record.</p>
<h2><a class="header" href="#log-and-event-record-definition" id="log-and-event-record-definition">Log and Event Record Definition</a></h2>
<p>Note: below we use type <code>any</code>, which can be a scalar value (number, string or
boolean), or an array or map of values. Arbitrary deep nesting of values for
arrays and maps is allowed (essentially allow to represent an equivalent of a
JSON object).</p>
<p><a href="0097-log-data-model.html#appendix-a-example-mappings">Appendix A</a> contains many examples that show how
existing log formats map to the fields defined below. If there are questions
about the meaning of the field reviewing the examples may be helpful.</p>
<p>Here is the list of fields in a log record:</p>
<table><thead><tr><th>Field Name</th><th>Description</th></tr></thead><tbody>
<tr><td>Timestamp</td><td>Time when the event occurred.</td></tr>
<tr><td>TraceId</td><td>Request trace id.</td></tr>
<tr><td>SpanId</td><td>Request span id.</td></tr>
<tr><td>TraceFlags</td><td>W3C trace flag.</td></tr>
<tr><td>SeverityText</td><td>The severity text (also known as log level).</td></tr>
<tr><td>SeverityNumber</td><td>Numerical value of the severity.</td></tr>
<tr><td>Name</td><td>Short event identifier.</td></tr>
<tr><td>Body</td><td>The body of the log record.</td></tr>
<tr><td>Resource</td><td>Describes the source of the log.</td></tr>
<tr><td>Attributes</td><td>Additional information about the event.</td></tr>
</tbody></table>
<p>Below is the detailed description of each field.</p>
<h3><a class="header" href="#field-timestamp" id="field-timestamp">Field: <code>Timestamp</code></a></h3>
<p>Type: Timestamp, uint64 nanoseconds since Unix epoch.</p>
<p>Description: Time when the event occurred measured by the origin clock. This
field is optional, it may be missing if the timestamp is unknown.</p>
<h3><a class="header" href="#trace-context-fields" id="trace-context-fields">Trace Context Fields</a></h3>
<h4><a class="header" href="#field-traceid" id="field-traceid">Field: <code>TraceId</code></a></h4>
<p>Type: byte sequence.</p>
<p>Description: Request trace id as defined in
<a href="https://www.w3.org/TR/trace-context/#trace-id">W3C Trace Context</a>. Can be set
for logs that are part of request processing and have an assigned trace id. This
field is optional.</p>
<h4><a class="header" href="#field-spanid" id="field-spanid">Field: <code>SpanId</code></a></h4>
<p>Type: byte sequence.</p>
<p>Description: Span id. Can be set for logs that are part of a particular
processing span. If SpanId is present TraceId SHOULD be also present. This field
is optional.</p>
<h4><a class="header" href="#field-traceflags" id="field-traceflags">Field: <code>TraceFlags</code></a></h4>
<p>Type: byte.</p>
<p>Description: Trace flag as defined in
<a href="https://www.w3.org/TR/trace-context/#trace-flags">W3C Trace Context</a>
specification. At the time of writing the specification defines one flag - the
SAMPLED flag. This field is optional.</p>
<h3><a class="header" href="#severity-fields" id="severity-fields">Severity Fields</a></h3>
<h4><a class="header" href="#field-severitytext" id="field-severitytext">Field: <code>SeverityText</code></a></h4>
<p>Type: string.</p>
<p>Description: severity text (also known as log level). This is the original
string representation of the severity as it is known at the source. If this
field is missing and <code>SeverityNumber</code> is present then the short name that
corresponds to the <code>SeverityNumber</code> may be used as a substitution. This field is
optional.</p>
<h4><a class="header" href="#field-severitynumber" id="field-severitynumber">Field: <code>SeverityNumber</code></a></h4>
<p>Type: number.</p>
<p>Description: numerical value of the severity, normalized to values described in
this document. This field is optional.</p>
<p><code>SeverityNumber</code> is an integer number. Smaller numerical values correspond to
less severe events (such as debug events), larger numerical values correspond to
more severe events (such as errors and critical events). The following table
defines the meaning of <code>SeverityNumber</code> value:</p>
<table><thead><tr><th>SeverityNumber range</th><th>Range name</th><th>Meaning</th></tr></thead><tbody>
<tr><td>1-4</td><td>TRACE</td><td>A fine-grained debugging event. Typically disabled in default configurations.</td></tr>
<tr><td>5-8</td><td>DEBUG</td><td>A debugging event.</td></tr>
<tr><td>9-12</td><td>INFO</td><td>An informational event. Indicates that an event happened.</td></tr>
<tr><td>13-16</td><td>WARN</td><td>A warning event. Not an error but is likely more important than an informational event.</td></tr>
<tr><td>17-20</td><td>ERROR</td><td>An error event. Something went wrong.</td></tr>
<tr><td>21-24</td><td>FATAL</td><td>A fatal error such as application or system crash.</td></tr>
</tbody></table>
<p>Smaller numerical values in each range represent less important (less severe)
events. Larger numerical values in each range represent more important (more
severe) events. For example <code>SeverityNumber=17</code> describes an error that is less
critical than an error with <code>SeverityNumber=20</code>.</p>
<h4><a class="header" href="#mapping-of-severitynumber" id="mapping-of-severitynumber">Mapping of <code>SeverityNumber</code></a></h4>
<p>Mappings from existing logging systems and formats (or <strong>source format</strong> for
short) must define how severity (or log level) of that particular format
corresponds to <code>SeverityNumber</code> of this data model based on the meaning given
for each range in the above table.</p>
<p>If the source format has more than one severity that matches a single range in
this table then the severities of the source format must be assigned numerical
values from that range according to how severe (important) the source severity
is.</p>
<p>For example if the source format defines &quot;Error&quot; and &quot;Critical&quot; as error events
and &quot;Critical&quot; is a more important and more severe situation then we can choose
the following <code>SeverityNumber</code> values for the mapping: &quot;Error&quot;-&gt;17,
&quot;Critical&quot;-&gt;18.</p>
<p>If the source format has only a single severity that matches the meaning of the
range then it is recommended to assign that severity the smallest value of the
range.</p>
<p>For example if the source format has an &quot;Informational&quot; log level and no other
log levels with similar meaning then it is recommended to use
<code>SeverityNumber=9</code> for &quot;Informational&quot;.</p>
<p>Source formats that do not define a concept of severity or log level MAY omit
<code>SeverityNumber</code> and <code>SeverityText</code> fields. Backend and UI may represent log
records with missing severity information distinctly or may interpret log
records with missing <code>SeverityNumber</code> and <code>SeverityText</code> fields as if the
<code>SeverityNumber</code> was set equal to INFO (numeric value of 9).</p>
<h4><a class="header" href="#reverse-mapping" id="reverse-mapping">Reverse Mapping</a></h4>
<p>When performing a reverse mapping from <code>SeverityNumber</code> to a specific format
and the <code>SeverityNumber</code> has no corresponding mapping entry for that format
then it is recommended to choose the target severity that is in the same
severity range and is closest numerically.</p>
<p>For example Zap has only one severity in the INFO range, called &quot;Info&quot;. When
doing reverse mapping all <code>SeverityNumber</code> values in INFO range (numeric 9-12)
will be mapped to Zap’s &quot;Info&quot; level.</p>
<h4><a class="header" href="#error-semantics" id="error-semantics">Error Semantics</a></h4>
<p>If <code>SeverityNumber</code> is present and has a value of ERROR (numeric 17) or higher
then it is an indication that the log record represents an erroneous situation.
It is up to the reader of this value to make a decision on how to use this fact
(e.g. UIs may display such errors in a different color or have a feature to find
all erroneous log records).</p>
<p>If the log record represents an erroneous event and the source format does not
define a severity or log level concept then it is recommended to set
<code>SeverityNumber</code> to ERROR (numeric 17) during the mapping process. If the log
record represents a non-erroneous event the <code>SeverityNumber</code> field may be
omitted or may be set to any numeric value less than ERROR (numeric 17). The
recommended value in this case is INFO (numeric 9). See
<a href="0097-log-data-model.html#appendix-b-severitynumber-example-mappings">Appendix B</a> for more mapping
examples.</p>
<h4><a class="header" href="#displaying-severity" id="displaying-severity">Displaying Severity</a></h4>
<p>The following table defines the recommended short name for each
<code>SeverityNumber</code> value. The short name can be used for example for representing
the <code>SeverityNumber</code> in the UI:</p>
<table><thead><tr><th>SeverityNumber</th><th>Short Name</th></tr></thead><tbody>
<tr><td>1</td><td>TRACE</td></tr>
<tr><td>2</td><td>TRACE2</td></tr>
<tr><td>3</td><td>TRACE3</td></tr>
<tr><td>4</td><td>TRACE4</td></tr>
<tr><td>5</td><td>DEBUG</td></tr>
<tr><td>6</td><td>DEBUG2</td></tr>
<tr><td>7</td><td>DEBUG3</td></tr>
<tr><td>8</td><td>DEBUG4</td></tr>
<tr><td>9</td><td>INFO</td></tr>
<tr><td>10</td><td>INFO2</td></tr>
<tr><td>11</td><td>INFO3</td></tr>
<tr><td>12</td><td>INFO4</td></tr>
<tr><td>13</td><td>WARN</td></tr>
<tr><td>14</td><td>WARN2</td></tr>
<tr><td>15</td><td>WARN3</td></tr>
<tr><td>16</td><td>WARN4</td></tr>
<tr><td>17</td><td>ERROR</td></tr>
<tr><td>18</td><td>ERROR2</td></tr>
<tr><td>19</td><td>ERROR3</td></tr>
<tr><td>20</td><td>ERROR4</td></tr>
<tr><td>21</td><td>FATAL</td></tr>
<tr><td>22</td><td>FATAL2</td></tr>
<tr><td>23</td><td>FATAL3</td></tr>
<tr><td>24</td><td>FATAL4</td></tr>
</tbody></table>
<p>When an individual log record is displayed it is recommended to show both
<code>SeverityText</code> and <code>SeverityNumber</code> values. A recommended combined string in
this case begins with the short name followed by <code>SeverityText</code> in parenthesis.</p>
<p>For example &quot;Informational&quot; Syslog record will be displayed as <strong>INFO
(Informational)</strong>. When for a particular log record the <code>SeverityNumber</code> is
defined but the <code>SeverityText</code> is missing it is recommended to only show the
short name, e.g. <strong>INFO</strong>.</p>
<p>When drop down lists (or other UI elements that are intended to represent the
possible set of values) are used for representing the severity it is preferable
to display the short name in such UI elements.</p>
<p>For example a dropdown list of severities that allows filtering log records by
severities is likely to be more usable if it contains the short names of
<code>SeverityNumber</code> (and thus has a limited upper bound of elements) compared to a
dropdown list, which lists all distinct <code>SeverityText</code> values that are known to
the system (which can be a large number of elements, often differing only in
capitalization or abbreviated, e.g. &quot;Info&quot; vs &quot;Information&quot;).</p>
<h4><a class="header" href="#comparing-severity" id="comparing-severity">Comparing Severity</a></h4>
<p>In the contexts where severity participates in less-than / greater-than
comparisons <code>SeverityNumber</code> field should be used. <code>SeverityNumber</code> can be
compared to another <code>SeverityNumber</code> or to numbers in the 1..24 range (or to the
corresponding short names).</p>
<p>When severity is used in equality or inequality comparisons (for example in
filters in the UIs) the recommendation is to attempt to use both <code>SeverityText</code>
and short name of <code>SeverityNumber</code> to perform matches (i.e. equality with either
of these fields should be considered a match). For example if we have a record
with <code>SeverityText</code> field equal to &quot;Informational&quot; and <code>SeverityNumber</code> field
equal to INFO then it may be preferable from the user experience perspective to
ensure that <strong>severity=&quot;Informational&quot;</strong> and <strong>severity=&quot;INFO&quot;</strong> conditions both
to are TRUE for that record.</p>
<h3><a class="header" href="#field-name" id="field-name">Field: <code>Name</code></a></h3>
<p>Type: string.</p>
<p>Description: Short event identifier that does not contain varying parts.
<code>Name</code> describes what happened (e.g. &quot;ProcessStarted&quot;). Recommended to be
no longer than 50 characters. Not guaranteed to be unique in any way. Typically
used for filtering and grouping purposes in backends. This field is optional.</p>
<h3><a class="header" href="#field-body" id="field-body">Field: <code>Body</code></a></h3>
<p>Type: any.</p>
<p>Description: A value containing the body of the log record (see the description
of <code>any</code> type above). Can be for example a human-readable string message
(including multi-line) describing the event in a free form or it can be a
structured data composed of arrays and maps of other values. Can vary for each
occurrence of the event coming from the same source. This field is optional.</p>
<h3><a class="header" href="#field-resource" id="field-resource">Field: <code>Resource</code></a></h3>
<p>Type: key/value pair list.</p>
<p>Description: Describes the source of the log, aka
<a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/overview.md#resources">resource</a>.
&quot;key&quot; of each pair is a <code>string</code> and &quot;value&quot; is of <code>any</code> type. Multiple
occurrences of events coming from the same event source can happen across time
and they all have the same value of <code>Resource</code>. Can contain for example
information about the application that emits the record or about the
infrastructure where the application runs. Data formats that represent this data
model may be designed in a manner that allows the <code>Resource</code> field to be
recorded only once per batch of log records that come from the same source.
SHOULD follow OpenTelemetry
<a href="https://github.com/open-telemetry/opentelemetry-specification/tree/master/specification/resource/semantic_conventions">semantic conventions for Resources</a>.
This field is optional.</p>
<h3><a class="header" href="#field-attributes" id="field-attributes">Field: <code>Attributes</code></a></h3>
<p>Type: key/value pair list.</p>
<p>Description: Additional information about the specific event occurrence. &quot;key&quot;
of each pair is a <code>string</code> and &quot;value&quot; is of <code>any</code> type. Unlike the <code>Resource</code>
field, which is fixed for a particular source, <code>Attributes</code> can vary for each
occurrence of the event coming from the same source. Can contain information
about the request context (other than TraceId/SpanId). SHOULD follow
OpenTelemetry
<a href="https://github.com/open-telemetry/opentelemetry-specification/tree/master/specification/trace/semantic_conventions">semantic conventions for Attributes</a>.
This field is optional.</p>
<h2><a class="header" href="#example-log-records" id="example-log-records">Example Log Records</a></h2>
<p>Below are examples that show one possible representation of log records in JSON.
These are just examples to help understand the data model. Don’t treat the
examples as <em>the</em> way to represent this data model in JSON.</p>
<p>This document does not define the actual encoding and format of the log record
representation. Format definitions will be done in separate OTEPs (e.g. the log
records may be represented as msgpack, JSON, Protocol Buffer messages, etc).</p>
<p>Example 1</p>
<pre><code class="language-javascript">{
  &quot;Timestamp&quot;: 1586960586000, // JSON needs to make a decision about
                              // how to represent nanoseconds.
  &quot;Attributes&quot;: {
    &quot;http.status_code&quot;: 500,
    &quot;http.url&quot;: &quot;http://example.com&quot;,
    &quot;my.custom.application.tag&quot;: &quot;hello&quot;,
  },
  &quot;Resource&quot;: {
    &quot;service.name&quot;: &quot;donut_shop&quot;,
    &quot;service.version&quot;: &quot;semver:2.0.0&quot;,
    &quot;k8s.pod.uid&quot;: &quot;1138528c-c36e-11e9-a1a7-42010a800198&quot;,
  },
  &quot;TraceId&quot;: &quot;f4dbb3edd765f620&quot;, // this is a byte sequence
                                 // (hex-encoded in JSON)
  &quot;SpanId&quot;: &quot;43222c2d51a7abe3&quot;,
  &quot;SeverityText&quot;: &quot;INFO&quot;,
  &quot;SeverityNumber&quot;: 9,
  &quot;Body&quot;: &quot;20200415T072306-0700 INFO I like donuts&quot;
}
</code></pre>
<p>Example 2</p>
<pre><code class="language-javascript">{
  &quot;Timestamp&quot;: 1586960586000,
  ...
  &quot;Body&quot;: {
    &quot;i&quot;: &quot;am&quot;,
    &quot;an&quot;: &quot;event&quot;,
    &quot;of&quot;: {
      &quot;some&quot;: &quot;complexity&quot;
    }
  }
}
</code></pre>
<p>Example 3</p>
<pre><code class="language-javascript">{
   &quot;Timestamp&quot;: 1586960586000,
   &quot;Attributes&quot;:{
      &quot;http.scheme&quot;:&quot;https&quot;,
      &quot;http.host&quot;:&quot;donut.mycie.com&quot;,
      &quot;http.target&quot;:&quot;/order&quot;,
      &quot;http.method&quot;:&quot;post&quot;,
      &quot;http.status_code&quot;:500,
      &quot;http.flavor&quot;:&quot;1.1&quot;,
      &quot;http.user_agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36&quot;,
   }
}
</code></pre>
<h2><a class="header" href="#questions-resolved-during-otep-discussion" id="questions-resolved-during-otep-discussion">Questions Resolved during OTEP discussion</a></h2>
<p>These were Open Questions that were discussed and resolved in
<a href="https://github.com/open-telemetry/oteps/pull/97">OTEP Pull Request</a></p>
<h3><a class="header" href="#traceflags-vs-traceparent-and-tracestate" id="traceflags-vs-traceparent-and-tracestate">TraceFlags vs TraceParent and TraceState</a></h3>
<p>Question: Should we store entire
<a href="https://www.w3.org/TR/trace-context/">W3C Trace Context</a>, including
<code>traceparent</code> and <code>tracestate</code> fields instead of only <code>TraceFlags</code>?</p>
<p>Answer: the discussion did not reveal any evidence that <code>traceparent</code> and
<code>tracestate</code> are needed.</p>
<h3><a class="header" href="#severity-fields-1" id="severity-fields-1">Severity Fields</a></h3>
<p>Question: Is <code>SeverityText</code>/<code>SeverityNumber</code> fields design good enough?</p>
<p>Answer: Discussions have shown that the design is reasonable.</p>
<h3><a class="header" href="#timestamp-requirements" id="timestamp-requirements">Timestamp Requirements</a></h3>
<p>Question: Early draft of this proposal specified that <code>Timestamp</code> should be
populated from a monotonic, NTP-synchronized source. We removed this requirement
to avoid confusion. Do we need any requirements for timestamp sources?</p>
<p>Answer: discussions revealed that it is not data model's responsibility to
specify such requirements.</p>
<h3><a class="header" href="#security-logs" id="security-logs">Security Logs</a></h3>
<p>Question: Is there a need for special treatment of security logs?</p>
<p>Answer: discussions in the OTEP did not reveal the need for any special
treatment of security logs in the context of the data model proposal.</p>
<h2><a class="header" href="#alternate-design" id="alternate-design">Alternate Design</a></h2>
<p>An
<a href="https://docs.google.com/document/d/1ix9_4TQO3o-qyeyNhcOmqAc1MTyr-wnXxxsdWgCMn9c/edit?ts=5e990fe2#heading=h.cw69q2ga62p6">alternate design</a>
that used an envelop approach was considered but I did not find it to be overall
better than this one.</p>
<h2><a class="header" href="#prior-art" id="prior-art">Prior Art</a></h2>
<h3><a class="header" href="#rfc5424-syslog" id="rfc5424-syslog">RFC5424 Syslog</a></h3>
<p><a href="https://tools.ietf.org/html/rfc5424">RFC5424</a> defines structured log data
format and protocol. The protocol is ubiquitous (although unfortunately many
implementations don’t follow structured data recommendations). Here are some
drawbacks that do not make Syslog a serious contender for a data model:</p>
<ul>
<li>
<p>While it allows structured attributes the body of the message can be only a
string.</p>
</li>
<li>
<p>Severity is hard-coded to 8 possible numeric values, and does not allow custom
severity texts.</p>
</li>
<li>
<p>Structured data does not allow arbitrary nesting and is 2-level only.</p>
</li>
<li>
<p>No clear separate place to specify data source (aka resource). There are a
couple hard-coded fields that serve this purpose in a limited way (HOSTNAME,
APP-NAME, FACILITY).</p>
</li>
</ul>
<h3><a class="header" href="#fluentd-forward-protocol-model" id="fluentd-forward-protocol-model">Fluentd Forward Protocol Model</a></h3>
<p><a href="https://github.com/fluent/fluentd/wiki/Forward-Protocol-Specification-v1">Forward protocol</a>
defines a log Entry concept as a timestamped record. The record consists of 2
elements: a tag and a map of arbitrary key/value pairs.</p>
<p>The model is universal enough to represent any log record. However, here are
some drawbacks:</p>
<ul>
<li>
<p>All attributes of a record are represented via generic key/value pairs (except
tag and timestamp). This misses the optimization opportunities (see <a href="0097-log-data-model.html#design-notes">Design
Notes</a>).</p>
</li>
<li>
<p>There is no clear separate place to specify data source (aka resource).</p>
</li>
<li>
<p>There is no mention of how exactly keys should be named and what are expected
values. This lack of any naming convention or standardization of key/value
pairs makes interoperability difficult.</p>
</li>
</ul>
<h2><a class="header" href="#appendix-a-example-mappings" id="appendix-a-example-mappings">Appendix A. Example Mappings</a></h2>
<p>This section contains examples of mapping of other events and logs formats to
this data model.</p>
<h3><a class="header" href="#rfc5424-syslog-1" id="rfc5424-syslog-1">RFC5424 Syslog</a></h3>
<table>
  <tr>
    <td>Property</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>TIMESTAMP</td>
    <td>Timestamp</td>
    <td>Time when an event occurred measured by the origin clock.</td>
    <td>Timestamp</td>
  </tr>
  <tr>
    <td>SEVERITY</td>
    <td>enum</td>
    <td>Defines the importance of the event. Example: `Debug`</td>
    <td>Severity</td>
  </tr>
  <tr>
    <td>FACILITY</td>
    <td>enum</td>
    <td>Describes where the event originated. A predefined list of Unix processes. Part of event source identity. Example: `mail system`</td>
    <td>Attributes["syslog.facility"]</td>
  </tr>
  <tr>
    <td>VERSION</td>
    <td>number</td>
    <td>Meta: protocol version, orthogonal to the event.</td>
    <td>Attributes["syslog.version"]</td>
  </tr>
  <tr>
    <td>HOSTNAME</td>
    <td>string</td>
    <td>Describes the location where the event originated. Possible values are FQDN, IP address, etc.</td>
    <td>Resource["host.hostname"]</td>
  </tr>
  <tr>
    <td>APP-NAME</td>
    <td>string</td>
    <td>User-defined app name. Part of event source identity.</td>
    <td>Resource["service.name"]</td>
  </tr>
  <tr>
    <td>PROCID</td>
    <td>string</td>
    <td>Not well defined. May be used as a meta field for protocol operation purposes or may be part of event source identity.</td>
    <td>Attributes["syslog.procid"]</td>
  </tr>
  <tr>
    <td>MSGID</td>
    <td>string</td>
    <td>Defines the type of the event. Part of event source identity. Example: "TCPIN"</td>
    <td>Name</td>
  </tr>
  <tr>
    <td>STRUCTURED-DATA</td>
    <td>array of maps of string to string</td>
    <td>A variety of use cases depending on the SDID:
Can describe event source identity
Can include data that describes particular occurrence of the event.
Can be meta-information, e.g. quality of timestamp value.</td>
    <td>SDID origin.swVersion map to Resource["service.version"]
<p>SDID origin.ip map to attribute[net.host.ip&quot;]</p>
<p>Rest of SDIDs -&gt; Attributes[&quot;syslog.*&quot;]</td></p>
</tr>
  <tr>
    <td>MSG</td>
    <td>string</td>
    <td>Free-form text message about the event. Typically human readable.</td>
    <td>Body</td>
  </tr>
</table>
<h3><a class="header" href="#windows-event-log" id="windows-event-log">Windows Event Log</a></h3>
<table>
  <tr>
    <td>Property</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>TimeCreated</td>
    <td>Timestamp</td>
    <td>The time stamp that identifies when the event was logged.</td>
    <td>Timestamp</td>
  </tr>
  <tr>
    <td>Level</td>
    <td>enum</td>
    <td>Contains the severity level of the event.</td>
    <td>Severity</td>
  </tr>
  <tr>
    <td>Computer</td>
    <td>string</td>
    <td>The name of the computer on which the event occurred.</td>
    <td>Resource["host.hostname"]</td>
  </tr>
  <tr>
    <td>EventID</td>
    <td>uint</td>
    <td>The identifier that the provider used to identify the event.</td>
    <td>Name</td>
  </tr>
  <tr>
    <td>Message</td>
    <td>string</td>
    <td>The message string.</td>
    <td>Body</td>
  </tr>
  <tr>
    <td>Rest of the fields.</td>
    <td>any</td>
    <td>All other fields in the event.</td>
    <td>Attributes["winlog.*"]</td>
  </tr>
</table>
<h3><a class="header" href="#signalfx-events" id="signalfx-events">SignalFx Events</a></h3>
<table>
  <tr>
    <td>Field</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>Timestamp</td>
    <td>Timestamp</td>
    <td>Time when the event occurred measured by the origin clock.</td>
    <td>Timestamp</td>
  </tr>
  <tr>
    <td>EventType</td>
    <td>string</td>
    <td>Short machine understandable string describing the event type. SignalFx specific concept. Non-namespaced. Example: k8s Event Reason field.</td>
    <td>Name</td>
  </tr>
  <tr>
    <td>Category</td>
    <td>enum</td>
    <td>Describes where the event originated and why. SignalFx specific concept. Example: AGENT. If this attribute is not present on the SignalFx Event, it should be set to the null attribute value in the LogRecord -- this will allow unambigous identification of SignalFx events when they are represented as LogRecords.</td>
    <td>Attributes["com.splunk.signalfx.event_category"]</td>
  </tr>
  <tr>
    <td>Dimensions</td>
    <td>map of string to string</td>
    <td>Helps to define the identity of the event source together with EventType and Category. Multiple occurrences of events coming from the same event source can happen across time and they all have the same value of Dimensions. In SignalFx, event Dimensions, along with the EventType, determine individual Event Time Series (ETS).</td>
    <td>Attributes</td>
  </tr>
  <tr>
    <td>Properties</td>
    <td>map of string to any</td>
    <td>Additional information about the specific event occurrence. Unlike Dimensions which are fixed for a particular event source, Properties can have different values for each occurrence of the event coming from the same event source. In SignalFx, event Properties are considered additional metadata about an event and do not factor into the identity of an Event Time Series (ETS).</td>
    <td>Attributes["com.splunk.signalfx.event_properties"]</td>
  </tr>
</table>
<h3><a class="header" href="#splunk-hec" id="splunk-hec">Splunk HEC</a></h3>
<table>
  <tr>
    <td>Field</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>time</td>
    <td>numeric, string</td>
    <td>The event time in epoch time format, in seconds.</td>
    <td>Timestamp</td>
  </tr>
  <tr>
    <td>host</td>
    <td>string</td>
    <td>The host value to assign to the event data. This is typically the host name of the client that you are sending data from.</td>
    <td>Resource["host.hostname"]</td>
  </tr>
  <tr>
    <td>source</td>
    <td>string</td>
    <td>The source value to assign to the event data. For example, if you are sending data from an app you are developing, you could set this key to the name of the app.</td>
    <td>Resource["service.name"]</td>
  </tr>
  <tr>
    <td>sourcetype</td>
    <td>string</td>
    <td>The sourcetype value to assign to the event data.</td>
    <td>Attributes["source.type"]</td>
  </tr>
  <tr>
    <td>event</td>
    <td>any</td>
    <td>The JSON representation of the raw body of the event. It can be a string, number, string array, number array, JSON object, or a JSON array.</td>
    <td>Body</td>
  </tr>
  <tr>
    <td>fields</td>
    <td>Map of any</td>
    <td>Specifies a JSON object that contains explicit custom fields.</td>
    <td>Attributes</td>
  </tr>
  <tr>
    <td>index</td>
    <td>string</td>
    <td>The name of the index by which the event data is to be indexed. The index you specify here must be within the list of allowed indexes if the token has the indexes parameter set.</td>
    <td>TBD, most like will go to attributes</td>
  </tr>
</table>
<h3><a class="header" href="#log4j" id="log4j">Log4j</a></h3>
<table>
  <tr>
    <td>Field</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>Instant</td>
    <td>Timestamp</td>
    <td>Time when an event occurred measured by the origin clock.</td>
    <td>Timestamp</td>
  </tr>
  <tr>
    <td>Level</td>
    <td>enum</td>
    <td>Log level.</td>
    <td>Severity</td>
  </tr>
  <tr>
    <td>Message</td>
    <td>string</td>
    <td>Human readable message.</td>
    <td>Body</td>
  </tr>
  <tr>
    <td>All other fields</td>
    <td>any</td>
    <td>Structured data.</td>
    <td>Attributes</td>
  </tr>
</table>
<h3><a class="header" href="#zap" id="zap">Zap</a></h3>
<table>
  <tr>
    <td>Field</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>ts</td>
    <td>Timestamp</td>
    <td>Time when an event occurred measured by the origin clock.</td>
    <td>Timestamp</td>
  </tr>
  <tr>
    <td>level</td>
    <td>enum</td>
    <td>Logging level.</td>
    <td>Severity</td>
  </tr>
  <tr>
    <td>caller</td>
    <td>string</td>
    <td>Calling function's filename and line number.
</td>
    <td>Attributes, key=TBD</td>
  </tr>
  <tr>
    <td>msg</td>
    <td>string</td>
    <td>Human readable message.</td>
    <td>Body</td>
  </tr>
  <tr>
    <td>All other fields</td>
    <td>any</td>
    <td>Structured data.</td>
    <td>Attributes</td>
  </tr>
</table>
<h3><a class="header" href="#apache-http-server-access-log" id="apache-http-server-access-log">Apache HTTP Server access log</a></h3>
<table>
  <tr>
    <td>Field</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>%t</td>
    <td>Timestamp</td>
    <td>Time when an event occurred measured by the origin clock.</td>
    <td>Timestamp</td>
  </tr>
  <tr>
    <td>%a</td>
    <td>string</td>
    <td>Client IP</td>
    <td>Attributes["net.peer.ip"]</td>
  </tr>
  <tr>
    <td>%A</td>
    <td>string</td>
    <td>Server IP</td>
    <td>Attributes["net.host.ip"]</td>
  </tr>
  <tr>
    <td>%h</td>
    <td>string</td>
    <td>Remote hostname. </td>
    <td>Attributes["net.peer.name"]</td>
  </tr>
  <tr>
    <td>%m</td>
    <td>string</td>
    <td>The request method.</td>
    <td>Attributes["http.method"]</td>
  </tr>
  <tr>
    <td>%v,%p,%U,%q</td>
    <td>string</td>
    <td>Multiple fields that can be composed into URL.</td>
    <td>Attributes["http.url"]</td>
  </tr>
  <tr>
    <td>%>s</td>
    <td>string</td>
    <td>Response status.</td>
    <td>Attributes["http.status_code"]</td>
  </tr>
  <tr>
    <td>All other fields</td>
    <td>any</td>
    <td>Structured data.</td>
    <td>Attributes, key=TBD</td>
  </tr>
</table>
<h3><a class="header" href="#cloudtrail-log-event" id="cloudtrail-log-event">CloudTrail Log Event</a></h3>
<table>
  <tr>
    <td>Field</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>eventTime</td>
    <td>string</td>
    <td>The date and time the request was made, in coordinated universal time (UTC).</td>
    <td>Timestamp</td>
  </tr>
  <tr>
    <td>eventSource</td>
    <td>string</td>
    <td>The service that the request was made to. This name is typically a short form of the service name without spaces plus .amazonaws.com.</td>
    <td>Resource["service.name"]?</td>
  </tr>
  <tr>
    <td>awsRegion</td>
    <td>string</td>
    <td>The AWS region that the request was made to, such as us-east-2.</td>
    <td>Resource["cloud.region"]</td>
  </tr>
  <tr>
    <td>sourceIPAddress</td>
    <td>string</td>
    <td>The IP address that the request was made from.</td>
    <td>Resource["net.peer.ip"] or Resource["net.host.ip"]? TBD</td>
  </tr>
  <tr>
    <td>errorCode</td>
    <td>string</td>
    <td>The AWS service error if the request returns an error.</td>
    <td>Name</td>
  </tr>
  <tr>
    <td>errorMessage</td>
    <td>string</td>
    <td>If the request returns an error, the description of the error.</td>
    <td>Body</td>
  </tr>
  <tr>
    <td>All other fields</td>
    <td>*</td>
    <td></td>
    <td>Attributes["cloudtrail.*"]</td>
  </tr>
</table>
<h3><a class="header" href="#google-cloud-logging" id="google-cloud-logging">Google Cloud Logging</a></h3>
<table><thead><tr><th>Field</th><th>Type</th><th>Description</th><th>Maps to Unified Model Field</th></tr></thead><tbody>
<tr><td>timestamp</td><td>string</td><td>The time the event described by the log entry occurred.</td><td>Timestamp</td></tr>
<tr><td>resource</td><td>MonitoredResource</td><td>The monitored resource that produced this log entry.</td><td>Resource</td></tr>
<tr><td>log_name</td><td>string</td><td>The URL-encoded LOG_ID suffix of the log_name field identifies which log stream this entry belongs to.</td><td>Name</td></tr>
<tr><td>json_payload</td><td>google.protobuf.Struct</td><td>The log entry payload, represented as a structure that is expressed as a JSON object.</td><td>Body</td></tr>
<tr><td>proto_payload</td><td>google.protobuf.Any</td><td>The log entry payload, represented as a protocol buffer.</td><td>Body</td></tr>
<tr><td>text_payload</td><td>string</td><td>The log entry payload, represented as a Unicode string (UTF-8).</td><td>Body</td></tr>
<tr><td>severity</td><td>LogSeverity</td><td>The severity of the log entry.</td><td>Severity</td></tr>
<tr><td>trace</td><td>string</td><td>The trace associated with the log entry, if any.</td><td>TraceId</td></tr>
<tr><td>span_id</td><td>string</td><td>The span ID within the trace associated with the log entry.</td><td>SpanId</td></tr>
<tr><td>labels</td><td>map&lt;string,string&gt;</td><td>A set of user-defined (key, value) data that provides additional information about the log entry.</td><td>Attributes</td></tr>
<tr><td>All other fields</td><td></td><td></td><td>Attributes[&quot;google.*&quot;]</td></tr>
</tbody></table>
<h2><a class="header" href="#elastic-common-schema" id="elastic-common-schema">Elastic Common Schema</a></h2>
<table>
  <tr>
    <td>Field</td>
    <td>Type</td>
    <td>Description</td>
    <td>Maps to Unified Model Field</td>
  </tr>
  <tr>
    <td>@timestamp</td>
    <td>datetime</td>
    <td>Time the event was recorded</td>
    <td>timestamp</td>
  </tr>
  <tr>
    <td>message</td>
    <td>string</td>
    <td>Any type of message</td>
    <td>body</td>
  </tr>
  <tr>
    <td>labels</td>
    <td>key/value</td>
    <td>Arbitrary labels related to the event</td>
    <td>attributes[*]</td>
  </tr>
  <tr>
    <td>tags</td>
    <td>array of string</td>
    <td>List of values related to the event</td>
    <td>?</td>
  </tr>
  <tr>
    <td>trace.id</td>
    <td>string</td>
    <td>Trace ID</td>
    <td>TraceId</td>
  </tr>
  <tr>
    <td>span.id*</td>
    <td>string</td>
    <td>Span ID</td>
    <td>SpanId</td>
  </tr>
  <tr>
    <td>agent.ephemeral_id</td>
    <td>string</td>
    <td>Ephemeral ID created by agent</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>agent.id</td>
    <td>string</td>
    <td>Unique identifier of this agent</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>agent.name</td>
    <td>string</td>
    <td>Name given to the agent</td>
    <td>resource["telemetry.sdk.name"]</td>
  </tr>
  <tr>
    <td>agent.type</td>
    <td>string</td>
    <td>Type of agent</td>
    <td>resource["telemetry.sdk.language"]</td>
  </tr>
  <tr>
    <td>agent.version</td>
    <td>string</td>
    <td>Version of agent</td>
    <td>resource["telemetry.sdk.version"]</td>
  </tr>
  <tr>
    <td>source.ip, client.ip</td>
    <td>string</td>
    <td>The IP address that the request was made from.</td>
    <td>attributes["net.peer.ip"] or attributes["net.host.ip"]</td>
  </tr>
  <tr>
    <td>cloud.account.id</td>
    <td>string</td>
    <td>ID of the account in the given cloud</td>
    <td>resource["cloud.account.id"]</td>
  </tr>
  <tr>
    <td>cloud.availability_zone</td>
    <td>string</td>
    <td>Availability zone in which this host is running.</td>
    <td>resource["cloud.zone"]</td>
  </tr>
  <tr>
    <td>cloud.instance.id</td>
    <td>string</td>
    <td>Instance ID of the host machine.</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>cloud.instance.name</td>
    <td>string</td>
    <td>Instance name of the host machine.</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>cloud.machine.type</td>
    <td>string</td>
    <td>Machine type of the host machine.</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>cloud.provider</td>
    <td>string</td>
    <td>Name of the cloud provider. Example values are aws, azure, gcp, or digitalocean.</td>
    <td>resource["cloud.provider"]</td>
  </tr>
  <tr>
    <td>cloud.region</td>
    <td>string</td>
    <td>Region in which this host is running.</td>
    <td>resource["cloud.region"]</td>
  </tr>
  <tr>
    <td>cloud.image.id*</td>
    <td>string</td>
    <td></td>
    <td>resource["host.image.name"]</td>
  </tr>
  <tr>
    <td>container.id</td>
    <td>string</td>
    <td>Unique container id</td>
    <td>resource["container.id"]</td>
  </tr>
  <tr>
    <td>container.image.name</td>
    <td>string</td>
    <td>Name of the image the container was built on.</td>
    <td>resource["container.image.name"]</td>
  </tr>
  <tr>
    <td>container.image.tag</td>
    <td>Array of string</td>
    <td>Container image tags.</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>container.labels</td>
    <td>key/value</td>
    <td>Image labels.</td>
    <td>attributes[*]</td>
  </tr>
  <tr>
    <td>container.name</td>
    <td>string</td>
    <td>Container name.</td>
    <td>resource["container.name"]</td>
  </tr>
  <tr>
    <td>container.runtime</td>
    <td>string</td>
    <td>Runtime managing this container. Example: "docker"</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>destination.address</td>
    <td>string</td>
    <td>Destination address for the event</td>
    <td>attributes["destination.address"]</td>
  </tr>
  <tr>
    <td>error.code</td>
    <td>string</td>
    <td>Error code describing the error.</td>
    <td>attributes["error.code"]</td>
  </tr>
  <tr>
    <td>error.id</td>
    <td>string</td>
    <td>Unique identifier for the error.</td>
    <td>attributes["error.id"]</td>
  </tr>
  <tr>
    <td>error.message</td>
    <td>string</td>
    <td>Error message.</td>
    <td>attributes["error.message"]</td>
  </tr>
  <tr>
    <td>error.stack_trace</td>
    <td>string</td>
    <td>The stack trace of this error in plain text.</td>
    <td>attributes["error.stack_trace]</td>
  </tr>
  <tr>
    <td>host.architecture</td>
    <td>string</td>
    <td>Operating system architecture</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>host.domain</td>
    <td>string</td>
    <td>Name of the domain of which the host is a member.
<p>For example, on Windows this could be the host’s Active Directory domain or NetBIOS domain name. For Linux this could be the domain of the host’s LDAP provider.</td></p>
<td>**resource</td>
  </tr>
  <tr>
    <td>host.hostname</td>
    <td>string</td>
    <td>Hostname of the host.
<p>It normally contains what the hostname command returns on the host machine.</td></p>
<td>resource["host.hostname"]</td>
</tr>
  <tr>
    <td>host.id</td>
    <td>string</td>
    <td>Unique host id.</td>
    <td>resource["host.id"]</td>
  </tr>
  <tr>
    <td>host.ip</td>
    <td>Array of string</td>
    <td>Host IP</td>
    <td>resource["host.ip"]</td>
  </tr>
  <tr>
    <td>host.mac</td>
    <td>array of string</td>
    <td>MAC addresses of the host</td>
    <td>resource["host.mac"]</td>
  </tr>
  <tr>
    <td>host.name</td>
    <td>string</td>
    <td>Name of the host.
<p>It may contain what hostname returns on Unix systems, the fully qualified, or a name specified by the user. </td></p>
<td>resource["host.name"]</td>
</tr>
  <tr>
    <td>host.type</td>
    <td>string</td>
    <td>Type of host.</td>
    <td>resource["host.type"]</td>
  </tr>
  <tr>
    <td>host.uptime</td>
    <td>string</td>
    <td>Seconds the host has been up.</td>
    <td>?</td>
  </tr>
  <tr>
    <td>service.ephemeral_id
</td>
    <td>string</td>
    <td>Ephemeral identifier of this service</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>service.id</td>
    <td>string</td>
    <td>Unique identifier of the running service. If the service is comprised of many nodes, the service.id should be the same for all nodes.</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>service.name</td>
    <td>string</td>
    <td>Name of the service data is collected from.</td>
    <td>resource["service.name"]</td>
  </tr>
  <tr>
    <td>service.node.name</td>
    <td>string</td>
    <td>Specific node serving that service</td>
    <td>resource["service.instance.id"]</td>
  </tr>
  <tr>
    <td>service.state</td>
    <td>string</td>
    <td>Current state of the service.</td>
    <td>attributes["service.state"]</td>
  </tr>
  <tr>
    <td>service.type</td>
    <td>string</td>
    <td>The type of the service data is collected from.</td>
    <td>**resource</td>
  </tr>
  <tr>
    <td>service.version</td>
    <td>string</td>
    <td>Version of the service the data was collected from.</td>
    <td>resource["service.version"]</td>
  </tr>
  <tr>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</table>
<p>* Not yet formalized into ECS.</p>
<p>** A resource that doesn’t exist in the
<a href="https://github.com/open-telemetry/opentelemetry-specification/tree/master/specification/resource/semantic_conventions">OpenTelemetry resource semantic convention</a>.</p>
<p>This is a selection of the most relevant fields. See
<a href="https://www.elastic.co/guide/en/ecs/current/ecs-field-reference.html">for the full reference</a>
for an exhaustive list.</p>
<h2><a class="header" href="#appendix-b-severitynumber-example-mappings" id="appendix-b-severitynumber-example-mappings">Appendix B: <code>SeverityNumber</code> example mappings</a></h2>
<table><thead><tr><th>Syslog</th><th>WinEvtLog</th><th>Log4j</th><th>Zap</th><th>java.util.logging</th><th>SeverityNumber</th></tr></thead><tbody>
<tr><td></td><td></td><td>TRACE</td><td></td><td>FINEST</td><td>TRACE</td></tr>
<tr><td>Debug</td><td>Verbose</td><td>DEBUG</td><td>Debug</td><td>FINER</td><td>DEBUG</td></tr>
<tr><td></td><td></td><td></td><td></td><td>FINE</td><td>DEBUG2</td></tr>
<tr><td></td><td></td><td></td><td></td><td>CONFIG</td><td>DEBUG3</td></tr>
<tr><td>Informational</td><td>Information</td><td>INFO</td><td>Info</td><td>INFO</td><td>INFO</td></tr>
<tr><td>Notice</td><td></td><td></td><td></td><td></td><td>INFO2</td></tr>
<tr><td>Warning</td><td>Warning</td><td>WARN</td><td>Warn</td><td>WARNING</td><td>WARN</td></tr>
<tr><td>Error</td><td>Error</td><td>ERROR</td><td>Error</td><td>SEVERE</td><td>ERROR</td></tr>
<tr><td>Critical</td><td>Critical</td><td></td><td>Dpanic</td><td></td><td>ERROR2</td></tr>
<tr><td>Emergency</td><td></td><td></td><td>Panic</td><td></td><td>ERROR3</td></tr>
<tr><td>Alert</td><td></td><td>FATAL</td><td>Fatal</td><td></td><td>FATAL</td></tr>
</tbody></table>
<h2><a class="header" href="#references" id="references">References</a></h2>
<ul>
<li>
<p><a href="https://docs.google.com/document/d/1ix9_4TQO3o-qyeyNhcOmqAc1MTyr-wnXxxsdWgCMn9c/edit#">Draft discussion of Data Model</a></p>
</li>
<li>
<p><a href="https://docs.google.com/document/d/1WQDz1jF0yKBXe3OibXWfy3g6lor9SvjZ4xT-8uuDCiA/edit#">Discussion of Severity field</a></p>
</li>
</ul>
<h1><a class="header" href="#explain-the-metric-instruments" id="explain-the-metric-instruments">Explain the metric instruments</a></h1>
<p>Propose and explain final names for the standard metric instruments theorized in <a href="./0088-metric-instrument-optional-refinements.html">OTEP 88</a> and address related confusion.</p>
<h2><a class="header" href="#motivation-23" id="motivation-23">Motivation</a></h2>
<p><a href="./0088-metric-instrument-optional-refinements.html">OTEP 88</a> introduced a logical structure for metric instruments with two foundational categories of instrument, called &quot;synchronous&quot; vs. &quot;asynchronous&quot;, named &quot;Measure&quot; and &quot;Observer&quot; in the abstract sense.  The proposal identified four kinds of &quot;refinement&quot; and mapped out the space of <em>possible</em> instruments, while not proposing which would actually be included in the standard.</p>
<p><a href="https://github.com/open-telemetry/oteps/pull/93">OTEP 93</a> proposed with a list of six standard instruments, the most necessary and useful combination of instrument refinements, plus one special case used to record timing measurements.  OTEP 93 was closed without merging after a more consistent approach to naming was uncovered.  <a href="https://github.com/open-telemetry/oteps/pull/96">OTEP 96</a> made another proposal, that was closed in favor of this one after more debate surfaced.</p>
<p>This proposal finalizes the naming proposal for the standard instruments, seeking to address core confusion related to the &quot;Measure&quot; and &quot;Observer&quot; terms:</p>
<ol>
<li><a href="./0088-metric-instrument-optional-refinements.html">OTEP 88</a> stipulates that the terms currently in use to name synchronous and asynchronous instruments--&quot;Measure&quot; and &quot;Observer&quot;--become <em>abstract</em> terms.  It also used phrases like &quot;Measure-like&quot; and &quot;Observer-like&quot; to discuss instruments with refinements.  This proposal states that we shall prefer the adjectives, commonly abbreviated &quot;Sync&quot; and &quot;Async&quot;, when describing the kind of an instrument.  &quot;Measure-like&quot; means an instrument is synchronous.  &quot;Observer-like&quot; means that an instrument is asynchronous.</li>
<li>There is inconsistency in the hypothetical naming scheme for instruments presented in <a href="./0088-metric-instrument-optional-refinements.html">OTEP 88</a>.  Note that &quot;Counter&quot; and &quot;Observer&quot; end in &quot;-er&quot;, a noun suffix used in the sense of &quot;<a href="https://www.merriam-webster.com/dictionary/-er">person occupationally connected with</a>&quot;, while the term &quot;Measure&quot; does not fit this pattern.  This proposal proposes to replace the abstract term &quot;Measure&quot; by &quot;Recorder&quot;, since the associated function name (verb) is specified as <code>Record()</code>.</li>
</ol>
<p>This proposal also repeats the current specification--and the justification--for the default aggregation of each standard instrument.</p>
<h2><a class="header" href="#explanation-16" id="explanation-16">Explanation</a></h2>
<p>The following table summarizes the final proposed standard instruments resulting from this set of proposals.  The columns are described in more detail below.</p>
<table><thead><tr><th>Existing name</th><th><strong>Standard name</strong></th><th>Instrument kind</th><th>Function name</th><th>Input temporal quality</th><th>Default aggregation</th><th>Rate support (Monotonic)</th><th>Notes</th></tr></thead><tbody>
<tr><td>Counter</td><td><strong>Counter</strong></td><td>Sync</td><td>Add()</td><td>Delta</td><td>Sum</td><td>Yes</td><td>Per-request, part of a monotonic sum</td></tr>
<tr><td></td><td><strong>UpDownCounter</strong></td><td>Sync</td><td>Add()</td><td>Delta</td><td>Sum</td><td>No</td><td>Per-request, part of a non-monotonic sum</td></tr>
<tr><td>Measure</td><td><strong>ValueRecorder</strong></td><td>Sync</td><td>Record()</td><td>Instantaneous</td><td>MinMaxSumCount</td><td>No</td><td>Per-request, any non-additive measurement</td></tr>
<tr><td></td><td><strong>SumObserver</strong></td><td>Async</td><td>Observe()</td><td>Cumulative</td><td>Sum</td><td>Yes</td><td>Per-interval, reporting a monotonic sum</td></tr>
<tr><td></td><td><strong>UpDownSumObserver</strong></td><td>Async</td><td>Observe()</td><td>Cumulative</td><td>Sum</td><td>No</td><td>Per-interval, reporting a non-monotonic sum</td></tr>
<tr><td>Observer</td><td><strong>ValueObserver</strong></td><td>Async</td><td>Observe()</td><td>Instantaneous</td><td>MinMaxSumCount</td><td>No</td><td>Per-interval, any non-additive measurement</td></tr>
</tbody></table>
<p>There are three synchronous instruments and three asynchronous instruments in this proposal, although a hypothetical 10 instruments were discussed in <a href="./0088-metric-instrument-optional-refinements.html">OTEP 88</a>.  Although we consider them rational and logical, two categories of instrument are excluded in this proposal: synchronous cumulative instruments and asynchronous delta instruments.</p>
<p>Synchronous cumulative instruments are excluded from the standard based on the <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/performance.md">OpenTelemetry library performance guidelines</a>.  To report a cumulative value correctly at runtime requires a degree of order dependence--thus synchronization--that OpenTelemetry API will not itself admit.  In a hypothetical example, if two actors both synchronously modify a sum and were to capture it using a synchronous cumulative metric event, the OpenTelemetry library would have to guarantee those measurements were processed in order.  The library guidelines do not support this level of synchronization; we cannot block for the sake of instrumentation, therefore we do not support synchronous cumulative instruments.</p>
<p>Asynchronous delta instruments are excluded from the standard based on the lack of motivating examples, but we could also justify this as a desire to keep asynchronous callbacks stateless. An observer has to have memory in order to compute deltas; it is simpler for asynchronous code to report cumulative values.</p>
<p>With six instruments in total, one may be curious--how does the historical Metrics API term <em>Gauge</em> translate into this specification?  <em>Gauge</em>, in Metrics API terminology, may cover all of these instrument use-cases with the exception of <code>Counter</code>.  As defined in <a href="./0088-metric-instrument-optional-refinements.html">OTEP 88</a>, the OpenTelemetry Metrics API will disambiguate these use-cases by requiring <em>single purpose instruments</em>.  The choice of instrument implies a default interpretation, a standard aggregation, and suggests how to treat Metric data in observability systems, out of the box.  Uses of <code>Gauge</code> translate into the various OpenTelemetry Metric instruments depending on what kind of values is being captured and whether the measurement is made synchronously or not.</p>
<p>Summarizing the naming scheme:</p>
<ul>
<li>If you've measured an amount of something that adds up to a total, where you are mainly interested in that total, use one of the additive instruments:
<ul>
<li>If synchronous and monotonic, use <code>Counter</code> with non-negative values</li>
<li>If synchronous and not monotonic, use <code>UpDownCounter</code> with arbitrary values</li>
<li>If asynchronous and a cumulative, monotonic sum is measured, use <code>SumObserver</code></li>
<li>If asynchronous and a cumulative, arbitrary sum is measured, use <code>UpDownSumObserver</code></li>
</ul>
</li>
<li>If the measurements are non-additive or additive with an interest in the distribution, use an instantaneous instrument:
<ul>
<li>If synchronous, use <code>ValueRecorder</code> to record a value that is part of a distribution</li>
<li>If asynchronous use <code>ValueObserver</code> to record a single measurement nearing the end of a collection interval.</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#sync-vs-async-instruments" id="sync-vs-async-instruments">Sync vs Async instruments</a></h3>
<p>Synchronous instruments are called in a request context, meaning they potentially have an associated tracing context and distributed correlation values.  Multiple metric events may occur for a synchronous instrument within a given collection interval.  Note that synchronous instruments may be called outside of a request context, such as for background computation.  In these scenarios, we may simply consider the Context to be empty.</p>
<p>Asynchronous instruments are reported by a callback, once per collection interval, and lack request context.  They are permitted to report only one value per distinct label set per period.  If the application observes multiple values in a single callback, for one collection interval, the last value &quot;wins&quot;.</p>
<h3><a class="header" href="#temporal-quality" id="temporal-quality">Temporal quality</a></h3>
<p>Measurements can be described in terms of their relationship with time.  Note: although this term logically applies and is used throughout this OTEP, discussion in the Metrics SIG meeting (4/30/2020) leads us to exclude this term from use in documenting the Metric API.  The explanation of terms here is consistent with the [terminology used in the protocol], but we will prefer to use these adjectives to describe properties of an aggregation, not properties of an instrument (despite this document continuing to use the terms freely).  In the API specification, this distinction will be described using &quot;additive synchronous&quot; in contrast with &quot;additive asynchronous&quot;.</p>
<p>Delta measurements are those that measure a change to a sum.  Delta instruments are usually selected because the program does not need to compute the sum for itself, but is able to measure the change.  In these cases, it would require extra state for the user to report cumulative values and reporting deltas is natural.</p>
<p>Cumulative measurements are those that report the current value of a sum.  Cumulative instruments are usually selected because the program maintains a sum for its own purposes, or because changes in the sum are not instrumented.  In these cases, it would require extra state for the user to report delta values and reporting cumulative values is natural.</p>
<p>Instantaneous measurements are those that report a non-additive measurement, one where it is not natural to compute a sum.  Instantaneous instruments are usually chosen when the distribution of values is of interest, not only the sum.</p>
<p>The terms &quot;Delta&quot;, &quot;Cumulative&quot;, and &quot;Instantaneous&quot; as used in this proposal refer to measurement values passed to the Metric API.  The argument to an (additive) instrument with the Delta temporal quality is the change in a sum.  The argument to an (additive) instrument with the Cumulative temporal quality is itself a sum.  The argument to an instrument with the Instantaneous temporal quality is simply a value.  In the SDK specification, as measurements are aggregated and transformed for export, these terms will be used again, with the same meanings, to describe aggregates.</p>
<h3><a class="header" href="#function-names" id="function-names">Function names</a></h3>
<p>Synchronous delta instruments support an <code>Add()</code> function, signifying that they add to a sum and are not cumulative.</p>
<p>Synchronous instantaneous instruments support a <code>Record()</code> function, signifying that they capture individual events, not only a sum.</p>
<p>Asynchronous instruments all support an <code>Observe()</code> function, signifying that they capture only one value per measurement interval.</p>
<h3><a class="header" href="#rate-support" id="rate-support">Rate support</a></h3>
<p>Rate aggregation is supported for Counter and SumObserver instruments in the default implementation.</p>
<p>The <code>UpDown-</code> forms of additive instrument are not suitable for aggregating rates because the up- and down-changes in state may cancel each other.</p>
<p>Non-additive instruments can be used to derive a sum, meaning rate aggregation is possible when the values are non-negative. There is not a standard non-additive instrument with a non-negative refinement in the standard.</p>
<h3><a class="header" href="#default-aggregations" id="default-aggregations">Default Aggregations</a></h3>
<p>Additive instruments use <code>Sum</code> aggregation by default, since by definition they are used when only the sum is of interest.</p>
<p>Instantaneous instruments use <code>MinMaxSumCount</code> aggregation by default, which is an inexpensive way to summarize a distribution of values.</p>
<h2><a class="header" href="#detail" id="detail">Detail</a></h2>
<p>Here we discuss the six proposed instruments individually and mention other names considered for each.</p>
<h3><a class="header" href="#counter" id="counter">Counter</a></h3>
<p><code>Counter</code> is the most common synchronous instrument.  This instrument supports an <code>Add(delta)</code> function for reporting a sum, and is restricted to non-negative deltas.  The default aggregation is <code>Sum</code>, as for any additive instrument, which are those instruments with Delta or Cumulative measurement kind.</p>
<p>Example uses for <code>Counter</code>:</p>
<ul>
<li>count the number of bytes received</li>
<li>count the number of accounts created</li>
<li>count the number of checkpoints run</li>
<li>count a number of 5xx errors.</li>
</ul>
<p>These example instruments would be useful for monitoring the rate of any of these quantities.  In these situations, it is usually more convenient to report a change of the associated sums, as the change happens, as opposed to maintaining and reporting the sum.</p>
<p>Other names considered: <code>Adder</code>, <code>SumCounter</code>.</p>
<h3><a class="header" href="#updowncounter" id="updowncounter">UpDownCounter</a></h3>
<p><code>UpDownCounter</code> is similar to <code>Counter</code> except that <code>Add(delta)</code> supports negative deltas.  This makes <code>UpDownCounter</code> not useful for computing a rate aggregation.  It aggregates a <code>Sum</code>, only the sum is non-monotonic.  It is generally useful for counting changes in an amount of resources used, or any quantity that rises and falls, in a request context.</p>
<p>Example uses for <code>UpDownCounter</code>:</p>
<ul>
<li>count memory in use by instrumenting <code>new</code> and <code>delete</code></li>
<li>count queue size by instrumenting <code>enqueue</code> and <code>dequeue</code></li>
<li>count semaphore <code>up</code> and <code>down</code> operations.</li>
</ul>
<p>These example instruments would be useful for monitoring resource levels across a group of processes.</p>
<p>Other names considered: <code>NonMonotonicCounter</code>.</p>
<h3><a class="header" href="#valuerecorder" id="valuerecorder">ValueRecorder</a></h3>
<p><code>ValueRecorder</code> is a non-additive synchronous instrument useful for recording any non-additive number, positive or negative.  Values captured by a <code>ValueRecorder</code> are treated as individual events belonging to a distribution that is being summarized.  <code>ValueRecorder</code> should be chosen either when capturing measurements that do not contribute meaningfully to a sum, or when capturing numbers that are additive in nature, but where the distribution of individual increments is considered interesting.</p>
<p>One of the most common uses for <code>ValueRecorder</code> is to capture latency measurements.  Latency measurements are not additive in the sense that there is little need to know the latency-sum of all processed requests.  We use a <code>ValueRecorder</code> instrument to capture latency measurements typically because we are interested in knowing mean, median, and other summary statistics about individual events.</p>
<p>The default aggregation for <code>ValueRecorder</code> computes the minimum and maximum values, the sum of event values, and the count of events, allowing the rate, the mean, and and range of input values to be monitored.</p>
<p>Example uses for <code>ValueRecorder</code> that are non-additive:</p>
<ul>
<li>capture any kind of timing information</li>
<li>capture the acceleration experienced by a pilot</li>
<li>capture nozzle pressure of a fuel injector</li>
<li>capture the velocity of a MIDI key-press.</li>
</ul>
<p>Example <em>additive</em> uses of <code>ValueRecorder</code> capture measurements that are cumulative or delta values, but where we may have an interest in the distribution of values and not only the sum:</p>
<ul>
<li>capture a request size</li>
<li>capture an account balance</li>
<li>capture a queue length</li>
<li>capture a number of board feet of lumber.</li>
</ul>
<p>These examples show that although they are additive in nature, choosing <code>ValueRecorder</code> as opposed to <code>Counter</code> or <code>UpDownCounter</code> implies an interest in more than the sum.  If you did not care to collect information about the distribution, you would have chosen one of the additive instruments instead.  Using <code>ValueRecorder</code> makes sense for distributions that are likely to be important in an observability setting.</p>
<p>Use these with caution because they naturally cost more than the use of additive measurements.</p>
<p>Other names considered: <code>Distribution</code>, <code>Measure</code>, <code>LastValueRecorder</code>, <code>GaugeRecorder</code>, <code>DistributionRecorder</code>.</p>
<h3><a class="header" href="#sumobserver" id="sumobserver">SumObserver</a></h3>
<p><code>SumObserver</code> is the asynchronous instrument corresponding to <code>Counter</code>, used to capture a monotonic count.  &quot;Sum&quot; appears in the name to remind users that it is a cumulative instrument.  Use a <code>SumObserver</code> to capture any value that starts at zero and rises throughout the process lifetime but never falls.</p>
<p>Example uses for <code>SumObserver</code>.</p>
<ul>
<li>capture process user/system CPU seconds</li>
<li>capture the number of cache misses.</li>
</ul>
<p>A <code>SumObserver</code> is a good choice in situations where a measurement is expensive to compute, such that it would be wasteful to compute on every request.  For example, a system call is needed to capture process CPU usage, therefore it should be done periodically, not on each request.  A <code>SumObserver</code> is also a good choice in situations where it would be impractical or wasteful to instrument individual deltas that comprise a sum.  For example, even though the number of cache misses is a sum of individual cache-miss events, it would be too expensive to synchronously capture each event using a <code>Counter</code>.</p>
<p>Other names considered: <code>CumulativeObserver</code>.</p>
<h3><a class="header" href="#updownsumobserver" id="updownsumobserver">UpDownSumObserver</a></h3>
<p><code>UpDownSumObserver</code> is the asynchronous instrument corresponding to <code>UpDownCounter</code>, used to capture a non-monotonic count.  &quot;Sum&quot; appears in the name to remind users that it is a cumulative instrument.  Use a <code>UpDownSumObserver</code> to capture any value that starts at zero and rises or falls throughout the process lifetime.</p>
<p>Example uses for <code>UpDownSumObserver</code>.</p>
<ul>
<li>capture process heap size</li>
<li>capture number of active shards</li>
<li>capture number of requests started/completed</li>
<li>capture current queue size.</li>
</ul>
<p>The same considerations mentioned for choosing <code>SumObserver</code> over the synchronous <code>Counter</code> apply for choosing <code>UpDownSumObserver</code> over the synchronous <code>UpDownCounter</code>.  If a measurement is expensive to compute, or if the corresponding delta events happen so frequently that it would be impractical to instrument them, use a <code>UpDownSumObserver</code>.</p>
<p>Other names considered: <code>UpDownCumulativeObserver</code>.</p>
<h3><a class="header" href="#valueobserver" id="valueobserver">ValueObserver</a></h3>
<p><code>ValueObserver</code> is the asynchronous instrument corresponding to <code>ValueRecorder</code>, used to capture non-additive measurements that are expensive to compute and/or are not request-oriented.</p>
<p>Example uses for <code>ValueObserver</code>:</p>
<ul>
<li>capture CPU fan speed</li>
<li>capture CPU temperature.</li>
</ul>
<p>Note that these examples use non-additive measurements.  In the <code>ValueRecorder</code> case above, example uses were given for capturing synchronous cumulative measurements in a request context (e.g., current queue size seen by a request).  In the asynchronous case, however, how should users decide whether to use <code>ValueObserver</code> as opposed to <code>UpDownSumObserver</code>?</p>
<p>Consider how to report the (cumulative) size of a queue asynchronously.  Both <code>ValueObserver</code> and <code>UpDownSumObserver</code> logically apply in this case.  Asynchronous instruments capture only one measurement per interval, so in this example the <code>SumObserver</code> reports a current sum, while the <code>ValueObserver</code> reports a current sum (equal to the max and the min) and a count equal to 1.  When there is no aggregation, these results are equivalent.</p>
<p>The recommendation is to choose the instrument with the more-appropriate default aggregation.  If you are observing a queue size across a group of machines and the only thing you want to know is the aggregate queue size, use <code>SumObserver</code>.  If you are observing a queue size across a group of machines and you are interested in knowing the distribution of queue sizes across those machines, use <code>ValueObserver</code>.</p>
<p>Other names considered: <code>GaugeObserver</code>, <code>LastValueObserver</code>, <code>DistributionObserver</code>.</p>
<h2><a class="header" href="#details-qa" id="details-qa">Details Q&amp;A</a></h2>
<h3><a class="header" href="#why-minmaxsumcount-for-valuerecorder-valueobserver" id="why-minmaxsumcount-for-valuerecorder-valueobserver">Why MinMaxSumCount for <code>ValueRecorder</code>, <code>ValueObserver</code>?</a></h3>
<p>There has been a question about the choice of <code>MinMaxSumCount</code> for the two non-additive instruments. The use of four values in the default aggregation for these instruments means that four values will be exported for these two instrument kinds.  The choice of Min, Max, Sum, and Count was intended to be an inexpensive default, but there is an even-more-minimal default aggregation we could choose.  The question was: Should &quot;SumCount&quot; be the default aggregation for these instruments?  The use of &quot;SumCount&quot; implies the ability to monitor the rate and the average, but not the range of values.</p>
<p>This proposal continues to specify the use of MinMaxSumCount for these two instruments.  Our belief is that in cases where performance and cost are concerns, usually the is an additive instruments that can be applied to lower cost.  In the case of <code>ValueObserver</code>, consider using a <code>SumObserver</code> or <code>UpDownSumObserver</code>.  In the case of <code>ValueRecorder</code>, consider configuring a less expensive view of these instruments than the default.</p>
<h3><a class="header" href="#valueobserver-temporal-quality-delta-or-instantaneous" id="valueobserver-temporal-quality-delta-or-instantaneous"><code>ValueObserver</code> temporal quality: Delta or Instantaneous?</a></h3>
<p>There has been a question about labeling <code>ValueObserver</code> measurements with the temporal quality Delta vs. Instantaneous.  There is a related question: What does it mean aggregate a Min and Max value for an asynchronous instrument, which may only produce one measurement per collection interval?</p>
<p>The purpose of defining the default aggregation, when there is only one measurement per interval, is to specify how values will be aggregated across multiple collection intervals.  When there is no aggregation being applied, the result of MinMaxSumCount aggregation for a single collection interval is a single measurement equal to the Min, the Max, and the Sum, as well as a Count equal to 1.  Before we apply aggregation to a <code>ValueObserver</code> measurement, we can clearly define it as an Intantaneous measurement.  A measurement, captured at an instant near the end of the collection interval, is neither a cumulative nor a delta with respect to the prior collection interval.</p>
<p><a href="./0088-metric-instrument-optional-refinements.html">OTEP 88</a> discusses the Last Value relationship to help address this question.  After capturing a single <code>ValueObserver</code> measurement for a given instrument and label set, that measurement becomes the Last value associated with that instrument until the next measurement is taken.</p>
<p>To aggregate <code>ValueObserver</code> measurements across spatial dimensions means to combine last values into a distribution at an effective moment in time.  MinMaxSumCount aggregation, in this case, means computing the Min and Max values, the measurement sum, and the count of distinct label sets that contributed measurements.  The aggregated result is considered instantaneous: it may have been computed using data points from different machines, potentially using different collection intervals.  The aggregate value must be considered approximate, with respect to time, since it averages the results from uncoordinated collection intervals.  We may have combined the last-value from a 1-minute collection interval with the last-value from a 10-second collection interval: the result is an instantaneous summary of the distribution across spatial dimensions.</p>
<p>Aggregating <code>ValueObserver</code> measurements across the time dimension for a given instrument and label set yields a set of measurements that were taken across a span of time, but this does not automatically lead us to consider them delta measurements.  If we aggregate 10 consecutive collection intervals for a given label set, what we have is distribution of instantaneous measurements with Count equal to 10, with the Min, Max and Sum serving to convey the average value and the range of values present in the distribution.  The result is a time-averaged distribution of instantaneous measurements.</p>
<p>Whether aggregating across time or space, it has been argued, the result of a <code>ValueObserver</code> instrument has the Instantaneous temporal quality.</p>
<h4><a class="header" href="#temporal-and-spatial-aggregation-of-valueobserver-measurements" id="temporal-and-spatial-aggregation-of-valueobserver-measurements">Temporal and spatial aggregation of <code>ValueObserver</code> measurements</a></h4>
<p>Aggregating <code>ValueObserver</code> measurements across both spatial and time dimensions must be done carefully to avoid a bias toward results computed over shorter collection intervals.  A time-averaged aggregation across spatial dimensions must take the collection interval into account, which can be done as follows:</p>
<ol>
<li>Decide the time span being queried, say [T_begin, T_end].</li>
<li>Divide the time span into a list of timestamps, say [T_begin, T_begin+(T_end-T_begin)/2, T_end].</li>
<li>For each distinct label set and timestamp, compute the spatial aggregation using the last-value definition at that timestamp.  This results in a set of timestamped aggregate measurements with comparable counts.</li>
<li>Aggregate the timestamped measurements from step 3.</li>
</ol>
<p>Steps 2 and 3 ensure that measurements taken less frequently have equal representation in the output, by virtue of computing the spatial aggregation first.  If we were to compute the temporal aggregation first, then aggreagate across spatial dimensions, then instruments collected at a higher frequency will contribute correspondingly more points to the aggregation.  Thus, we must aggregate across <code>ValueObserver</code> instruments across spatial dimensions before averaging across time.</p>
<h2><a class="header" href="#open-questions-8" id="open-questions-8">Open Questions</a></h2>
<h3><a class="header" href="#timing-instrument" id="timing-instrument">Timing instrument</a></h3>
<p>One potentially important special-purpose instrument, found in some metrics APIs, is a dedicated instrument for reporting timings.  The rationale is that when reporting timings, getting the units right is important and often not easy.  Many programming languages use a different type to represent time or a difference between times.  To correctly report a timing distribution, OpenTelemetry requires using a <code>ValueRecorder</code> but also configuring it for the units output by the clock that was used.</p>
<p>In the past, a proposal to create a dedicated <code>TimingValueRecorder</code> instrument was rejected.  This instrument would be identical to a <code>ValueRecorder</code>, but its <code>Record()</code> method would be specialized for the correct type used to represent a duration, so that the units could be set correctly and automatically.  A related pattern is a <code>Timer</code> or <code>StopWatch</code> instrument, one responsible for both measuring and capturing a timing.</p>
<p>Should types such as these be added as helpers?  For example, should <code>TimingValueRecorder</code> be a real instrument, or should it be a helper that wraps around a <code>ValueRecorder</code>?  There is a concern that making <code>TimingValueRecorder</code> into a helper makes it less visible, less standard, and that not having it at all will encourage instrumentation mistakes.</p>
<p>This may be revisited in the future.</p>
<h3><a class="header" href="#synchronous-cumulative-and-asynchronous-delta-helpers" id="synchronous-cumulative-and-asynchronous-delta-helpers">Synchronous cumulative and asynchronous delta helpers</a></h3>
<p>A cumulative measurement can be converted into delta measurement by remembering the last-reported value.  A helper instrument could offer to emulate synchronous cumulative measurements by remembering the last-reported value and reporting deltas synchronously.</p>
<p>A delta measurement can be converted into a cumluative measurement by remembering the sum of all reported values.  A helper instrument could offer to emulate asynchronous delta measurements in this way.</p>
<p>Should helpers of this nature be standardized, if there is demand?  These helpers are excluded from the standard because they carry a number of caveats, but as helpers they can easily do what an OpenTelemery SDK cannot do in general.  For example, we are avoiding synchronous cumulative instruments because they seem to imply ordering that an SDK is not required to support, however an instrument helper that itself uses a lock can easily convert to deltas.</p>
<p>Should such helpers be standardized?  The answer is probably no.</p>
<h1><a class="header" href="#otlphttp-http-transport-extension-for-otlp" id="otlphttp-http-transport-extension-for-otlp">OTLP/HTTP: HTTP Transport Extension for OTLP</a></h1>
<p>This is a proposal to add HTTP Transport extension for
<a href="0035-opentelemetry-protocol.html">OTLP</a> (OpenTelemetry Protocol).</p>
<h2><a class="header" href="#table-of-contents-2" id="table-of-contents-2">Table of Contents</a></h2>
<ul>
<li><a href="0099-otlp-http.html#motivation">Motivation</a></li>
<li><a href="0099-otlp-http.html#otlphttp-protocol-details">OTLP/HTTP Protocol Details</a>
<ul>
<li><a href="0099-otlp-http.html#request">Request</a></li>
<li><a href="0099-otlp-http.html#response">Response</a>
<ul>
<li><a href="0099-otlp-http.html#success">Success</a></li>
<li><a href="0099-otlp-http.html#failures">Failures</a></li>
<li><a href="0099-otlp-http.html#throttling">Throttling</a></li>
<li><a href="0099-otlp-http.html#all-other-responses">All Other Responses</a></li>
</ul>
</li>
<li><a href="0099-otlp-http.html#connection">Connection</a></li>
<li><a href="0099-otlp-http.html#parallel-connections">Parallel Connections</a></li>
</ul>
</li>
<li><a href="0099-otlp-http.html#prior-art-and-alternatives">Prior Art and Alternatives</a></li>
</ul>
<h2><a class="header" href="#motivation-24" id="motivation-24">Motivation</a></h2>
<p>OTLP can be currently communicated only via one transport: gRPC. While using
gRPC has certain benefits there are also drawbacks:</p>
<ul>
<li>
<p>Some users have infrastructure limitations that make gRPC-based protocol
usage impossible. For example AWS ALB does not support gRPC connections.</p>
</li>
<li>
<p>gRPC is a relatively big dependency, which some clients are not willing to
take. Plain HTTP is a smaller dependency and is built in the standard
libraries of many programming languages.</p>
</li>
</ul>
<h2><a class="header" href="#otlphttp-protocol-details" id="otlphttp-protocol-details">OTLP/HTTP Protocol Details</a></h2>
<p>This proposal keeps the existing specification of OTLP over gRPC transport
(OTLP/gRPC for short) and defines an additional way to use OTLP protocol over
HTTP transport (OTLP/HTTP for short). OTLP/HTTP uses the same ProtoBuf payload
that is used by OTLP/gRPC and defines how this payload is communicated over HTTP
transport.</p>
<p>OTLP/HTTP uses HTTP POST requests to send telemetry data from clients to
servers. Implementations MAY use HTTP/1.1 or HTTP/2 transports. Implementations
that use HTTP/2 transport SHOULD fallback to HTTP/1.1 transport if HTTP/2
connection cannot be established.</p>
<h3><a class="header" href="#request" id="request">Request</a></h3>
<p>Telemetry data is sent via HTTP POST request.</p>
<p>The default URL path for requests that carry trace data is <code>/v1/traces</code> (for
example the full URL when connecting to &quot;example.com&quot; server will be
<code>https://example.com/v1/traces</code>). The request body is a ProtoBuf-encoded
<a href="https://github.com/open-telemetry/opentelemetry-proto/blob/e6c3c4a74d57f870a0d781bada02cb2b2c497d14/opentelemetry/proto/collector/trace/v1/trace_service.proto#L38"><code>ExportTraceServiceRequest</code></a>
message.</p>
<p>The default URL path for requests that carry metric data is <code>/v1/metrics</code> and the
request body is a ProtoBuf-encoded
<a href="https://github.com/open-telemetry/opentelemetry-proto/blob/e6c3c4a74d57f870a0d781bada02cb2b2c497d14/opentelemetry/proto/collector/metrics/v1/metrics_service.proto#L35"><code>ExportMetricsServiceRequest</code></a>
message.</p>
<p>The client MUST set &quot;Content-Type: application/x-protobuf&quot; request header. The
client MAY gzip the content and in that case SHOULD include &quot;Content-Encoding:
gzip&quot; request header. The client MAY include &quot;Accept-Encoding: gzip&quot; request
header if it can receive gzip-encoded responses.</p>
<p>Non-default URL paths for requests MAY be configured on the client and server
sides.</p>
<h3><a class="header" href="#response" id="response">Response</a></h3>
<h4><a class="header" href="#success" id="success">Success</a></h4>
<p>On success the server MUST respond with <code>HTTP 200 OK</code>. Response body MUST be
ProtoBuf-encoded
<a href="https://github.com/open-telemetry/opentelemetry-proto/blob/e6c3c4a74d57f870a0d781bada02cb2b2c497d14/opentelemetry/proto/collector/trace/v1/trace_service.proto#L47"><code>ExportTraceServiceResponse</code></a>
message for traces and
<a href="https://github.com/open-telemetry/opentelemetry-proto/blob/e6c3c4a74d57f870a0d781bada02cb2b2c497d14/opentelemetry/proto/collector/metrics/v1/metrics_service.proto#L44"><code>ExportMetricsServiceResponse</code></a>
message for metrics.</p>
<p>The server MUST set &quot;Content-Type: application/x-protobuf&quot; response header. If
the request header &quot;Accept-Encoding: gzip&quot; is present in the request the server
MAY gzip-encode the response and set &quot;Content-Encoding: gzip&quot; response header.</p>
<p>The server SHOULD respond with success no sooner than after successfully
decoding and validating the request.</p>
<h4><a class="header" href="#failures" id="failures">Failures</a></h4>
<p>If the processing of the request fails the server MUST respond with appropriate
<code>HTTP 4xx</code> or <code>HTTP 5xx</code> status code. See sections below for more details about
specific failure cases and HTTP status codes that should be used.</p>
<p>Response body for all <code>HTTP 4xx</code> and <code>HTTP 5xx</code> responses MUST be a
ProtoBuf-encoded
<a href="https://godoc.org/google.golang.org/genproto/googleapis/rpc/status#Status">Status</a>
message that describes the problem.</p>
<p>This specification does not use <code>Status.code</code> field and the server MAY omit
<code>Status.code</code> field. The clients are not expected to alter their behavior based
on <code>Status.code</code> field but MAY record it for troubleshooting purposes.</p>
<p>The <code>Status.message</code> field SHOULD contain a developer-facing error message as
defined in <code>Status</code> message schema.</p>
<p>The server MAY include <code>Status.details</code> field with additional details. Read
below about what this field can contain in each specific failure case.</p>
<h4><a class="header" href="#bad-data" id="bad-data">Bad Data</a></h4>
<p>If the processing of the request fails because the request contains data that
cannot be decoded or is otherwise invalid and such failure is permanent then the
server MUST respond with <code>HTTP 400 Bad Request</code>. The <code>Status.details</code> field in
the response SHOULD contain a
<a href="https://github.com/googleapis/googleapis/blob/d14bf59a446c14ef16e9931ebfc8e63ab549bf07/google/rpc/error_details.proto#L166">BadRequest</a>
that describes the bad data.</p>
<p>The client MUST NOT retry the request when it receives <code>HTTP 400 Bad Request</code>
response.</p>
<h4><a class="header" href="#throttling-1" id="throttling-1">Throttling</a></h4>
<p>If the server receives more requests than the client is allowed or the server is
overloaded the server SHOULD respond with <code>HTTP 429 Too Many Requests</code> or
<code>HTTP 503 Service Unavailable</code> and MAY include
<a href="https://tools.ietf.org/html/rfc7231#section-7.1.3">&quot;Retry-After&quot;</a> header with a
recommended time interval in seconds to wait before retrying.</p>
<p>The client SHOULD honour the waiting interval specified in &quot;Retry-After&quot; header
if it is present. If the client receives <code>HTTP 429</code> or <code>HTTP 503</code> response and
&quot;Retry-After&quot; header is not present in the response then the client SHOULD
implement an exponential backoff strategy between retries.</p>
<h4><a class="header" href="#all-other-responses" id="all-other-responses">All Other Responses</a></h4>
<p>All other HTTP responses that are not explicitly listed in this document should
be treated according to HTTP specification.</p>
<p>If the server disconnects without returning a response the client SHOULD retry
and send the same request. The client SHOULD implement an exponential backoff
strategy between retries to avoid overwhelming the server.</p>
<h3><a class="header" href="#connection" id="connection">Connection</a></h3>
<p>If the client is unable to connect to the server the client SHOULD retry the
connection using exponential backoff strategy between retries. The interval
between retries must have a random jitter.</p>
<p>The client SHOULD keep the connection alive between requests.</p>
<p>Server implementations MAY handle OTLP/gRPC and OTLP/HTTP requests on the same
port and multiplex the connections to the corresponding transport handler based
on &quot;Content-Type&quot; request header.</p>
<h3><a class="header" href="#parallel-connections" id="parallel-connections">Parallel Connections</a></h3>
<p>To achieve higher total throughput the client MAY send requests using several
parallel HTTP connections. In that case the maximum number of parallel
connections SHOULD be configurable.</p>
<h2><a class="header" href="#prior-art-and-alternatives-16" id="prior-art-and-alternatives-16">Prior Art and Alternatives</a></h2>
<p>I have also considered HTTP/1.1+WebSocket transport. Experimental implementation
of OTLP over WebSocket transport has shown that it typically has better
performance than plain HTTP transport implementation (WebSocket uses less CPU,
higher throughput in high latency connections). However WebSocket transport
requires slightly more complicated implementation and WebSocket libraries are
less ubiquitous than plain HTTP, which may make implementation in certain
languages difficult or impossible.</p>
<p>HTTP/1.1+WebSocket transport may be considered as a future transport for
high-performance use cases as it exhibits better performance than OTLP/gRPC and
OTLP/HTTP.</p>
<h1><a class="header" href="#metric-instrument-naming-guidelines" id="metric-instrument-naming-guidelines">Metric instrument naming guidelines</a></h1>
<h2><a class="header" href="#purpose" id="purpose">Purpose</a></h2>
<p>Names and labels for metric instruments are primarily how humans interact with metric data -- users rely on these names to build dashboards and perform analysis. The names and hierarchical structure need to be understandable and discoverable during routine exploration -- and this becomes critical during incidents.</p>
<p>To ensure these goals and consistency in future metric naming standards, this outlines a meta-standard for these names.</p>
<h2><a class="header" href="#guidelines" id="guidelines">Guidelines</a></h2>
<p>Metric names and labels exist within a single universe and a single hierarchy. Metric names and labels MUST be considered within the universe of all existing metric names. When defining new metric names and labels, consider the prior art of existing standard metrics and metrics from frameworks/libraries.</p>
<p>Associated metrics SHOULD be nested together in a hierarchy based on their usage. Define a top-level hierarchy for common metric categories: for OS metrics, like CPU and network; for app runtimes, like GC internals. Libraries and frameworks should nest their metrics into a hierarchy as well. This aids in discovery and adhoc comparison. This allows a user to find similar metrics given a certain metric.</p>
<p>The hierarchical structure of metrics defines the namespacing. Supporting OpenTelemetry artifacts define the metric structures and hierarchies for some categories of metrics, and these can assist decisions when creating future metrics.</p>
<p>Common labels SHOULD be consistently named. This aids in discoverability and disambiguates similar labels to metric names.</p>
<p><a href="https://prometheus.io/docs/practices/naming/#metric-names">&quot;As a rule of thumb, <strong>aggregations</strong> over all the dimensions of a given metric <strong>SHOULD</strong> be meaningful,&quot;</a> as Prometheus recommends.</p>
<p>Semantic ambiguity SHOULD be avoided. Use prefixed metric names in cases where similar metrics have significantly different implementations across the breadth of all existing metrics. For example, every garbage collected runtime has slightly different strategies and measures. Using a single set of metric names for GC, not divided by the runtime, could create dissimilar comparisons and confusion for end users. (For example, prefer <code>runtime.java.gc*</code> over <code>runtime.gc.*</code>.) Measures of many operating system metrics are similar.</p>
<p>For conventional metrics or metrics that have their units included in OpenTelemetry metadata (eg <code>metric.WithUnit</code> in Go), SHOULD NOT include the units in the metric name. Units may be included when it provides additional meaning to the metric name. Metrics MUST, above all, be understandable and usable.</p>
<h1><a class="header" href="#zpages-general-direction-110" id="zpages-general-direction-110">zPages: general direction (#110)</a></h1>
<p>Make zPages a standard OpenTelemetry component.</p>
<h2><a class="header" href="#motivation-25" id="motivation-25">Motivation</a></h2>
<p>Self-introspection debug pages or zPages are in-process web pages that display collected data from the process they are attached to. They are used to provide in-process diagnostics without the need of any backend to examine traces or metrics. Various implementations of zPages are widely used in many environments. The standard extensible implementation of zPages in OpenTelemetry will benefit everybody.</p>
<h2><a class="header" href="#explanation-17" id="explanation-17">Explanation</a></h2>
<p>This OTEP is a request to get a general approval for zPages development as an experimental feature <a href="https://github.com/open-telemetry/opentelemetry-specification/pull/632">open-telemetry/opentelemetry-specification#62</a>. See <a href="https://opencensus.io/zpages/">opencensus.io/zpages</a> for the overview of zPages.</p>
<h2><a class="header" href="#internal-details-14" id="internal-details-14">Internal details</a></h2>
<p>Implementation of zPages includes multiple components - data collection (sampling, filtering, configuration), storage and aggregation, and a framework to expose this data.</p>
<p>This is a request for a general direction approval. There are a few principles for the development:</p>
<ol>
<li>zPages MUST NOT be hardcoded into OpenTelemetry SDK.</li>
<li>OpenTelemetry implementation of zPages MUST be split as two separate components - one for data, another for rendering. So that, for example, data providers could be also integrated into other rendering frameworks.</li>
<li>zPages SHOULD be built as a framework that provides a way to extend information exposed from the process. Ideally all the way to replace OpenTelemetry SDK with alternative source of information.</li>
</ol>
<h2><a class="header" href="#trade-offs-and-mitigations-14" id="trade-offs-and-mitigations-14">Trade-offs and mitigations</a></h2>
<p>We may discover that implementation of zPages as a vendor-specific or user-specific plugins may be preferable. Based on initial investigation, extensible standard implementation will benefit everybody.</p>
<h2><a class="header" href="#prior-art-and-alternatives-17" id="prior-art-and-alternatives-17">Prior art and alternatives</a></h2>
<p><a href="https://opencensus.io/zpages/">opencensus.io/zpages</a></p>
<h2><a class="header" href="#open-questions-9" id="open-questions-9">Open questions</a></h2>
<p>N/A</p>
<h2><a class="header" href="#future-possibilities-5" id="future-possibilities-5">Future possibilities</a></h2>
<p>N/A</p>
<h1><a class="header" href="#automatic-resource-detection" id="automatic-resource-detection">Automatic Resource Detection</a></h1>
<p>Introduce a mechanism to support auto-detection of resources.</p>
<h2><a class="header" href="#motivation-26" id="motivation-26">Motivation</a></h2>
<p>Resource information, i.e. attributes associated with the entity producing
telemetry, can currently be supplied to tracer and meter providers or appended
in custom exporters. In addition to this, it would be useful to have a mechanism
to automatically detect resource information from the host (e.g. from an
environment variable or from aws, gcp, etc metadata) and apply this to all kinds
of telemetry. This will in many cases prevent users from having to manually
configure resource information.</p>
<p>Note there are some existing implementations of this already in the SDKs (see
<a href="0111-auto-resource-detection.html#prior-art-and-alternatives">below</a>), but nothing currently in the
specification.</p>
<h2><a class="header" href="#explanation-18" id="explanation-18">Explanation</a></h2>
<p>In order to apply auto-detected resource information to all kinds of telemetry,
a user will need to configure which resource detector(s) they would like to run
(e.g. AWS EC2 detector).</p>
<p>If multiple detectors are configured, and more than one of these successfully
detects a resource, the resources will be merged according to the Merge
interface already defined in the specification, i.e. the earliest matched
resource's attributes will take precedence. Each detector may be run in
parallel, but to ensure deterministic results, the resources must be merged in
the order the detectors were added.</p>
<p>A default implementation of a detector that reads resource data from the
<code>OTEL_RESOURCE</code> environment variable will be included in the SDK. The
environment variable will contain of a list of key value pairs, and these are
expected to be represented in a format similar to the <a href="https://github.com/w3c/baggage/blob/master/baggage/HTTP_HEADER_FORMAT.md#header-content">W3C
Baggage</a>,
except that additional semi-colon delimited metadata is not supported, i.e.:
<code>key1=value1,key2=value2</code>. If the user does not specify any resource, this
detector will be run by default.</p>
<p>Custom resource detectors related to specific environments (e.g. specific cloud
vendors) must be implemented as packages separate to the core SDK, and users
will need to import these separately.</p>
<h2><a class="header" href="#internal-details-15" id="internal-details-15">Internal details</a></h2>
<p>As described above, the following will be added to the Resource SDK
specification:</p>
<ul>
<li>An interface for &quot;detectors&quot;, to retrieve resource information</li>
<li>Specification for a global function to merge resources returned by a set of
detectors</li>
<li>Details of the &quot;from environment variable&quot; detector implementation as
described above</li>
<li>Specification that default detection (from environment variable) runs once on
startup, and is used by all tracer &amp; meter providers by default if no custom
resource is supplied</li>
</ul>
<h3><a class="header" href="#usage" id="usage">Usage</a></h3>
<p>The following example in Go creates a tracer and meter provider that uses
resource information automatically detected from AWS or GCP:</p>
<p>Assumes a dependency has been added on the <code>otel/api</code>, <code>otel/sdk</code>,
<code>otel/awsdetector</code>, and <code>otel/gcpdetector</code> packages.</p>
<pre><code class="language-go">resource, _ := sdkresource.Detect(ctx, 5 * time.Second, awsdetector.ec2, gcpdetector.gce)
tp := sdktrace.NewProvider(sdktrace.WithResource(resource))
mp := push.New(..., push.WithResource(resource))
</code></pre>
<h3><a class="header" href="#components" id="components">Components</a></h3>
<h4><a class="header" href="#detector" id="detector">Detector</a></h4>
<p>The <code>Detector</code> interface will simply contain a <code>Detect</code> function that returns a
Resource.</p>
<p>The <code>Detect</code> function should contain a mechanism to timeout and cancel the
operation. If a detector is not able to detect a resource, it must return an
uninitialized resource such that the result of each call to <code>Detect</code> can be
merged.</p>
<h4><a class="header" href="#global-function" id="global-function">Global Function</a></h4>
<p>The SDK will also provide a global <code>Detect</code> function. This will take a timeout
duration and a set of detectors that should be run and merged in order as
described in the intro, and return a resource.</p>
<h3><a class="header" href="#error-handling" id="error-handling">Error Handling</a></h3>
<p>In the case of one or more detectors raising an error, there are two reasonable
options:</p>
<ol>
<li>Ignore that detector, and continue with a warning (likely meaning we will
continue without expected resource information)</li>
<li>Crash the application (raise a panic)</li>
</ol>
<p>The user can decide how to recover from failure.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-15" id="trade-offs-and-mitigations-15">Trade-offs and mitigations</a></h2>
<ul>
<li>This OTEP proposes storing Vendor resource detection packages outside of the
SDK. This ensures the SDK is free of vendor specific code. Given the
relatively straightforward &amp; minimal amount of code generally needed to
perform resource detection, and the relatively small number of cloud
providers, we may instead decide its okay for all the resource detection code
to live in the SDK directly.
<ul>
<li>If we do allow Vendor resource detection packages in the SDK, we presumably
need to restrict these to not being able to use non-trivial libraries</li>
</ul>
</li>
<li>This OTEP proposes only performing environment variable resource detection by
default. Given the relatively small number of cloud providers, we may instead
decide its okay to run all detectors by default. This raises the question of
if any restrictions would need to be put on this, and how we would handle this
in the future if the number of Cloud Providers rises. It would be difficult to
back out of running these by default as that would lead to a breaking change.</li>
<li>This OTEP proposes a global function the user calls with the detectors they
want to run, and then expects the user to pass these into the providers. An
alternative option (that was previously proposed in this OTEP) would be to
supply a set of detectors directly to the metric or trace provider instead of,
or as an additional option to, a static resource. That would result in
marginally simpler setup code where the user doesn't need to call <code>AutoDetect</code>
themselves. Another advantage of this approach is that its easier to specify
default detectors and override these separately to any static resource the
user may want to provide. On the downside, this approach adds the complexity
of having to deal with the merging the detected resource with a static
resource if provided. It also potentially adds a lot of complexity around how
to avoid having detectors run multiple times since they will be configured for
each provider. Avoiding having to specify detectors for tracer &amp; meter
providers is the primary reason for not going with that option in the end.</li>
<li>The attribute proto now supports arrays &amp; maps. We could support parsing this
out of the <code>OTEL_RESOURCE</code> environment variable similar to how Correlation
Context supports semi colon lists of keys &amp; key-value pairs, but the added
complexity is probably not worthwhile implementing unless someone has a good
use case for this.</li>
<li>In the case of an error at resource detection time, another alternative would
be to start a background thread to retry following some strategy, but it's not
clear that there would be much value in doing this, and it would add
considerable unnecessary complexity.</li>
</ul>
<h2><a class="header" href="#prior-art-and-alternatives-18" id="prior-art-and-alternatives-18">Prior art and alternatives</a></h2>
<p>This proposal is largely inspired by the existing OpenCensus specification, the
OpenCensus Go implementation, and the OpenTelemetry JS implementation. For
reference, see the relevant section of the <a href="https://github.com/census-instrumentation/opencensus-specs/blob/master/resource/Resource.md#populating-resources">OpenCensus
specification</a></p>
<h3><a class="header" href="#existing-opentelemetry-implementations" id="existing-opentelemetry-implementations">Existing OpenTelemetry implementations</a></h3>
<ul>
<li>Resource detection implementation in JS SDK
<a href="https://github.com/open-telemetry/opentelemetry-js/tree/master/packages/opentelemetry-resources">here</a>:
The JS implementation is very similar to this proposal. This proposal states
that the SDK will allow detectors to be passed into telemetry providers
directly instead of just having a global <code>DetectResources</code> function which the
user will need to call and pass in explicitly. In addition, vendor specific
resource detection code is currently in the JS resource package, so this would
need to be separated.</li>
<li>Environment variable resource detection in Java SDK
<a href="https://github.com/open-telemetry/opentelemetry-java/blob/master/sdk/src/main/java/io/opentelemetry/sdk/resources/EnvVarResource.java">here</a>:
This implementation does not currently include a detector interface, but is
used by default for tracer and meter providers</li>
</ul>
<h2><a class="header" href="#open-questions-10" id="open-questions-10">Open questions</a></h2>
<ul>
<li>Does this interfere with any other upcoming specification changes related to
resources?</li>
<li>If custom detectors need to live outside the core repo, what is the
expectation regarding where they should be hosted?</li>
<li>Also see the <a href="0111-auto-resource-detection.html#trade-offs-and-mitigations">Trade-offs and mitigations</a> section</li>
</ul>
<h2><a class="header" href="#future-possibilities-6" id="future-possibilities-6">Future possibilities</a></h2>
<p>When the Collector is run as an agent, the same interface, shared with the Go
SDK, could be used to append resource information detected from the host to all
kinds of telemetry in a Processor (probably as an extension to the existing
Resource Processor). This would require a translation from the SDK resource to
the collector's internal representation of a resource.</p>
<h1><a class="header" href="#integrate-exemplars-with-metrics" id="integrate-exemplars-with-metrics">Integrate Exemplars with Metrics</a></h1>
<p>This OTEP adds exemplar support to aggregations defined in the Metrics SDK.</p>
<h2><a class="header" href="#definition" id="definition">Definition</a></h2>
<p>Exemplars are example data points for aggregated data. They provide specific context to otherwise general aggregations. For histogram-type metrics, exemplars are points associated with each bucket in the histogram giving an example of what was aggregated into the bucket. Exemplars are augmented beyond just measurements with references to the sampled trace where the measurement was recorded and labels that were attached to the measurement.</p>
<h2><a class="header" href="#motivation-27" id="motivation-27">Motivation</a></h2>
<p>Defining exemplar behaviour for aggregations allows OpenTelemetry to support exemplars in Google Cloud Monitoring.</p>
<p>Exemplars provide a link between metrics and traces. Consider a user using a Histogram aggregation to track response latencies over time for a high QPS server. The histogram is composed of buckets based on the speed of the request, for example, &quot;there were 55 requests that took 400-500 milliseconds&quot;. The user wants to troubleshoot slow requests, so they would need to find a trace where the latency was high. With exemplars, the user is able to get an exemplar trace from a high latency bucket, an exemplar trace from a low latency bucket, and compare them to figure out the reason for the high latency.</p>
<p>Exemplars are meaningful for all aggregations where relevant traces can provide more context to the aggregation, as well as when exemplars can display specific information not otherwise shown in the aggregation (for example, the full set of labels where they otherwise might be aggregated away).</p>
<h2><a class="header" href="#internal-details-16" id="internal-details-16">Internal details</a></h2>
<p>An exemplar is a <code>RawValue</code>, which is defined as:</p>
<pre><code>message RawValue {
  // Numerical value of the measurement that was recorded. Only one of these two fields is
  // used for the data, depending on its type
  double double_value = 0;
  int64 int64_value = 1;
  
  // Exact time that the measurement was recorded
  fixed64 time_unix_nano = 2;

  // 'label:value' map of all labels that were provided by the user recording the measurement
  repeated opentelemetry.proto.common.v1.StringKeyValue labels = 3;

  // Span ID of the current trace
  optional bytes span_id = 4;

  // Trace ID of the current trace
  optional bytes trace_id = 5;

  // When sample_count is non-zero, this exemplar has been chosen in a statistically
  // unbiased way such that the exemplar is representative of `sample_count` individual events
  optional double sample_count = 6;
}
</code></pre>
<p>Exemplar collection should be enabled through an optional parameter (disabled by default), and when not enabled, there should be no collection/logic performed related to exemplars. This is to ensure that when necessary, aggregators are as high performance as possible. Aggregators should also have a parameter to determine whether exemplars should only be collected if they are recorded during a sampled trace, or if tracing should have no effect on which exemplars are sampled. This allows aggregations to prioritize either the link between metrics and traces or the statistical significance of exemplars, when necessary.</p>
<p><a href="https://github.com/open-telemetry/opentelemetry-specification/pull/347">#347</a> describes a set of standard aggregators in the metrics SDK. Here we describe how exemplars could be implemented for each aggregator.</p>
<h3><a class="header" href="#exemplar-behaviour-for-standard-aggregators" id="exemplar-behaviour-for-standard-aggregators">Exemplar behaviour for standard aggregators</a></h3>
<h4><a class="header" href="#histogramaggregator" id="histogramaggregator">HistogramAggregator</a></h4>
<p>The HistogramAggregator MUST (when enabled) maintain a list of exemplars whose values are distributed across all buckets of the histogram (there should be one or more exemplars in every bucket that has a population of at least one sample-able measurement). Implementations SHOULD NOT retain an unbounded number of exemplars.</p>
<h4><a class="header" href="#sketch" id="sketch">Sketch</a></h4>
<p>A Sketch aggregator SHOULD maintain a list of exemplars whose values are spaced out across the distribution. There is no specific number of exemplars that should be retained (although the amount SHOULD NOT be unbounded), but the implementation SHOULD pick exemplars that represent as much of the distribution as possible. (Specific details not defined, see open questions.)</p>
<h4><a class="header" href="#last-value" id="last-value">Last-Value</a></h4>
<p>Most (if not all) Last-Value aggregators operate asynchronously and do not ever interact with context. Since the value of a Last-Value is the last measurement (essentially the other parts of an exemplar), exemplars are not worth implementing for Last-Value.</p>
<h4><a class="header" href="#exact" id="exact">Exact</a></h4>
<p>The Exact aggregator will function by maintaining a list of <code>RawValue</code>s, which contain all of the information exemplars would carry. Therefore the Exact aggregator will not need to maintain any exemplars.</p>
<h4><a class="header" href="#counter-1" id="counter-1">Counter</a></h4>
<p>Exemplars give value to counter aggregations in two ways: One, by tying metric and trace data together, and two, by providing necessary information to re-create the input distribution. When enabled, the aggregator will retain a bounded list of exemplars at each checkpoint, sampled from across the distribution of the data. Exemplars should be sampled in a statistically significant way.</p>
<h4><a class="header" href="#minmaxsumcount" id="minmaxsumcount">MinMaxSumCount</a></h4>
<p>Similar to Counter, MinMaxSumCount should retain a bounded list of exemplars that were sampled from across the input distribution in a statistically significant way.</p>
<h4><a class="header" href="#custom-aggregators" id="custom-aggregators">Custom Aggregators</a></h4>
<p>Custom aggregators MAY support exemplars by maintaining a list of exemplars that can be retrieved by exporters. Custom aggregators should select exemplars based on their usage by the connected exporter (for example, exemplars recorded for Google Cloud Monitoring should only be retained if they were recorded within a sampled trace).</p>
<p>Exemplars will always be retrieved from aggregations (by the exporter) as a list of RawValue objects. They will be communicated via a</p>
<pre><code>optional repeated RawValue exemplars = 6
</code></pre>
<p>attribute on the <code>Metric</code> object.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-16" id="trade-offs-and-mitigations-16">Trade-offs and mitigations</a></h2>
<p>Performance (in terms of memory usage and to some extent time complexity) is the main concern of implementing exemplars. However, by making recording exemplars optional, there should be minimal overhead when exemplars are not enabled.</p>
<h2><a class="header" href="#prior-art-and-alternatives-19" id="prior-art-and-alternatives-19">Prior art and alternatives</a></h2>
<p>Exemplars are implemented in <a href="https://github.com/census-instrumentation/opencensus-specs/blob/master/stats/Exemplars.md#exemplars">OpenCensus</a>, but only for HistogramAggregator. This OTEP is largely a port from the OpenCensus definition of exemplars, but it also adds exemplar support to other aggregators.</p>
<p><a href="https://cloud.google.com/monitoring/api/ref_v3/rpc/google.api#google.api.Distribution.Exemplar">Cloud monitoring API doc for exemplars</a></p>
<h2><a class="header" href="#open-questions-11" id="open-questions-11">Open questions</a></h2>
<ul>
<li>
<p>Exemplars usually refer to a span in a sampled trace. While using the collector to perform tail-sampling, the sampling decision may be deferred until after the metric would be exported. How do we create exemplars in this case?</p>
</li>
<li>
<p>We don’t have a strong grasp on how the sketch aggregator works in terms of implementation - so we don’t have enough information to design how exemplars should work properly.</p>
</li>
<li>
<p>The spec doesn't yet define a standard set of aggregations, just default aggregations for standard metric instruments. Since exemplars are always attached to particular aggregations, it's impossible to fully specify the behavior of exemplars.</p>
</li>
</ul>
<h1><a class="header" href="#standard-names-for-systemruntime-metric-instruments" id="standard-names-for-systemruntime-metric-instruments">Standard names for system/runtime metric instruments</a></h1>
<p>This OTEP proposes a set of standard names, labels, and semantic conventions for common system/runtime metric instruments in OpenTelemetry. The instrument names proposed here are common across the supported operating systems and runtime environments. Also included are general semantic conventions for system/runtime metrics including those not specific to a particular OS or runtime.</p>
<p>This OTEP is largely based on the existing implementation in the OpenTelemetry Collector's <a href="https://github.com/open-telemetry/opentelemetry-collector/tree/1ad767e62f3dff6f62f32c7360b6fefe0fbf32ff/receiver/hostmetricsreceiver">Host Metrics Receiver</a>. The proposed names aim to make system/runtime metrics unambiguous and easily discoverable. See <a href="https://github.com/open-telemetry/oteps/pull/108/files">OTEP #108</a> for additional motivation.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-17" id="trade-offs-and-mitigations-17">Trade-offs and mitigations</a></h2>
<p>When naming a metric instrument, there is a trade off between discoverability and ambiguity. For example, a metric called <code>system.cpu.load_average</code> is very discoverable, but the meaning of this metric is ambiguous. <a href="https://en.wikipedia.org/wiki/Load_(computing)">Load average</a> is well defined on UNIX, but is not a standard metric on Windows. While discoverability is important, names must be unambiguous.</p>
<h2><a class="header" href="#prior-art-1" id="prior-art-1">Prior art</a></h2>
<p>There are already a few implementations of system and/or runtime metric collection in OpenTelemetry:</p>
<ul>
<li>
<p><strong><a href="https://github.com/open-telemetry/oteps/pull/108/files">OTEP #108</a></strong></p>
<ul>
<li>Provides high level guidelines around naming metric instruments.</li>
<li>Came out of a <a href="https://docs.google.com/spreadsheets/d/1WlStcUe2eQoN1y_UF7TOd6Sw7aV_U0lFcLk5kBNxPsY/edit#gid=0">prior proposal</a> for system metrics?</li>
</ul>
</li>
<li>
<p><strong>Collector</strong></p>
<ul>
<li><a href="https://github.com/open-telemetry/opentelemetry-collector/tree/1ad767e62f3dff6f62f32c7360b6fefe0fbf32ff/receiver/hostmetricsreceiver">Host Metrics Receiver</a> generates metrics about the host system when run as an agent.</li>
<li>Currently is the most comprehensive implementation.</li>
<li>Collects system metrics for CPU, memory, swap, disks, filesystems, network, and load.</li>
<li>There are plans to collect process metrics for CPU, memory, and disk I/O.</li>
<li>Makes good use of labels rather than defining individual metrics.</li>
<li><a href="https://docs.google.com/spreadsheets/d/11qSmzD9e7PnzaJPYRFdkkKbjTLrAKmvyQpjBjpJsR2s">Overview of collected metrics</a>.</li>
</ul>
</li>
<li>
<p><strong>Go</strong></p>
<ul>
<li>Go <a href="https://github.com/open-telemetry/opentelemetry-go-contrib/tree/master/instrumentation/runtime">has instrumentation</a> to collect runtime metrics for GC, heap use, and goroutines.</li>
<li>This package does not export metrics with labels, instead exporting individual metrics.</li>
<li><a href="https://docs.google.com/spreadsheets/d/1r50cC9ass0A8SZIg2ZpLdvZf6HmQJsUSXFOu-rl4yaY/edit#gid=0">Overview of collected metrics</a>.</li>
</ul>
</li>
<li>
<p><strong>Python</strong></p>
<ul>
<li>Python <a href="https://github.com/open-telemetry/opentelemetry-python/tree/master/ext/opentelemetry-ext-system-metrics">has instrumentation</a> to collect some system and runtime metrics.</li>
<li>Collects system CPU, memory, and network metrics.</li>
<li>Collects runtime CPU, memory, and GC metrics.</li>
<li>Makes use of labels, similar to the Collector.</li>
<li><a href="https://docs.google.com/spreadsheets/d/1r50cC9ass0A8SZIg2ZpLdvZf6HmQJsUSXFOu-rl4yaY/edit#gid=0">Overview of collected metrics</a>.</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#semantic-conventions" id="semantic-conventions">Semantic Conventions</a></h2>
<p>The following semantic conventions aim to keep naming consistent. Not all possible metrics are covered by these conventions, but they provide guidelines for most of the cases in this proposal:</p>
<ul>
<li><strong>usage</strong> - an instrument that measures an amount used out of a known total amount should be called <code>entity.usage</code>. For example, <code>system.filesystem.usage</code> for the amount of disk spaced used. A measure of the amount of an unlimited resource consumed is differentiated from <strong>usage</strong>. This may be time, data, etc.</li>
<li><strong>utilization</strong> - an instrument that measures a <em>value ratio</em> of usage (like percent, but in the range <code>[0, 1]</code>) should be called <code>entity.utilization</code>. For example, <code>system.memory.utilization</code> for the ratio of memory in use.</li>
<li><strong>time</strong> - an instrument that measures passage of time should be called <code>entity.time</code>. For example, <code>system.cpu.time</code> with varying values of label <code>state</code> for idle, user, etc.</li>
<li><strong>io</strong> - an instrument that measures bidirectional data flow should be called <code>entity.io</code> and have labels for direction. For example, <code>system.network.io</code>.</li>
<li>Other instruments that do not fit the above descriptions may be named more freely. For example, <code>system.swap.page_faults</code> and <code>system.network.packets</code>. Units do not need to be specified in the names since they are included during instrument creation, but can be added if there is ambiguity.</li>
</ul>
<h2><a class="header" href="#internal-details-17" id="internal-details-17">Internal details</a></h2>
<p>The following standard metric instruments should be used in libraries instrumenting system/runtime metrics (here is a <a href="https://docs.google.com/spreadsheets/d/1r50cC9ass0A8SZIg2ZpLdvZf6HmQJsUSXFOu-rl4yaY/edit#gid=973941697">spreadsheet</a> of the tables below).</p>
<p>In the tables below, units of <code>1</code> refer to a ratio value that is always in the range <code>[0, 1]</code>. Instruments that measure an integer count of something use semantic units like <code>packets</code>, <code>errors</code>, <code>faults</code>, etc.</p>
<h3><a class="header" href="#standard-system-metrics---system" id="standard-system-metrics---system">Standard System Metrics - <code>system.</code></a></h3>
<hr />
<h4><a class="header" href="#systemcpu" id="systemcpu"><code>system.cpu.</code></a></h4>
<p><strong>Description:</strong> System level processor metrics.
|Name                  |Units  |Instrument Type  |Value Type|Label Key|Label Values                       |
|----------------------|-------|-----------------|----------|---------|-----------------------------------|
|system.cpu.time       |seconds|SumObserver      |Double    |state    |idle, user, system, interrupt, etc.|
|                      |       |                 |          |cpu      |1 - #cores                         |
|system.cpu.utilization|1      |UpDownSumObserver|Double    |state    |idle, user, system, interrupt, etc.|
|                      |       |                 |          |cpu      |1 - #cores                         |</p>
<h4><a class="header" href="#systemmemory" id="systemmemory"><code>system.memory.</code></a></h4>
<p><strong>Description:</strong> System level memory metrics.
|Name                     |Units|Instrument Type  |Value Type|Label Key|Label Values            |
|-------------------------|-----|-----------------|----------|---------|------------------------|
|system.memory.usage      |bytes|UpDownSumObserver|Int64     |state    |used, free, cached, etc.|
|system.memory.utilization|1    |UpDownSumObserver|Double    |state    |used, free, cached, etc.|</p>
<h4><a class="header" href="#systemswap" id="systemswap"><code>system.swap.</code></a></h4>
<p><strong>Description:</strong> System level swap/paging metrics.
|Name                        |Units     |Instrument Type  |Value Type|Label Key|Label Values|
|----------------------------|----------|-----------------|----------|---------|------------|
|system.swap.usage           |pages     |UpDownSumObserver|Int64     |state    |used, free  |
|system.swap.utilization     |1         |UpDownSumObserver|Double    |state    |used, free  |
|system.swap.page_faults    |faults    |SumObserver      |Int64     |type     |major, minor|
|system.swap.page_operations|operations|SumObserver      |Int64     |type     |major, minor|
|                            |          |                 |          |direction|in, out     |</p>
<h4><a class="header" href="#systemdisk" id="systemdisk"><code>system.disk.</code></a></h4>
<p><strong>Description:</strong> System level disk performance metrics.
|Name                        |Units     |Instrument Type|Value Type|Label Key|Label Values|
|----------------------------|----------|---------------|----------|---------|------------|
|system.disk.io<!--notlink-->|bytes     |SumObserver    |Int64     |device   |(identifier)|
|                            |          |               |          |direction|read, write |
|system.disk.operations      |operations|SumObserver    |Int64     |device   |(identifier)|
|                            |          |               |          |direction|read, write |
|system.disk.time            |seconds   |SumObserver    |Double    |device   |(identifier)|
|                            |          |               |          |direction|read, write |
|system.disk.merged          |1         |SumObserver    |Int64     |device   |(identifier)|
|                            |          |               |          |direction|read, write |</p>
<h4><a class="header" href="#systemfilesystem" id="systemfilesystem"><code>system.filesystem.</code></a></h4>
<p><strong>Description:</strong> System level filesystem metrics.
|Name                         |Units|Instrument Type  |Value Type|Label Key|Label Values        |
|-----------------------------|-----|-----------------|----------|---------|--------------------|
|system.filesystem.usage      |bytes|UpDownSumObserver|Int64     |device   |(identifier)        |
|                             |     |                 |          |state    |used, free, reserved|
|system.filesystem.utilization|1    |UpDownSumObserver|Double    |device   |(identifier)        |
|                             |     |                 |          |state    |used, free, reserved|</p>
<h4><a class="header" href="#systemnetwork" id="systemnetwork"><code>system.network.</code></a></h4>
<p><strong>Description:</strong> System level network metrics.
|Name                           |Units      |Instrument Type  |Value Type|Label Key|Label Values                                                                                  |
|-------------------------------|-----------|-----------------|----------|---------|----------------------------------------------------------------------------------------------|
|system.network.dropped_packets|packets    |SumObserver      |Int64     |device   |(identifier)                                                                                  |
|                               |           |                 |          |direction|transmit, receive                                                                             |
|system.network.packets         |packets    |SumObserver      |Int64     |device   |(identifier)                                                                                  |
|                               |           |                 |          |direction|transmit, receive                                                                             |
|system.network.errors          |errors     |SumObserver      |Int64     |device   |(identifier)                                                                                  |
|                               |           |                 |          |direction|transmit, receive                                                                             |
|system<!--notlink-->.network.io|bytes      |SumObserver      |Int64     |device   |(identifier)                                                                                  |
|                               |           |                 |          |direction|transmit, receive                                                                             |
|system.network.connections     |connections|UpDownSumObserver|Int64     |device   |(identifier)                                                                                  |
|                               |           |                 |          |protocol |tcp, udp, <a href="https://en.wikipedia.org/wiki/Transport_layer#Protocols">others</a>                   |
|                               |           |                 |          |state    |<a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Protocol_operation">e.g. for tcp</a>|</p>
<h4><a class="header" href="#os-specific-system-metrics---systemos" id="os-specific-system-metrics---systemos">OS Specific System Metrics - <code>system.{os}.</code></a></h4>
<p>Instrument names for system level metrics specific to a certain operating system should be prefixed with <code>system.{os}.</code> and follow the hierarchies listed above for different entities like CPU, memory, and network. For example, an instrument for counting the number of Linux merged disk operations (see <a href="https://unix.stackexchange.com/questions/462704/iostat-what-is-exactly-the-concept-of-merge">here</a> and <a href="https://man7.org/linux/man-pages/man1/iostat.1.html">here</a>) could be named <code>system.linux.disk.merged_operations</code>, reusing the <code>disk</code> name proposed above.</p>
<h3><a class="header" href="#standard-runtime-metrics---runtime" id="standard-runtime-metrics---runtime">Standard Runtime Metrics - <code>runtime.</code></a></h3>
<hr />
<p>Runtime environments vary widely in their terminology, implementation, and relative values for a given metric. For example, Go and Python are both garbage collected languages, but comparing heap usage between the two runtimes directly is not meaningful. For this reason, this OTEP does not propose any standard top-level runtime metric instruments. See <a href="https://github.com/open-telemetry/oteps/pull/108/files">OTEP #108</a> for additional discussion.</p>
<h4><a class="header" href="#runtime-specific-metrics---runtimeenvironment" id="runtime-specific-metrics---runtimeenvironment">Runtime Specific Metrics - <code>runtime.{environment}.</code></a></h4>
<p>Runtime level metrics specific to a certain runtime environment should be prefixed with <code>runtime.{environment}.</code> and follow the semantic conventions outlined in <a href="0119-standard-system-metrics.html#semantic-conventions">Semantic Conventions</a>. For example, Go runtime metrics use <code>runtime.go.</code> as a prefix.</p>
<p>Some programming languages have multiple runtime environments that vary significantly in their implementation, for example <a href="https://wiki.python.org/moin/PythonImplementations">Python has many implementations</a>. For these languages, consider using specific <code>environment</code> prefixes to avoid ambiguity, like <code>runtime.cpython.</code> and <code>runtime.pypy.</code>.</p>
<h2><a class="header" href="#open-questions-12" id="open-questions-12">Open questions</a></h2>
<ul>
<li>Should the individual runtimes have their specific naming conventions in the spec?</li>
<li>Is it ok to include instruments specific to an OS (or OS family) under a top-level prefix, as long as they are unambiguous? For example, naming inode related instruments, which of the below is preferred?
<ol>
<li>Top level: <code>system.filesystem.inodes.*</code></li>
<li>UNIX family level: <code>system.unix.filesystem.inodes.*</code></li>
<li>One for each UNIX OS: <code>system.linux.filesystem.inodes.*</code>, <code>system.freebsd.filesystem.inodes.*</code>, <code>system.netbsd.filesystem.inodes.*</code>, etc.</li>
</ol>
</li>
</ul>
<h1><a class="header" href="#a-dynamic-configuration-service-for-the-sdk" id="a-dynamic-configuration-service-for-the-sdk">A Dynamic Configuration Service for the SDK</a></h1>
<p>This proposal is a request to develop a prototype to configure metric collection periods. Per-metric and tracing configuration is also intended to be added, with details left for a later iteration.</p>
<p>It is related to <a href="https://github.com/open-telemetry/opentelemetry-proto/pull/155">this pull request</a></p>
<h2><a class="header" href="#motivation-28" id="motivation-28">Motivation</a></h2>
<p>During normal use, users may wish to collect metrics every 10 minutes. Later, while investigating a production issue, the same user could easily increase information available for debugging by reconfiguring some of their processes to collect metrics every 30 seconds. Because this change is centralized and does not require redeploying with new configurations, there is lower friction and risk in updating the configurations.</p>
<h2><a class="header" href="#explanation-19" id="explanation-19">Explanation</a></h2>
<p>This OTEP is a proposal for an experimental feature <a href="https://github.com/open-telemetry/opentelemetry-specification/pull/632">open-telemetry/opentelemetry-specification#62</a>, to be developed as a proof of concept. This means no development will be done inside either the OpenTelemetry SDK or the collector. Since this will be implemented in <a href="https://github.com/open-telemetry/opentelemetry-go-contrib">opentelemetry-go-contrib</a> and <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib">opentelemetry-collector-contrib</a>, all of this functionality will be optional.</p>
<p>The user, when instrumenting their application, can configure the SDK with the endpoint of their remote configuration service, the associated Resource, and a default config to be used if it fails to read from the configuration service.</p>
<p>The user must then set up the config service. This can be done through the collector, which can be set up to expose an arbitrary configuration service implementation. Depending on implementation, this allows the collector to either act as a stand-alone configuration service, or as a bridge to remote configurations of the user's monitoring backend by 'translating' the monitoring backend's protocol to comply with the OpenTelemetry configuration protocol.</p>
<h2><a class="header" href="#internal-details-18" id="internal-details-18">Internal details</a></h2>
<p>In the future, we intend to add per-metric configuration. For example, this would allow the user to collect 5xx server error counts ever minute, and CPU usage statistics every 10 minutes. The remote configuration protocol was designed with this in mind, meaning that it includes more details than simply the metric collection period.</p>
<p>Our remote configuration protocol will support this call:</p>
<pre><code>service MetricConfig {
  rpc GetMetricConfig (MetricConfigRequest) returns (MetricConfigResponse);
}
</code></pre>
<p>A request to the config service will look like this:</p>
<pre><code>message MetricConfigRequest{

  // Required. The resource for which configuration should be returned.
  opentelemetry.proto.resource.v1.Resource resource = 1;

  // Optional. The value of ConfigResponse.fingerprint for the last configuration
  // that the caller received and successfully applied.
  bytes last_known_fingerprint = 2;
}
</code></pre>
<p>While the response will look like this:</p>
<pre><code>message MetricConfigResponse {

  // Optional. The fingerprint associated with this MetricConfigResponse. Each
  // change in configs yields a different fingerprint.
  bytes fingerprint = 1;

  // A Schedule is used to apply a particular scheduling configuration to
  // a metric. If a metric name matches a schedule's patterns, then the metric
  // adopts the configuration specified by the schedule.

  message Schedule {

    // A light-weight pattern that can match 1 or more
    // metrics, for which this schedule will apply. The string is used to
    // match against metric names. It should not exceed 100k characters.
    message Pattern {
      oneof match {
        string equals = 1;       // matches the metric name exactly
        string starts_with = 2;  // prefix-matches the metric name
      }
    }

    // Metrics with names that match at least one rule in the inclusion_patterns are
    // targeted by this schedule. Metrics that match at least one rule from the
    // exclusion_patterns are not targeted for this schedule, even if they match an
    // inclusion pattern.

    // For this iteration, since we only want one Schedule that applies to all metrics,
    // we will not check the inclusion_patterns and exclusion_patterns.
    repeated Pattern exclusion_patterns = 1;
    repeated Pattern inclusion_patterns = 2;

    // Describes the collection period for each schedule in seconds.
    int32 period_sec = 3;
  }

  // For this iteration, since we only want one Schedule that applies to all metrics,
  // we will have a restriction that schedules must have a length of 1, and we will
  // not check the patterns when we apply the collection period.
  repeated Schedule schedules = 2;

  // Optional. The client is suggested to wait this long (in seconds) before
  // pinging the configuration service again.
  int32 suggested_wait_time_sec = 3;
}
</code></pre>
<p>The SDK will periodically read a config from the service using GetConfig. This reading interval can depend on If it fails to do so, it will just use either the default config or the most recent successfully read config. If it reads a new config, it will apply it.</p>
<p>Export frequency from the SDK depends on Schedules. There can only be one Schedule for now, which defines the schedule for all metrics. The schedule has a CollectionPeriod, which defines how often metrics are exported.</p>
<p>In the future, we will add per-metric configuration. Each Schedule also has inclusion_patterns and exclusion_patterns. Any metrics that match any of the inclusion_patterns and do not match any of the exclusion_patterns will be exported every CollectionPeriod (e.g. every minute). A component will be added that can export metrics that match a pattern from one Schedule at that Schedule's collection period, while exporting other metrics that match the patterns of another Schedule at that other Schedule's collection period.</p>
<p>The collector will support a new interface for a DynamicConfig service that can be used by an SDK, allowing a custom implementation of the configuration service protocol described above, to act as an optional bridge between an SDK and an arbitrary configuration service. This interface can be implemented as a shim to support accessing remote configurations from arbitrary backends. The collector is configured to expose an endpoint for requests to the DynamicConfig service, and returns results on that endpoint.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-18" id="trade-offs-and-mitigations-18">Trade-offs and mitigations</a></h2>
<p>This feature will be implemented purely as an experiment, to demonstrate its viability and usefulness. More investigation can be done after a rough prototype is demonstrated.</p>
<p>As mentioned <a href="https://github.com/open-telemetry/opentelemetry-proto/pull/155#issuecomment-640582048">here</a>, the configuration service can be a potential attack vector for an application instrumented with OpenTelemetry, depending on what we allow in the protocol. We can highlight in the remote configuration protocol that for future changes, caution is needed in terms of the sorts of configurations we allow.</p>
<p>Having a small polling interval (how often we read configs) would mean that config changes could be applied very quickly. However, this would also increase load on the configuration service. The typical use case probably does not need config changes to be applied immediately and config changes will likely be quite infrequent, so a typical polling interval probably needs to be no more frequent than every several minutes.</p>
<h2><a class="header" href="#prior-art-and-alternatives-20" id="prior-art-and-alternatives-20">Prior art and alternatives</a></h2>
<p>Jaegar has the option of a Remote sampler, which allows reading from a central configuration, even dynamically with an Adaptive sampler.</p>
<p>The main comparative for remote configuration is a push vs. polling mechanism. The benefits of having a mechanism where the configuration service pushes new configs is that it's less work for the user, with it being not necessary for them to set up a configuration service. There is also no load associated with polling the configuration service in the instrumented application, which would keep the OpenTelemetry SDK more lightweight.</p>
<p>Using a polling mechanism may be more performant in the context of large distributed applications with many instrumented processes. This is a result of having the instrumented processes polling the configuration service, rather than the config service having to push to processes. A polling mechanism is also compatible with more network protocols, not just gRPC.</p>
<h2><a class="header" href="#open-questions-13" id="open-questions-13">Open questions</a></h2>
<ul>
<li>As mentioned <a href="https://github.com/open-telemetry/opentelemetry-proto/pull/155#issuecomment-640582048">here</a>. what happens if a malicious/accidental config change overwhelms the application/monitoring system? Is it the responsibility of the user to be cautious while making config changes? Should we automatically decrease telemetry exporting if we can detect performance problems?</li>
</ul>
<h2><a class="header" href="#future-possibilities-7" id="future-possibilities-7">Future possibilities</a></h2>
<p>If this OTEP is implemented, there is the option to remotely and dynamically configure other things. As mentioned <a href="https://github.com/open-telemetry/opentelemetry-proto/pull/155#issuecomment-639878490">here</a>, possibilities include labels and aggregations. As mentioned <a href="https://github.com/open-telemetry/oteps/pull/121#discussion_r447839301">here</a>, it is also possible to configure the collector.</p>
<p>It is intended to add per-metric configuration and well as tracing configuration in the future.</p>
<h1><a class="header" href="#otlp-json-encoding-for-otlphttp" id="otlp-json-encoding-for-otlphttp">OTLP: JSON Encoding for OTLP/HTTP</a></h1>
<p>This is a proposal to add HTTP Transport extension supporting json serialization for
<a href="0035-opentelemetry-protocol.html">OTLP</a> (OpenTelemetry Protocol).</p>
<h2><a class="header" href="#table-of-contents-3" id="table-of-contents-3">Table of Contents</a></h2>
<ul>
<li><a href="0122-otlp-http-json.html#motivation">Motivation</a></li>
<li><a href="0122-otlp-http-json.html#otlphttpjson-protocol-details">OTLP/HTTP+JSON Protocol Details</a>
<ul>
<li><a href="0122-otlp-http-json.html#json-mapping">JSON Mapping</a></li>
<li><a href="0122-otlp-http-json.html#request">Request</a></li>
<li><a href="0122-otlp-http-json.html#response">Response</a>
<ul>
<li><a href="0122-otlp-http-json.html#success">Success</a></li>
<li><a href="0122-otlp-http-json.html#failures">Failures</a></li>
<li><a href="0122-otlp-http-json.html#throttling">Throttling</a></li>
<li><a href="0122-otlp-http-json.html#all-other-responses">All Other Responses</a></li>
</ul>
</li>
<li><a href="0122-otlp-http-json.html#connection">Connection</a></li>
<li><a href="0122-otlp-http-json.html#parallel-connections">Parallel Connections</a></li>
</ul>
</li>
</ul>
<h2><a class="header" href="#motivation-29" id="motivation-29">Motivation</a></h2>
<p>Protobuf is a relatively big dependency, which some clients are not willing to take. For example, webjs, iOS/Android (in some scenarios, the size of the installation package is limited, do not want to introduce protobuf dependencies). Plain JSON is a smaller dependency and is built in the standard libraries of many programming languages.</p>
<h2><a class="header" href="#otlphttpjson-protocol-details" id="otlphttpjson-protocol-details">OTLP/HTTP+JSON Protocol Details</a></h2>
<p>OTLP/HTTP+JSON will be consistent with the <a href="0099-otlp-http.html">OTLP/HTTP</a> specification except that the payload will use JSON instead of protobuf.</p>
<h3><a class="header" href="#json-mapping" id="json-mapping">JSON Mapping</a></h3>
<p>Use proto3 standard defined <a href="https://developers.google.com/protocol-buffers/docs/proto3#json">JSON Mapping</a> for mapping between protobuf and json. <code>trace_id</code>  and <code>span_id</code> is base64 encoded in OTLP/HTTP+JSON, not hex.</p>
<h3><a class="header" href="#request-1" id="request-1">Request</a></h3>
<p>Telemetry data is sent via HTTP POST request.</p>
<p>The default URL path for requests that carry trace data is <code>/v1/traces</code> (for
example the full URL when connecting to &quot;example.com&quot; server will be
<code>https://example.com/v1/traces</code>). The request body is a JSON-encoded
<a href="https://github.com/open-telemetry/opentelemetry-proto/blob/e6c3c4a74d57f870a0d781bada02cb2b2c497d14/opentelemetry/proto/collector/trace/v1/trace_service.proto#L38"><code>ExportTraceServiceRequest</code></a>
message.</p>
<p>The default URL path for requests that carry metric data is <code>/v1/metrics</code> and the
request body is a JSON-encoded
<a href="https://github.com/open-telemetry/opentelemetry-proto/blob/e6c3c4a74d57f870a0d781bada02cb2b2c497d14/opentelemetry/proto/collector/metrics/v1/metrics_service.proto#L35"><code>ExportMetricsServiceRequest</code></a>
message.</p>
<p>The client MUST set &quot;Content-Type: application/json&quot; request header. The
client MAY gzip the content and in that case SHOULD include &quot;Content-Encoding:
gzip&quot; request header. The client MAY include &quot;Accept-Encoding: gzip&quot; request
header if it can receive gzip-encoded responses.</p>
<p>Non-default URL paths for requests MAY be configured on the client and server
sides.</p>
<h3><a class="header" href="#response-1" id="response-1">Response</a></h3>
<h4><a class="header" href="#success-1" id="success-1">Success</a></h4>
<p>On success the server MUST respond with <code>HTTP 200 OK</code>. Response body MUST be
JSON-encoded
<a href="https://github.com/open-telemetry/opentelemetry-proto/blob/e6c3c4a74d57f870a0d781bada02cb2b2c497d14/opentelemetry/proto/collector/trace/v1/trace_service.proto#L47"><code>ExportTraceServiceResponse</code></a>
message for traces and
<a href="https://github.com/open-telemetry/opentelemetry-proto/blob/e6c3c4a74d57f870a0d781bada02cb2b2c497d14/opentelemetry/proto/collector/metrics/v1/metrics_service.proto#L44"><code>ExportMetricsServiceResponse</code></a>
message for metrics.</p>
<p>The server MUST set &quot;Content-Type: application/json&quot; response header. If
the request header &quot;Accept-Encoding: gzip&quot; is present in the request the server
MAY gzip-encode the response and set &quot;Content-Encoding: gzip&quot; response header.</p>
<p>The server SHOULD respond with success no sooner than after successfully
decoding and validating the request.</p>
<h4><a class="header" href="#failures-1" id="failures-1">Failures</a></h4>
<p>If the processing of the request fails the server MUST respond with appropriate
<code>HTTP 4xx</code> or <code>HTTP 5xx</code> status code. See sections below for more details about
specific failure cases and HTTP status codes that should be used.</p>
<p>Response body for all <code>HTTP 4xx</code> and <code>HTTP 5xx</code> responses MUST be a
JSON-encoded
<a href="https://godoc.org/google.golang.org/genproto/googleapis/rpc/status#Status">Status</a>
message that describes the problem.</p>
<p>This specification does not use <code>Status.code</code> field and the server MAY omit
<code>Status.code</code> field. The clients are not expected to alter their behavior based
on <code>Status.code</code> field but MAY record it for troubleshooting purposes.</p>
<p>The <code>Status.message</code> field SHOULD contain a developer-facing error message as
defined in <code>Status</code> message schema.</p>
<p>The server MAY include <code>Status.details</code> field with additional details. Read
below about what this field can contain in each specific failure case.</p>
<h4><a class="header" href="#bad-data-1" id="bad-data-1">Bad Data</a></h4>
<p>If the processing of the request fails because the request contains data that
cannot be decoded or is otherwise invalid and such failure is permanent then the
server MUST respond with <code>HTTP 400 Bad Request</code>. The <code>Status.details</code> field in
the response SHOULD contain a
<a href="https://github.com/googleapis/googleapis/blob/d14bf59a446c14ef16e9931ebfc8e63ab549bf07/google/rpc/error_details.proto#L166">BadRequest</a>
that describes the bad data.</p>
<p>The client MUST NOT retry the request when it receives <code>HTTP 400 Bad Request</code>
response.</p>
<h4><a class="header" href="#throttling-2" id="throttling-2">Throttling</a></h4>
<p>If the server receives more requests than the client is allowed or the server is
overloaded the server SHOULD respond with <code>HTTP 429 Too Many Requests</code> or
<code>HTTP 503 Service Unavailable</code> and MAY include
<a href="https://tools.ietf.org/html/rfc7231#section-7.1.3">&quot;Retry-After&quot;</a> header with a
recommended time interval in seconds to wait before retrying.</p>
<p>The client SHOULD honour the waiting interval specified in &quot;Retry-After&quot; header
if it is present. If the client receives <code>HTTP 429</code> or <code>HTTP 503</code> response and
&quot;Retry-After&quot; header is not present in the response then the client SHOULD
implement an exponential backoff strategy between retries.</p>
<h4><a class="header" href="#all-other-responses-1" id="all-other-responses-1">All Other Responses</a></h4>
<p>All other HTTP responses that are not explicitly listed in this document should
be treated according to HTTP specification.</p>
<p>If the server disconnects without returning a response the client SHOULD retry
and send the same request. The client SHOULD implement an exponential backoff
strategy between retries to avoid overwhelming the server.</p>
<h3><a class="header" href="#connection-1" id="connection-1">Connection</a></h3>
<p>If the client is unable to connect to the server the client SHOULD retry the
connection using exponential backoff strategy between retries. The interval
between retries must have a random jitter.</p>
<p>The client SHOULD keep the connection alive between requests.</p>
<p>Server implementations MAY handle OTLP/gRPC, OTLP/HTTP requests and OTLP/HTTP+JSON on the same
port and multiplex the connections to the corresponding transport handler based
on &quot;Content-Type&quot; request header.</p>
<h3><a class="header" href="#parallel-connections-1" id="parallel-connections-1">Parallel Connections</a></h3>
<p>To achieve higher total throughput the client MAY send requests using several
parallel HTTP connections. In that case the maximum number of parallel
connections SHOULD be configurable.</p>
<h1><a class="header" href="#a-proposal-for-sdk-support-for-configurable-batching-and-aggregations-basic-views" id="a-proposal-for-sdk-support-for-configurable-batching-and-aggregations-basic-views">A Proposal For SDK Support for Configurable Batching and Aggregations (Basic Views)</a></h1>
<p>Add support to the default SDK for the ability to configure Metric Aggregations.</p>
<h2><a class="header" href="#motivation-30" id="motivation-30">Motivation</a></h2>
<p>OpenTelemetry's architecture separates the concerns of instrumentation and operation. The Metric Instruments
provided by the Metric API are all defined to have a default aggregation. And, by default, aggregations are
performed with all Labels being used to define a unit of aggregation. Although this is a good default
configuration for the SDK to provide, more configurability is needed.</p>
<p>There are 3 main use-cases that this proposal is intended to address:</p>
<ol>
<li>The application developer/operator wishes to use an aggregation other than the default provided by the SDK
for a given instrument or set of instruments.</li>
<li>An exporter author wishes to inform the SDK what &quot;Temporality&quot; (delta vs. cumulative) the resulting metric
data points represent. &quot;Delta&quot; means only metric recordings since the last reporting interval are considered
in the aggregation, and &quot;Cumulative&quot; means that all metric recordings over the lifetime of the Instrument are
considered in the aggregation.</li>
<li>The application developer/operator wishes to constrain the cardinality of labels for metrics being reported
to the metric vendor/backend of choice.</li>
</ol>
<h2><a class="header" href="#explanation-20" id="explanation-20">Explanation</a></h2>
<p>I propose a new feature for the default SDK, available on the interface of the SDK's MeterProvider implementation, to configure
the batching strategies and aggregations that will be used by the SDK when metric recordings are made. This is the beginnings
of a &quot;Views&quot; API, but does not intend to implement the full View functionality from OpenCensus.</p>
<p>The basic API has two parts.</p>
<ul>
<li>InstrumentSelector - Enables specifying the selection of one or more instruments for the configuration to apply to.
<ul>
<li>Selection options include: the instrument type (Counter, ValueRecorder, etc), and a regex for instrument name.</li>
<li>If more than one option is provided, they are considered additive.</li>
<li>Example: select all ValueRecorders whose name ends with &quot;.duration&quot;.</li>
</ul>
</li>
<li>View - configures how the batching and aggregation should be done.
<ul>
<li>3 things can be specified: The aggregation (Sum, MinMaxSumCount, Histogram, etc), the &quot;temporality&quot; of the batching,
and a set of pre-defined labels to consider as the subset to be used for aggregations.
<ul>
<li>Note: &quot;temporality&quot; can be one of &quot;DELTA&quot; and &quot;CUMULATIVE&quot; and specifies whether the values of the aggregation
are reset after a collection is done or not, respectively.</li>
</ul>
</li>
<li>If not all are specified, then the others should be considered to be requesting the default.</li>
<li>Examples:
<ul>
<li>Use a MinMaxSumCount aggregation, and provide delta-style batching.</li>
<li>Use a Histogram aggregation, and only use two labels &quot;route&quot; and &quot;error&quot; for aggregations.</li>
<li>Use a quantile aggregation, and drop all labels when aggregating.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In this proposal, there is only one View associated with each selector.</p>
<p>As a concrete example, in Java, this might look something like this:</p>
<pre><code class="language-java"> // get a handle to the MeterSdkProvider (note, this is concrete name of the default SDK class in java, not a general SDK)
 MeterSdkProvider meterProvider = OpenTelemetrySdk.getMeterProvider();

 // create a selector to select which instruments to customize:
 InstrumentSelector instrumentSelector = InstrumentSelector.newBuilder()
  .instrumentType(InstrumentType.COUNTER)
  .build();

 // create a configuration of how you want the metrics aggregated:
 View view =
      View.create(Aggregations.minMaxSumCount(), Temporality.DELTA);

 //register the configuration with the MeterSdkProvider
 meterProvider.registerView(instrumentSelector, view);
</code></pre>
<h2><a class="header" href="#internal-details-19" id="internal-details-19">Internal details</a></h2>
<p>This OTEP does not specify how this should be implemented in a particular language, only the functionality that is desired.</p>
<p>A prototype with a partial implementation of this proposal in Java is available in PR form <a href="https://github.com/open-telemetry/opentelemetry-java/pull/1412">here</a></p>
<h2><a class="header" href="#trade-offs-and-mitigations-19" id="trade-offs-and-mitigations-19">Trade-offs and mitigations</a></h2>
<p>This does not intend to deliver a full &quot;Views&quot; API, although it is the basis for one. The goal here is
simply to allow configuration of the batching and aggregation by operators and exporter authors.</p>
<p>This does not intend to specify the exact interface for providing these configurations, nor does it
consider a non-programmatic configuration option.</p>
<h2><a class="header" href="#prior-art-and-alternatives-21" id="prior-art-and-alternatives-21">Prior art and alternatives</a></h2>
<ul>
<li>Prior Art is probably mostly in the <a href="https://opencensus.io/stats/view/">OpenCensus Views</a> system.</li>
<li>Another <a href="https://github.com/open-telemetry/oteps/pull/89">OTEP</a> attempted to address building a Views API.</li>
</ul>
<h2><a class="header" href="#open-questions-to-be-resolved-in-an-official-specification" id="open-questions-to-be-resolved-in-an-official-specification">Open questions (to be resolved in an official specification)</a></h2>
<ol>
<li>Should custom aggregations be allowable for all instruments? How should an SDK respond to a request for a non-supported aggregation?</li>
<li>Should the requesting of DELTA vs. CUMULATIVE be only available via an exporter-only API, rather than generally available to all operators?</li>
<li>Is regex-based name matching too broad and dangerous? Would the alternative (having to know the exact name of all instruments to configure) be too onerous?</li>
<li>Is there anything in this proposal that would make implementing a full Views API (i.e. having multiple, named aggregations per instrument) difficult?</li>
<li>How should an exporter interact with the SDK for which it is configured, in order to change aggregation settings?</li>
<li>Should the first implementation include label reduction, or should that be done in a follow-up OTEP/spec?</li>
<li>Does this support disabling an aggregation altogether, and if so, what is the interface for that?</li>
<li>What is the precedence of selectors, if more than one selector can apply to a given Instrument?</li>
</ol>
<h2><a class="header" href="#future-possibilities-8" id="future-possibilities-8">Future possibilities</a></h2>
<p>What are some future changes that this proposal would enable?</p>
<ul>
<li>A full-blown views API, which would allow multiple &quot;views&quot; per instrument. It's unclear how an exporter would specify which one it wanted, or if it would all the generated metrics.</li>
<li>Additional non-programmatic configuration options.</li>
</ul>
<h1><a class="header" href="#logs-ga-scope" id="logs-ga-scope">Logs GA Scope</a></h1>
<p>This document defines what's in scope for OpenTelemetry Logs General
Availability release.</p>
<h2><a class="header" href="#motivation-31" id="motivation-31">Motivation</a></h2>
<p>Clearly defined scope is important to align all logs contributors and to make
sure we know what target we are working towards. Note that some of the listed
items are already fully or partially implemented but are still listed for
completeness.</p>
<p>General Availability of OpenTelemetry Logs is expected after OpenTelemetry 1.0
GA (which will only include traces and metrics).</p>
<h2><a class="header" href="#logs-roadmap-items" id="logs-roadmap-items">Logs Roadmap Items</a></h2>
<h3><a class="header" href="#specification-and-sdk" id="specification-and-sdk">Specification and SDK</a></h3>
<ul>
<li>
<p>Write guidelines and specification for logging libraries to support
OpenTelemetry-compliant logs.</p>
</li>
<li>
<p>Implement OpenTelemetry logs SDK for Java that support trace context
extraction from the current execution context.</p>
</li>
<li>
<p>Show full OpenTelemetry-compliant implementation of an &quot;addon&quot; to one of the
popular logging libraries for Java (e.g. Log4J, SLF4J, etc). Use OpenTelemetry
SDK to automatically include extracted trace context in the logs and support
exporting via OTLP.</p>
</li>
<li>
<p>Implement an example Java application that shows how to emit correlated traces
and logs. Use the supported popular logging library together with
OpenTelemetry SDK, and export logs using OTLP.</p>
</li>
<li>
<p>Add Logs support to OTLP specification.</p>
</li>
</ul>
<h3><a class="header" href="#collector" id="collector">Collector</a></h3>
<ul>
<li>
<p>Implement receiver and exporter for OTLP logs in Collector.</p>
</li>
<li>
<p>Implement &quot;array&quot; value type for in-memory data structures (it is already part
of OTLP but is not supported by the Collector yet).</p>
</li>
<li>
<p>Implement log exporters in Collector for a few vendor formats from
participating vendors.</p>
</li>
<li>
<p>Implement Fluent Forward protocol receiver to receive logs from
FluentBit/FluentD.</p>
</li>
<li>
<p>Add support for log data type to the following processors: <code>resource</code>,
<code>batch</code>, <code>attributes</code>, <code>k8s_tagger</code>, <code>resourcedetection</code>.</p>
</li>
<li>
<p>Add end-to-end performance tests for log forwarding (similar to existing trace
and metric tests) at least for OTLP and Fluent Forward protocols.</p>
</li>
<li>
<p>Test operation of Collector together with at least one other logging agent
(e.g. FluentBit), allowing to read file logs as described here. Publish test
results (including performance).</p>
</li>
<li>
<p>Implement an example that shows how to use OpenTelemetry Collector to collect
correlated logs, traces and metrics from a distributed microservices
application (preferably running in a cloud-native control plane like
Kubernetes)</p>
</li>
</ul>
<h1><a class="header" href="#otlp-exporters-configurable-export-behavior" id="otlp-exporters-configurable-export-behavior">OTLP Exporters Configurable Export Behavior</a></h1>
<p>Add support for configurable export behavior in OTLP exporters.</p>
<p>The expected behavior required are 1) exporting cumulative values since start time by default, and 2) exporting delta values per collection interval when configured.</p>
<h2><a class="header" href="#motivation-32" id="motivation-32">Motivation</a></h2>
<ol>
<li><strong>Export behavior should be configurable</strong>: Metric backends such as Prometheus, Cortex and other backends supporting Prometheus time-series that ingest data from the Prometheus remote write API, require cumulative values for cumulative metrics and additive metrics, per collection interval. In order to export metrics generated by the SDK using the Collector, incoming values from the SDK should be cumulative values. Note than in comparison, backends like Statsd expect delta values for each collection interval. To support different backend requirements, OTLP metric export behavior needs to be configurable, with cumulative values exported as a default. See discussion in <a href="https://github.com/open-telemetry/opentelemetry-specification/issues/731">#731</a>.</li>
<li><strong>Cumulative export should be the default behavior since it is more reliable</strong>: Cumulative export also addresses the problem of missing delta values for an UpDownCounter. The final consumer of the UpDownCounter metrics is almost always interested in the cumulative value. If the Metrics SDK exports deltas and allows the consumer aggregate cumulative values, then any deltas lost in-transit will lead to inaccurate final values. This loss may impact the condition on which an alert is fired or not. On the other hand, exporting cumulative values guarantees only resolution is lost, but the value received by the final consumer will be correct eventually.
<ol>
<li><em>Note:</em> The <a href="https://docs.google.com/document/d/1LfDVyBJlIewwm3a0JtDtEjkusZjzQE3IAix8b0Fxy3Y/edit#heading=h.fxqkpi2ya3br">Metrics SIG</a> <em>July 23 and July 30 meetings concluded that cumulative export behavior is more reliable.</em> For example, Bogdan Drutu in <a href="https://github.com/open-telemetry/opentelemetry-specification/issues/725">#725</a> notes “When exporting delta values of an UpdownCounter instrument, the export pipeline becomes a single point of failure for the alerts, any dropped &quot;delta&quot; will influence the &quot;current&quot; value of the metric in an undefined way.&quot;</li>
</ol>
</li>
</ol>
<h2><a class="header" href="#explanation-21" id="explanation-21">Explanation</a></h2>
<p>In order to support Prometheus backends using cumulative values as well as other backends that use delta values, the SDK  needs to be configurable and support an OTLP exporter which handles both cumulative values by default and delta values for export. The implication is that the OTLP metric protocol should support both cumulative and delta reporting strategies.</p>
<p>Users should be allowed to declare an environment variable or configuration field that determines this setting for OTLP exporters.</p>
<h2><a class="header" href="#internal-details-20" id="internal-details-20">Internal details</a></h2>
<p>OTLP exporters can report using the behavior it needs to the Metrics SDK. The SDK can merge the previous state of metrics with current value and return the appropriate values to the exporter.</p>
<p>Configurable export behavior is already coded in the Metrics Processor component in the <a href="https://github.com/open-telemetry/opentelemetry-go/pull/840">Go SDK</a>. However, this functionality is hardcoded today and would need to rewritten to handle user-defined configuration. See the OTLP metrics definition in <a href="https://github.com/open-telemetry/opentelemetry-proto/pull/193">PR #193</a>, which support both export behaviors.</p>
<h2><a class="header" href="#trade-offs-and-mitigations-20" id="trade-offs-and-mitigations-20">Trade-offs and mitigations</a></h2>
<p>High memory usage: To support cumulative exports, the SDK needs to maintain state for each cumulative metrics. This means users with high-cardinality metrics can experience high memory usage.</p>
<p>The high-cardinality metrics use case could be addressed by adding the metrics aggregation processor in the Collector. This would enable the Collector, when configured as an Agent, to support converting delta OTLP to Cumulative OTLP. This functionality requires a single agent for each metric-generating client so that all delta values of a metric are converted by the same Collector instance.</p>
<h2><a class="header" href="#prior-art-and-alternatives-22" id="prior-art-and-alternatives-22">Prior art and alternatives</a></h2>
<p>A discussed solution is to convert deltas to cumulative in the Collector both as an agent and as a standalone service. However, supporting conversion in the Collector when it is a standalone service requires implementation of a routing mechanism across all Collector instances to ensure delta values of the same cumulative metric are aggregated by the same Collector instance.</p>
<h2><a class="header" href="#open-questions-14" id="open-questions-14">Open questions</a></h2>
<p>As stated in the previous section, delta to cumulative conversion in the Collector is needed to support Prometheus type backends. This may be necessary in the Collector in the future because the Collector may also accept metrics from other sources that report delta values. On the other hand, if sources are reporting cumulative values, cumulative to delta conversion is needed to support Statsd type backends.</p>
<p>The future implementation for conversions in the Collector is still under discussion. There is a proposal is to add a <a href="https://github.com/open-telemetry/opentelemetry-collector/issues/1422">Metric Aggregation Processor</a> in the Collector which recommends a solution for delta to cumulative conversion.</p>
<h2><a class="header" href="#future-possibilities-9" id="future-possibilities-9">Future possibilities</a></h2>
<p>A future improvement that could be considered is to support a dynamic configuration from a configuration server that determines the appropriate export strategy of OTLP clients at startup.</p>
<h1><a class="header" href="#error-flagging-with-status-codes" id="error-flagging-with-status-codes">Error Flagging with Status Codes</a></h1>
<p>This proposal reduces the number of status codes to three, adds a new field to identify status codes set by application developers and operators, and adds a mapping of semantic conventions to status codes. This clarifies how error reporting should work in OpenTelemetry.</p>
<p>Note: The term <strong>end user</strong> in this document is defined as the application developers and operators of the system running OpenTelemetry. The term <strong>instrumentation</strong> refers to <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/glossary.md#instrumentation-library">instrumentation libraries</a> for common code shared between different systems, such as web frameworks and database clients.</p>
<h2><a class="header" href="#motivation-33" id="motivation-33">Motivation</a></h2>
<p>Error reporting is a fundamental use case for distributed tracing. While we prefer that error flagging occurs within analysis tools, and not within instrumentation, a number of currently supported analysis tools and protocols rely on the existence of an explicit error flag reported from instrumentation. In OpenTelemetry, the error flag is called &quot;status codes&quot;.</p>
<p>However, there is confusion over the mapping of semantic conventions to status codes, and concern over the subjective nature of errors. Which network failures count as an error? Are 404s an error? The answer is often dependent on the situation, but without even a baseline of suggested status codes for each convention, the instrumentation author is placed under the heavy burden of making the decision. Worse, the decisions will not be in sync across different instrumentation.</p>
<p>There is one other missing piece, required for proper error flagging. Both application developers and operators have a deep understanding of what constitutes an error in their system. OpenTelemetry must provide a way for these users to control error flagging, and explicitly indicate that it is the end user setting the status code, and not instrumentation. In these specific cases, the error flagging is known to be correct: the end user has decided the status of the span, and they do not want another interpretation.</p>
<p>While generic instrumentation can only provide a generic schema, end users are capable of making subjective decisions about their systems. And, as the end user, they should get to have the final call in what constitutes an error. In order to accomplish this, there must be a way to differentiate between errors flagged by instrumentation, and errors flagged by the end user.</p>
<h2><a class="header" href="#explanation-22" id="explanation-22">Explanation</a></h2>
<p>The following changes add several missing features required for proper error reporting, and are completely backwards compatible with OpenTelemetry today.</p>
<h3><a class="header" href="#status-codes" id="status-codes">Status Codes</a></h3>
<p>Currently, OpenTelemetry does not have a use case for differentiating between different types of errors. However, this use case may appear in the future. For now, we would like to reduce the number of status codes, and then add them back in as the need becomes clear. We would also like to differentiate between status codes which have not been
set, and an explicit OK status set by an end user.</p>
<ul>
<li><code>UNSET</code> is the default status code.</li>
<li><code>ERROR</code> represents all error types.</li>
<li><code>OK</code> represents a span which has been explicitly marked as being free of errors, and should not be counted against an error budget. Note that only end users should set this status. Instead, instrumentation should leave the status as <code>UNSET</code> for operations that do not generate an error.</li>
</ul>
<h3><a class="header" href="#status-source" id="status-source"><code>Status Source</code></a></h3>
<p>A new Status Source field identifies the origin of the status code on the span. This is important, as statuses set by application developers and operators have been confirmed by the end user to be correct to the particular situation. Statuses set by instrumentation, on the other hand, are only following a generic schema.</p>
<ul>
<li><code>INSTRUMENTATION</code> is the default source. This is used for instrumentation contained within shared code, such as OSS libraries and frameworks. All instrumentation plugins shipped with OpenTelemetry use this status code.</li>
<li><code>USER</code> identifies statuses set by application developers or operators, either in application code or the collector.</li>
</ul>
<p>Analysis tools MAY disregard status codes, in favor of their own approach to error analysis. However, it is strongly suggested that analysis tools SHOULD pay attention to the status codes when set by <code>USER</code>, as it is a communication from the application developer or operator and contains valuable information.</p>
<h3><a class="header" href="#status-mapping-schema" id="status-mapping-schema">Status Mapping Schema</a></h3>
<p>As part of the specification, OpenTelemetry provides a mapping of semantic conventions to status codes. This removes any ambiguity as to what OpenTelemetry ships with out of the box.</p>
<p>Including the correct status codes as part of our semantic conventions will help ensure our instrumentation is consistent when errors relate to a cross-language concept, such as a database protocol.</p>
<p>Please note that semantic conventions, and thus status mapping from conventions, are still a work in progress and will continue to change after GA.</p>
<h3><a class="header" href="#status-processor" id="status-processor">Status Processor</a></h3>
<p>The collector will provide a processor and a configuration language to make adjustments to this status mapping schema. This provides the flexibility and customization needed for real world scenarios.</p>
<h3><a class="header" href="#convenience-methods" id="convenience-methods">Convenience methods</a></h3>
<p>As a convenience, OpenTelemetry provides helper functions for adding semantic conventions and exceptions to a span. These helper functions will also set the correct status code. This simplifies the life of the instrumentation author, and helps ensure compliance and data quality.</p>
<p>Note that these convenience methods simply wire together multiple API calls. They should live in a helper package, and should not be directly added to existing API interfaces. Given how many semantic conventions we have, there will be a pile of them.</p>
<h2><a class="header" href="#internal-details-21" id="internal-details-21">Internal details</a></h2>
<p>This proposal is mostly backwards compatible with existing code, protocols, and the OpenTracing bridge. The only potential exception is the removal of status code enums from the current OTLP protocol, and the rewriting of the small number of instrumentation that were making use of them.</p>
<h2><a class="header" href="#but-errors-are-subjective-how-can-we-know-what-is-an-error-who-are-we-to-define-this" id="but-errors-are-subjective-how-can-we-know-what-is-an-error-who-are-we-to-define-this">BUT ERRORS ARE SUBJECTIVE!! HOW CAN WE KNOW WHAT IS AN ERROR? WHO ARE WE TO DEFINE THIS?</a></h2>
<p>First of all, every tracing system to-date comes with a default set of errors. No system requires that end users start completely from scratch. So... be calm!! Have faith!!</p>
<p>While flagging errors can be a subjective decision, it is true that many semantic conventions qualify as an error. By providing a default mapping of semantic conventions to errors, we ensure compatibility with existing analysis tools (e.g. Jaeger), and provide guidance to users and future implementers.</p>
<p>Obviously, all systems are different, and users will want to adjust error reporting on a case by case basis. Unwanted errors may be suppressed, and additional errors may be added. The collector will provide a processor and a configuration language to make this a straightforward process. Working from a baseline of standard errors will provide a better experience than having to define a schema from scratch.</p>
<p>Note that analysis tools MAY disregard Span Status, and do their own error analysis. There is no requirement that the status code is respected, even when Status Source is set. However, it is strongly suggested that analysis tools SHOULD pay attention to the status code when Status Source is set, as it represents a subjective decision made by either the operator or application developer.</p>
<h2><a class="header" href="#remind-me-why-we-need-status-codes-again" id="remind-me-why-we-need-status-codes-again">Remind me why we need status codes again?</a></h2>
<p>Status codes provide a low overhead mechanism for checking if a span counts against an error budget, without having to scan every attribute and event. It is an inexpensive and low cardinality approach to track multiple types of error budgets. This reduces overhead and could be a benefit for many systems.</p>
<p>However, adding in an existing set of error types without first clearly defining their use and how they might be set has caused confusion. If the status codes are not set consistently and correctly, then the resulting error budgeting will not be useful. So we are consolidating all error types into a single ERROR type, to avoid this situation. We may add more error types back in if we can agree on their use cases and a method for applying them consistently.</p>
<h2><a class="header" href="#open-questions-15" id="open-questions-15">Open questions</a></h2>
<p>If we add error processing to the Collector, it is unclear what the overhead would be.</p>
<p>It is also unclear what the cost is for backends to scan for errors on every span, without a hint from instrumentation that an error might be present.</p>
<h2><a class="header" href="#prior-art-and-alternatives-23" id="prior-art-and-alternatives-23">Prior art and alternatives</a></h2>
<p>In OpenTracing, the lack of a Collector and status mapping schema proved to be unwieldy. It placed a burden on instrumentation plugin authors to set the error flag correctly, and led to an explosion of non-standardized configuration options in every plugin just to adjust the default error flagging. This in turn placed a configuration burden on application developers.</p>
<p>An alternative is the <code>error.hint</code> proposal, paired with the removal of status code. This would work, but essentially provides the same mechanism provided in this proposal, only with a large number of breaking changes. It also does not address the need for user overrides.</p>
<h2><a class="header" href="#future-work-1" id="future-work-1">Future Work</a></h2>
<p>The inclusion of status codes and status mappings help the OpenTelemetry community speak the same language in terms of error reporting. It lifts the burden on future analysis tools, and (when respected) it allows users to employ multiple analysis tools without having to synchronize an important form of configuration across multiple tools.</p>
<p>In the future, OpenTelemetry may add a control plane which allows dynamic configuration of the status mapping schema.</p>
<h1><a class="header" href="#versioning-and-stability-for-opentelemetry-clients" id="versioning-and-stability-for-opentelemetry-clients">Versioning and stability for OpenTelemetry clients</a></h1>
<p>OpenTelemetry is a large project with strict compatibility requirements. This proposal defines the stability guarantees offered by the OpenTelemetry clients, along with a versioning and lifecycle proposal which defines how we meet those requirements.</p>
<p>Language implementations are expected to follow this proposal exactly, unless a language or package manager convention interferes significantly. Implementations must take this cross-language proposal, and produce a language-specific proposal which details how these requirements will be met.</p>
<p>Note: In this document, the term OpenTelemetry specifically refers to the OpenTelemetry clients. It does not refer to the specification or the Collector.</p>
<h2><a class="header" href="#design-goals" id="design-goals">Design goals</a></h2>
<p><strong>Ensure that end users stay up to date with the latest release.</strong>
We want all users to stay up to date with the latest version of OpenTelemetry. We do not want to create hard breaks in support, of any kind, which leave users stranded on older versions. It must always be possible to upgrade to the latest minor version of OpenTelemetry, without creating compilation or runtime errors.</p>
<p><strong>Never create a dependency conflict between packages which rely on different versions of OpenTelemetry. Avoid breaking all stable public APIs.</strong>
Backwards compatibility is a strict requirement. Instrumentation APIs cannot create a version conflict, ever. Otherwise, OpenTelemetry cannot be embedded in widely shared libraries, such as web frameworks. Code written against older versions of the API must work with all newer versions of the API. Transitive dependencies of the API cannot create a version conflict. The OpenTelemetry API cannot depend on &quot;foo&quot; if there is any chance that any library or application may require a different, incompatible version of &quot;foo.&quot; A library using OpenTelemetry should never become incompatible with other libraries due to a version conflict in one of OpenTelemetry's dependencies. Theoretically, APIs can be deprecated and eventually removed, but this is a process measured in years and we have no plans to do so.</p>
<p><strong>Allow for multiple levels of package stability within the same release.</strong>
Provide maintainers a clear process for developing new, experimental APIs alongside stable APIs. DIfferent packages within the same release may have different levels of stability. This means that an implementation wishing to release stable tracing today must ensure that experimental metrics are factored out in such a way that breaking changes to metrics API do not destabilize the trace API packages.</p>
<h2><a class="header" href="#relevant-architecture" id="relevant-architecture">Relevant architecture</a></h2>
<p><img src="img/0143_cross_cutting.png" alt="Cross cutting concerns" /></p>
<p>At the highest architectural level, OpenTelemetry is organized into signals. Each signal provides a specialized form of observability. For example, tracing, metrics, and baggage are three separate signals. Signals share a common subsystem – context propagation – but they function independently from each other.</p>
<p>Each signal provides a mechanism for software to describe itself. A codebase, such as an API handler or a database client, takes a dependency on various signals in order to describe itself. OpenTelemetry instrumentation code is then mixed into the other code within that codebase. This makes OpenTelemetry a <strong>cross-cutting concern</strong> - a piece of software which must be mixed into many other pieces of software in order to provide value. Cross-cutting concerns, by their very nature, violate a core design principle – separation of concerns. As a result, OpenTelemetry requires extra care and attention to avoid creating issues for the codebase which depend upon these cross-cutting APIs.</p>
<p>OpenTelemetry is designed to separate the portion of each signal which must be imported as cross-cutting concerns from the portions of OpenTelemetry which can be managed independently. OpenTelemetry is also designed to be an extensible framework. To accomplish this these goals, each signal consists of four types of packages:</p>
<p><strong>API -</strong> API packages consist of the cross-cutting public interfaces used for instrumentation. Any portion of OpenTelemetry which 3rd-party libraries and application code depend upon is considered part of the API. To manage different levels of stability, every signal has its own, independent API package. These individual APIs may also be bundled up into a shared global API, for convenience.</p>
<p><strong>SDK -</strong> The implementation of the API. The SDK is managed by the application owner. Note that the SDKs includes additional public interfaces which are not considered part of the API package, as they are not cross-cutting concerns. These public interfaces are defined as <strong>constructors</strong> and <strong>plugin interfaces</strong>. Examples of plugin interfaces include the SpanProcessor, Exporter, and Sampler interfaces. Examples of constructors include configuration objects, environment variables, and SDK builders. Application owners may interact with SDK constructors; plugin authors may interact with SDK plugin interfaces. Instrumentation authors must never directly reference any SDK package of any kind, only the API.</p>
<p><strong>Semantic Conventions -</strong> A schema defining the attributes which describe common concepts and operations which the signal observes. Note that unlike the API or SDK, stable conventions for all signals may be placed in the same package, as they are often useful across different signals.</p>
<p><strong>Contrib –</strong> plugins and instrumentation that make use of the API or SDK interfaces, but are not part of the core packages necessary for running OTel. The term &quot;contrib&quot; specifically refers to the plugins and instrumentation maintained by the OpenTelemetry organization outside of the SDK; it does not refer to third party plugins hosted elsewhere, or core plugins which are required to be part of the SDK release, such as OTLP Exporters and TraceContext Propagators. <strong>API Contrib</strong> refers to packages which depend solely upon the API; <strong>SDK Contrib</strong> refers to packages which also depend upon the SDK.</p>
<h2><a class="header" href="#signal-lifecycle" id="signal-lifecycle">Signal lifecycle</a></h2>
<p>OpenTelemetry is structured around signals. Each signal represents a coherent, stand-alone set of functionality. Each signal follows a lifecycle.</p>
<p><img src="img/0143_api_lifecycle.png" alt="API Lifecycle" /></p>
<h3><a class="header" href="#lifecycle-stages" id="lifecycle-stages">Lifecycle stages</a></h3>
<p><strong>Experimental –</strong> Breaking changes and performance issues may occur. Components may not be feature-complete. The experiment may be discarded.</p>
<p><strong>Stable –</strong> Stability guarantees apply, based on component type (API, SDK, Conventions, and Contrib). Long term dependencies may now be taken against these packages.</p>
<p><strong>Deprecated –</strong> this signal has been replaced but is still retains the same stability guarantees.</p>
<p><strong>Removed -</strong> a deprecated signal is no longer supported, and is removed.</p>
<p>All signal components may become stable together, or one by one in the following order: API, Semantic Conventions, API Contrib, SDK, SDK Contrib.</p>
<p>When transitioning from experimental to stable to deprecated, packages <strong>should not move or otherwise break how they are imported by users</strong>. Do NOT use and &quot;experimental&quot; directory or package suffix.</p>
<p>Optionally, package <strong>version numbers</strong> MAY include a suffix, such as -alpha, -beta, -rc, or -experimental, to differentiate stable and experimental packages.</p>
<h3><a class="header" href="#stability" id="stability">Stability</a></h3>
<p>Once a signal component is marked as stable, the following rules apply until the end of that signal’s existence.</p>
<p><strong>API Stability -</strong>
No backward-incompatible changes to the API are allowed unless the major version number is incremented. All existing API calls must continue to compile and function against all future minor versions of the same major version. ABI compatibility for the API may be offered on a language by language basis.</p>
<p><strong>SDK Stability -</strong>
Public portions of the SDK must remain backwards compatible. There are two categories: <strong>plugin interfaces</strong> and <strong>constructors</strong>. Examples of plugins include the SpanProcessor, Exporter, and Sampler interfaces. Examples of constructors include configuration objects, environment variables, and SDK builders.</p>
<p>ABI compatibility for SDK plugin interfaces and constructors may be offered on a language by language basis.</p>
<p><strong>Semantic Conventions Stability -</strong>
Semantic Conventions may not be removed once they are stable. New conventions may be added to replace usage of older conventions, but the older conventions are never removed, they will only be marked as deprecated in favor of the newer ones.</p>
<p><strong>Contrib Stability -</strong>
Plugins and instrumentation are kept up to date, and are released simultaneously (or shortly after) the latest release of the API. The goal is to ensure users can update to the latest version of OpenTelemetry, and not be held back by the plugins that they depend on.</p>
<p>Public portions of contrib packages (constructors, configuration, interfaces) must remain backwards compatible. ABI compatibility for contrib packages may be offered on a language by language basis.</p>
<p>Telemetry produced by contrib instrumentation must also remain stable and backwards compatible, to avoid breaking alerts and dashboards. This means that existing data may not be mutated or removed without a major version bump. Additional data may be added. This applies to spans, metrics, resources, attributes, events, and any other data types that OpenTelemetry emits.</p>
<h3><a class="header" href="#deprecation" id="deprecation">Deprecation</a></h3>
<p>In theory, signals could be replaced. When this happens, they are marked as deprecated.</p>
<p>Code is only marked as deprecated when the replacement becomes stable. Deprecated code still abides by the same support guarantees as stable code. Deprecated APIs remain stable and backwards compatible.</p>
<h3><a class="header" href="#removal" id="removal">Removal</a></h3>
<p>Packages are end-of-life’d by being removed from the release. The release then makes a major version bump.</p>
<p>We currently have no plans for deprecating signals or creating a major version past v1.0.</p>
<p>For clarity, it is still possible to create a new, backwards incompatible version of an existing type of signal without actually moving to v2.0 and breaking support. Allow me to explain.</p>
<p>Imagine we develop a new, better tracing API - let's call it AwesomeTrace. We will never mutate the current tracing API into AwesomeTrace. Instead, AwesomeTrace would be added as an entirely new signal which coexists and interoperates with the current tracing signal. This would make adding AwesomeTrace a minor version bump, <em>not</em> v2.0. v2.0 would mark the end of support for current tracing, not the addition of AwesomeTrace. And we don't want to ever end that support, if we can help it.</p>
<p>This is not actually a theoretical example. OpenTelemetry already supports two tracing APIs: OpenTelemetry and OpenTracing. We invented a new tracing API, but continue to support the old one.</p>
<h2><a class="header" href="#version-numbers" id="version-numbers">Version Numbers</a></h2>
<p>OpenTelemetry follows <a href="https://semver.org/">semver 2.0</a> conventions, with the following distinction.</p>
<p>OpenTelemetry clients have four components: API, Semantic Conventions, SDK, and Contrib.</p>
<p>For the purposes of versioning, all code within a component is treated as if it were part of a single package, and versioned with the same version number, except for Contrib, which may be a collection of packages versioned separately.</p>
<ul>
<li>All packages within the API share the same version number. API packages for all signals version together, across all signals. Signals do not have separate version numbers. There is one version number that applies to all signals that are included in the API release that is labeled with that particular version number.</li>
<li>All packages within the SDK share the same version number. SDK packages for all signals version together, across all signals. There is one version number that applies to all signals that are included in the SDK release that is labeled with that particular version number.</li>
<li>All Semantic Conventions are contained within a single package with a single version number.</li>
<li>Each contrib package has it's own version.</li>
<li>The API, SDK, Semantic Conventions, and contrib components are not required to share a version number. For example, the latest version of <code>opentelemetry-python-api</code> may be at v1.2.3, while the latest version of <code>opentelemetry-python-sdk</code> may be at v2.3.1.</li>
<li>Different language implementations do not need to have matching version numbers. For example, it is fine to have <code>opentelemetry-python-api</code> at v1.2.8 when <code>opentelemetry-java-api</code> is at v1.3.2.</li>
<li>Language implementations do not need to match the version of the specification they implement. For example, it is fine for v1.8.2 of <code>opentelemetry-python-api</code> to implement v1.1.1 of the specification.</li>
</ul>
<p><strong>Exception:</strong> in some languages, package managers may react poorly to experimental packages having a version higher than 0.X. In these cases, a language-specific workaround is required. Go, Ruby, and Javascript are examples.</p>
<p><strong>Major version bump</strong>
Major version bumps only occur when there is a breaking change to a stable interface, or the removal of deprecated signals.</p>
<p>OpenTelemetry values long term support. The expectation is that we will version to v1.0 once the first set of packages are declared stable. OpenTelemetry will then remain at v1.0 for years. There are no plans for a v2.0 of OpenTelemetry at this time. Additional stable packages, such as metrics and logs, will be added as minor version bumps.</p>
<p><strong>Minor version bump</strong>
Most changes to OpenTelemetry result in a minor version bump.</p>
<ul>
<li>New backward-compatible functionality added to any component.</li>
<li>Breaking changes to internal SDK components.</li>
<li>Breaking changes to experimental signals.</li>
<li>New experimental packages are added.</li>
<li>Experimental packages become stable.</li>
</ul>
<p><strong>Patch version bump</strong>
Patch versions make no changes which would require recompilation or potentially break application code. The following are examples of patch fixes.</p>
<ul>
<li>Bug fixes which don't require minor version bump per rules above.</li>
<li>Security fixes.</li>
<li>Documentation.</li>
</ul>
<p>Currently, OpenTelemetry does NOT have plans to backport bug and security fixes to prior minor versions. Security and bug fixes are only applied to the latest minor version. We are committed to making it feasible for end users to stay up to date with the latest version of OpenTelemetry.</p>
<h2><a class="header" href="#long-term-support" id="long-term-support">Long Term Support</a></h2>
<p><img src="img/0143_long_term.png" alt="long term support" /></p>
<h3><a class="header" href="#api-support" id="api-support">API support</a></h3>
<p>Major versions of the API will be supported for a minimum of <strong>three years</strong> after the release of the next major API version. Support covers the following areas.</p>
<p>API stability, as defined above, will be maintained.</p>
<p>A version of the SDK which supports the last major version of the API will continue to be maintained during this period. Bug and security fixes will be backported. Additional feature development is not guaranteed.</p>
<p>Contrib packages available when the API is versioned will continue to be maintained for the duration of this period. Bug and security fixes will be backported. Additional feature development is not guaranteed.</p>
<h3><a class="header" href="#sdk-support" id="sdk-support">SDK Support</a></h3>
<p>SDK stability, as defined above, will be maintained for a minimum of <strong>one year</strong> after after the release of the next major SDK version.</p>
<h3><a class="header" href="#contrib-support" id="contrib-support">Contrib Support</a></h3>
<p>Contrib stability, as defined above, will be maintained for a minimum of <strong>one year</strong> after after the release of the next major version of a contrib package.</p>
<h2><a class="header" href="#opentelemetry-ga" id="opentelemetry-ga">OpenTelemetry GA</a></h2>
<p>The term “OpenTelemetry GA” refers to the point at which a stable version of both tracing and metrics has been released in at least three languages.</p>
<h1><a class="header" href="#scenarios-for-metrics-apisdk-prototyping" id="scenarios-for-metrics-apisdk-prototyping">Scenarios for Metrics API/SDK Prototyping</a></h1>
<p>With the stable release of the tracing specification, the OpenTelemetry
community is willing to spend more energy on metrics API/SDK. The goal is to get
the metrics API/SDK specification to
<a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/versioning-and-stability.md#experimental"><code>Experimental</code></a>
state by end of 5/2021, and make it
<a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/versioning-and-stability.md#stable"><code>Stable</code></a>
before end of 2021:</p>
<ul>
<li>
<p>By end of 5/31/2021, we should have a good confidence that we can recommend
language client owners to work on metrics preview release. This means starting
from 6/1/2021 the specification should not have major surprises or big
changes. We will then start recommending client maintainers to implement it.
We might introduce additional features but there should be a high bar.</p>
</li>
<li>
<p>By end of 9/30/2021, we should mark the metrics API/SDK specification as
<a href="https://github.com/open-telemetry/opentelemetry-specification/blob/1afab39e5658f807315abf2f3256809293bfd421/specification/document-status.md#feature-freeze"><code>Feature-freeze</code></a>,
and focus on bug fixing or editorial changes.</p>
</li>
<li>
<p>By end of 11/30/2021, we want to have a stable release of metrics API/SDK
specification, with multiple language SIGs providing RC (release candidate) or
<a href="https://github.com/open-telemetry/opentelemetry-specification/blob/9047c91412d3d4b7f28b0f7346d8c5034b509849/specification/versioning-and-stability.md#stable">stable</a>
clients.</p>
</li>
</ul>
<p>In this document, we will focus on two scenarios that we use for prototyping
metrics API/SDK. The goal is to have two scenarios which clearly capture the
major requirements, so we can work with language client SIGs to prototype,
gather the learnings, determine the scopes and stages. Later the scenarios can
be used as examples and test cases for all the language clients.</p>
<p>Here are the languages we've agreed to use during the prototyping:</p>
<ul>
<li>C#</li>
<li>Java</li>
<li>Python</li>
</ul>
<p>In order to not undertake such an enormous task at once, we will need to have an incremental approach and divide the work into multiple
stages:</p>
<ol>
<li>
<p>Do the end-to-end prototype to get the overall understanding of the problem
domain. We should also clarify the scope and be able to articulate it
precisely during this stage, here are some examples:</p>
<ul>
<li>Why do we want to introduce brand new metrics APIs versus taking a well
established API (e.g. Prometheus and Micrometer), what makes OpenTelemetry
metrics API different (e.g. Baggage)?</li>
<li>Do we need to consider OpenCensus Stats API shim, or this is out of scope?</li>
</ul>
</li>
<li>
<p>Focus on a core subset of API, cover the end-to-end library instrumentation
scenario. At this stage we don't expect to cover all the APIs as some of them
might be very similar (e.g. if we know how to record an integer, we don't
have to work on float/double as we can add them later by replicating what
we've done for integer).</p>
</li>
<li>
<p>Focus on a core subset of SDK. This would help us to get the end-to-end
application.</p>
</li>
<li>
<p>Replicate stage 2 to cover the complete set of APIs.</p>
</li>
<li>
<p>Replicate stage 3 to cover the complete set of SDKs.</p>
</li>
</ol>
<h2><a class="header" href="#scenario-1-grocery" id="scenario-1-grocery">Scenario 1: Grocery</a></h2>
<p>The <strong>Grocery</strong> scenario covers how a developer could use metrics API and SDK in
a final application. It is a self-contained application which covers:</p>
<ul>
<li>How to instrument the code in a vendor agnostic way</li>
<li>How to configure the SDK and exporter</li>
</ul>
<p>Considering there might be multiple grocery stores, the metrics we collect will
have the store name as a dimension - which is fairly static (not changing while
the store is running).</p>
<p>The store has plenty supply of potato and tomato, with the following price:</p>
<ul>
<li>Potato: $1.00 / ea</li>
<li>Tomato: $3.00 / ea</li>
</ul>
<p>Each customer has a unique name (e.g. customerA, customerB), a customer could
come to the store multiple times. Here goes the Python snippet:</p>
<pre><code class="language-python">store = GroceryStore(&quot;Portland&quot;)
store.process_order(&quot;customerA&quot;, {&quot;potato&quot;: 2, &quot;tomato&quot;: 3})
store.process_order(&quot;customerB&quot;, {&quot;tomato&quot;: 10})
store.process_order(&quot;customerC&quot;, {&quot;potato&quot;: 2})
store.process_order(&quot;customerA&quot;, {&quot;tomato&quot;: 1})
</code></pre>
<p>We will need the following metrics every minute:</p>
<p><strong>Order info:</strong></p>
<table><thead><tr><th>Store</th><th>Customer</th><th>Number of Orders</th><th>Amount (USD)</th></tr></thead><tbody>
<tr><td>Portland</td><td>customerA</td><td>2</td><td>14.00</td></tr>
<tr><td>Portland</td><td>customerB</td><td>1</td><td>30.00</td></tr>
<tr><td>Portland</td><td>customerC</td><td>1</td><td>2.00</td></tr>
</tbody></table>
<p><strong>Items sold:</strong></p>
<table><thead><tr><th>Store</th><th>Customer</th><th>Item</th><th>Count</th></tr></thead><tbody>
<tr><td>Portland</td><td>customerA</td><td>potato</td><td>2</td></tr>
<tr><td>Portland</td><td>customerA</td><td>tomato</td><td>4</td></tr>
<tr><td>Portland</td><td>customerB</td><td>tomato</td><td>10</td></tr>
<tr><td>Portland</td><td>customerC</td><td>potato</td><td>2</td></tr>
</tbody></table>
<h2><a class="header" href="#scenario-2-http-server" id="scenario-2-http-server">Scenario 2: HTTP Server</a></h2>
<p>The <em>HTTP Server</em> scenario covers how a library developer X could use metrics
API to instrument a library, and how the application developer Y can configure
the library to use OpenTelemetry SDK in a final application. X and Y are working
for different companies and they don't communicate. The demo has two parts - the
library (HTTP lib and ClimateControl lib owned by X) and the server app (owned by Y):</p>
<ul>
<li>How developer X could instrument the library code in a vendor agnostic way
<ul>
<li>Performance is critical for X</li>
<li>X doesn't know which metrics and which dimensions will Y pick</li>
<li>X doesn't know the aggregation time window, nor the final destination of the
metrics</li>
<li>X would like to provide some default recommendation (e.g. default
dimensions, aggregation time window, histogram buckets) so consumers of his
library can have a better onboarding experience.</li>
</ul>
</li>
<li>How developer Y could configure the SDK and exporter
<ul>
<li>How should Y hook up the metrics SDK with the library</li>
<li>How should Y configure the time window(s) and destination(s)</li>
<li>How should Y pick the metrics and the dimensions</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#library-requirements" id="library-requirements">Library Requirements</a></h3>
<p>The library developer (developer X) will provide two libraries:</p>
<ul>
<li>Server climate control library - a library which monitors and controls the
temperature and humidity of the server.</li>
<li>HTTP server library - a library which provides HTTP service.</li>
</ul>
<p>Both libraries will provide out-of-box metrics, the metrics have two categories:</p>
<ul>
<li>Push metrics - the value is reported (via the API) when it is available, and
collected (via the SDK) based on the ask from consumer(s). If there is no ask
from the consumer, the API will be no-op and the data will be dropped on the
floor.</li>
<li>Pull metrics - the value is always available, and is only reported and
collected based on the ask from consumer(s). If there is no ask from the
consumer, the value will not be reported at all (e.g. there is no API call to
fetch the temperature unless someone is asking for the temperature).</li>
</ul>
<h4><a class="header" href="#server-climate-control-library" id="server-climate-control-library">Server Climate Control Library</a></h4>
<p>Note: the <strong>Host Name</strong> should leverage <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/resource/sdk.md"><code>OpenTelemetry Resource</code></a>,
so it should be covered by the metrics SDK rather than API, and strictly
speaking it is not considered as a &quot;dimension&quot; from the SDK perspective.</p>
<p><strong>Server temperature:</strong></p>
<table><thead><tr><th>Host Name</th><th>Temperature (F)</th></tr></thead><tbody>
<tr><td>MachineA</td><td>65.3</td></tr>
</tbody></table>
<p><strong>Server humidity:</strong></p>
<table><thead><tr><th>Host Name</th><th>Humidity (%)</th></tr></thead><tbody>
<tr><td>MachineA</td><td>21</td></tr>
</tbody></table>
<h4><a class="header" href="#http-server-library" id="http-server-library">HTTP Server Library</a></h4>
<p><strong>Received HTTP requests:</strong></p>
<p>Note: the <strong>Client Type</strong> is passed in via the <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/baggage/api.md"><code>OpenTelemetry Baggage</code></a>,
strictly speaking it is not part of the metrics API, but it is considered as a
&quot;dimension&quot; from the metrics SDK perspective.</p>
<table><thead><tr><th>Host Name</th><th>Process ID</th><th>Client Type</th><th>HTTP Method</th><th>HTTP Host</th><th>HTTP Flavor</th><th>Peer IP</th><th>Peer Port</th><th>Host IP</th><th>Host Port</th></tr></thead><tbody>
<tr><td>MachineA</td><td>1234</td><td>Android</td><td>GET</td><td>otel.org</td><td>1.1</td><td>127.0.0.1</td><td>51327</td><td>127.0.0.1</td><td>80</td></tr>
<tr><td>MachineA</td><td>1234</td><td>Android</td><td>POST</td><td>otel.org</td><td>1.1</td><td>127.0.0.1</td><td>51328</td><td>127.0.0.1</td><td>80</td></tr>
<tr><td>MachineA</td><td>1234</td><td>iOS</td><td>PUT</td><td>otel.org</td><td>1.1</td><td>127.0.0.1</td><td>51329</td><td>127.0.0.1</td><td>80</td></tr>
</tbody></table>
<p><strong>HTTP server request duration:</strong></p>
<p>Note: the server duration is only available for <strong>finished HTTP requests</strong>.</p>
<table><thead><tr><th>Host Name</th><th>Process ID</th><th>Client Type</th><th>HTTP Method</th><th>HTTP Host</th><th>HTTP Status Code</th><th>HTTP Flavor</th><th>Peer IP</th><th>Peer Port</th><th>Host IP</th><th>Host Port</th><th>Duration (ms)</th></tr></thead><tbody>
<tr><td>MachineA</td><td>1234</td><td>Android</td><td>GET</td><td>otel.org</td><td>200</td><td>1.1</td><td>127.0.0.1</td><td>51327</td><td>127.0.0.1</td><td>80</td><td>8.5</td></tr>
<tr><td>MachineA</td><td>1234</td><td>Android</td><td>POST</td><td>otel.org</td><td>304</td><td>1.1</td><td>127.0.0.1</td><td>51328</td><td>127.0.0.1</td><td>80</td><td>100.0</td></tr>
</tbody></table>
<h3><a class="header" href="#application-requirements" id="application-requirements">Application Requirements</a></h3>
<p>The application owner (developer Y) would only want the following metrics:</p>
<ul>
<li>
<p>Server temperature - reported every 5 seconds</p>
</li>
<li>
<p>Server humidity - reported every minute</p>
</li>
<li>
<p>HTTP server request duration, reported every 5 seconds, with a subset of the
dimensions:</p>
<ul>
<li>Host Name</li>
<li>HTTP Method</li>
<li>HTTP Host</li>
<li>HTTP Status Code</li>
<li>Client Type</li>
<li>90%, 95%, 99% and 99.9% server duration</li>
</ul>
</li>
<li>
<p>HTTP request counters, reported every 5 seconds:</p>
<ul>
<li>Total number of received HTTP requests</li>
<li>Total number of finished HTTP requests</li>
<li>Number of currently-in-flight HTTP requests (concurrent HTTP requests)</li>
</ul>
<table><thead><tr><th>Host Name</th><th>Process ID</th><th>HTTP Host</th><th>Received Requests</th><th>Finished Requests</th><th>Concurrent Requests</th></tr></thead><tbody>
<tr><td>MachineA</td><td>1234</td><td>otel.org</td><td>630</td><td>601</td><td>29</td></tr>
<tr><td>MachineA</td><td>5678</td><td>otel.org</td><td>1005</td><td>1001</td><td>4</td></tr>
</tbody></table>
</li>
<li>
<p>Exception samples (exemplar) - in case HTTP 5xx happened, developer Y would
want to see a sample request with trace id, span id and all the dimensions
(IP, Port, etc.)</p>
<table><thead><tr><th>Trace ID</th><th>Span ID</th><th>Host Name</th><th>Process ID</th><th>Client Type</th><th>HTTP Method</th><th>HTTP Host</th><th>HTTP Status Code</th><th>HTTP Flavor</th><th>Peer IP</th><th>Peer Port</th><th>Host IP</th><th>Host Port</th><th>Exception</th></tr></thead><tbody>
<tr><td>8389584945550f40820b96ce1ceb9299</td><td>745239d26e408342</td><td>MachineA</td><td>1234</td><td>iOS</td><td>PUT</td><td>otel.org</td><td>500</td><td>1.1</td><td>127.0.0.1</td><td>51329</td><td>127.0.0.1</td><td>80</td><td>SocketException(...)</td></tr>
</tbody></table>
</li>
</ul>
<p><a href="introduction.html">Introduction</a></p>
<ul>
<li><a href="0001-telemetry-without-manual-instrumentation.html">0001-telemetry-without-manual-instrumentation</a></li>
<li><a href="0002-remove-spandata.html">0002-remove-spandata</a></li>
<li><a href="0003-measure-metric-type.html">0003-measure-metric-type</a></li>
<li><a href="0005-global-init.html">0005-global-init</a></li>
<li><a href="0006-sampling.html">0006-sampling</a></li>
<li><a href="0007-no-out-of-band-reporting.html">0007-no-out-of-band-reporting</a></li>
<li><a href="0008-metric-observer.html">0008-metric-observer</a></li>
<li><a href="0009-metric-handles.html">0009-metric-handles</a></li>
<li><a href="0010-cumulative-to-counter.html">0010-cumulative-to-counter</a></li>
<li><a href="0016-named-tracers.html">0016-named-tracers</a></li>
<li><a href="0035-opentelemetry-protocol.html">0035-opentelemetry-protocol</a></li>
<li><a href="0038-version-semantic-attribute.html">0038-version-semantic-attribute</a></li>
<li><a href="0049-metric-label-set.html">0049-metric-label-set</a></li>
<li><a href="0059-otlp-trace-data-format.html">0059-otlp-trace-data-format</a></li>
<li><a href="0066-separate-context-propagation.html">0066-separate-context-propagation</a></li>
<li><a href="0070-metric-bound-instrument.html">0070-metric-bound-instrument</a></li>
<li><a href="0072-metric-observer.html">0072-metric-observer</a></li>
<li><a href="0080-remove-metric-gauge.html">0080-remove-metric-gauge</a></li>
<li><a href="0083-component.html">0083-component</a></li>
<li><a href="0088-metric-instrument-optional-refinements.html">0088-metric-instrument-optional-refinements</a></li>
<li><a href="0090-remove-labelset-from-metrics-api.html">0090-remove-labelset-from-metrics-api</a></li>
<li><a href="0091-logs-vocabulary.html">0091-logs-vocabulary</a></li>
<li><a href="0092-logs-vision.html">0092-logs-vision</a></li>
<li><a href="0097-log-data-model.html">0097-log-data-model</a></li>
<li><a href="0098-metric-instruments-explained.html">0098-metric-instruments-explained</a></li>
<li><a href="0099-otlp-http.html">0099-otlp-http</a></li>
<li><a href="0108-naming-guidelines.html">0108-naming-guidelines</a></li>
<li><a href="0110-z-pages.html">0110-z-pages</a></li>
<li><a href="0111-auto-resource-detection.html">0111-auto-resource-detection</a></li>
<li><a href="0113-exemplars.html">0113-exemplars</a></li>
<li><a href="0119-standard-system-metrics.html">0119-standard-system-metrics</a></li>
<li><a href="0121-config-service.html">0121-config-service</a></li>
<li><a href="0122-otlp-http-json.html">0122-otlp-http-json</a></li>
<li><a href="0126-Configurable-Metric-Aggregations.html">0126-Configurable-Metric-Aggregations</a></li>
<li><a href="0130-logs-1.0ga-definition.html">0130-logs-1.0ga-definition</a></li>
<li><a href="0131-otlp-export-behavior.html">0131-otlp-export-behavior</a></li>
<li><a href="0136-error_flagging.html">0136-error_flagging</a></li>
<li><a href="0143-versioning-and-stability.html">0143-versioning-and-stability</a></li>
<li><a href="0146-metrics-prototype-scenarios.html">0146-metrics-prototype-scenarios</a></li>
<li><a href="SUMMARY.html">SUMMARY</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
